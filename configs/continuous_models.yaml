global:
  logging_params:
    log_dir: "logs"
    name: "sequence_models"
  schedules_path: "tmp/nts_home_schedules.csv"
  # attributes_path: "tmp/nts_home_attributes_groups.csv"
  loader_params:
    train_batch_size: 1024
    val_batch_size:  1024
    num_workers: 8

  trainer_params:
    min_epochs: 50
    max_epochs: 200
    patience: 10

  experiment_params:
    LR: 0.001
    weight_decay: 0.0
    scheduler_gamma: 0.99
    kld_weight: 0.01

  seed: 1234

  encoder_params:
    name: "sequence"
    max_length: 16
    duration: 1440
    jitter: 0.3

# auto_attention:
#   encoder_params:
#     name: "sequence_staggered"
#     max_length: 16
#     duration: 1440
#     jitter: 0.1
#   model_params:
#     name: "AutoSeqAtt"
#     latent_dim: 6
#     hidden_n: 2
#     heads: 4
#     hidden_size: 128
#     ffwd_size: 256
#     dropout: 0.1

xattention:
  encoder_params:
    name: "sequence_staggered"
    max_length: 16
    duration: 1440
    jitter: 0.1
    fix_durations: False
  model_params:
    name: "VAESeqXAtt"
    latent_dim: 6
    hidden_n: 3
    heads: 7
    hidden_size: 128
    ffwd_size: 256
    dropout: 0
    embedding: concat
    position_embedding: learnt
    time_embedding: none
    latent_context: xattention

# cnn1d:
#   model_params:
#     name: "VAESeqCNN1D"
#     latent_dim: 6
#     hidden_layers: [256,256,256,256,256]
#     dropout: 0.0
#     stride: 2
#     kernel_size: 2
#     padding: 1

# cnn2d:
#   model_params:
#     name: "VAESeqCNN2D"
#     latent_dim: 6
#     hidden_layers: [256,256,256,256,256,256] 
#     dropout: 0.1

# fc:
#   model_params:
#     name: "VAESeqFC"
#     latent_dim: 6
#     hidden_layers: [128,128,128,128] 
#     dropout: 0.0

# rnn:
#   model_params:
#     name: "VAESeqLSTM"
#     latent_dim: 6
#     hidden_n: 5
#     hidden_size: 256
#     dropout: 0.1

