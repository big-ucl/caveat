global:
  logging_params:
    log_dir: "logs"
    name: "jm_z24_batch"

  schedules_path: "tmp/nts_home_schedules.csv"
  attributes_path: "tmp/nts_home_attributes.csv"

  conditionals:
    gender: nominal
    age: nominal
    # ethnicity: nominal
    # education: nominal
    # license: nominal
    car_access: nominal
    work_status: nominal
    # area: nominal
    income: nominal
    # hh_size: ordinal
    # hh_composition: nominal
    # hh_children: ordinal
    # hh_cars: ordinal
    # hh_bikes: ordinal
    # hh_motorcycles: ordinal

  evaluation_params:
    split_on: [gender, age, car_access, work_status, income]

  attribute_encoder: "tokens"

  encoder_params:
    name: "sequence"
    max_length: 12
    norm_duration: 1440
    jitter: 0.2

  model_params:
    name: "JVAESeqLSTM"
    hidden_layers: 5
    hidden_size: 128
    latent_dim: 24
    dropout: 0.1
    teacher_forcing_ratio: 0.5

  loader_params:
    train_batch_size: 1024
    val_batch_size:  1024
    num_workers: 8

  trainer_params:
    min_epochs: 30
    patience: 30
    max_epochs: 100

  experiment_params:
    LR: 0.01
    weight_decay: 0.0
    scheduler_gamma: 0.95
    kld_weight: 0.0001
    label_loss_weight: 0.00001


A:
  seed: 11111

B:
  seed: 22222

C:
  seed: 33333

D:
  seed: 44444

E:
  seed: 55555


# label:
#   trainer_params:
#     min_epochs: 50
#     patience: 10
#     max_epochs: 100
#     loss_scheduling:
#       label_loss_schedule: [10, 30]

# kld:
#   trainer_params:
#     min_epochs: 50
#     patience: 10
#     max_epochs: 100
#     loss_scheduling:
#       kld_loss_schedule: [10, 30]

# kld+batch:
#   trainer_params:
#     min_epochs: 50
#     patience: 10
#     max_epochs: 100
#     loss_scheduling:
#       kld_loss_schedule: [10, 30]
#       label_loss_schedule: [10, 30]

# vanilla:
#   trainer_params:
#     min_epochs: 50
#     patience: 10
#     max_epochs: 100

    
