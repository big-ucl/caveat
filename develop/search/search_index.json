{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative models for human activity sequences.</p> <p> </p> <p>Caveat is for building models that generate human activity sequences.</p>"},{"location":"#framework","title":"Framework","text":"<p>Caveat provides a framework to train and test generative models. A model run is composed of:</p> <ul> <li>Data - see the caveat examples for synthetic and real data generation</li> <li>Encoder - see caveat encoders for available encoders</li> <li>Model - see caveat models for available models</li> <li>Report - see caveat report</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Once installed get started using <code>caveat --help</code>.</p> <p><code>caveat run --help</code></p> <p>Train and report on a model using <code>caveat run configs/toy_run.yaml</code>. The run data, encoder, model and other parameters are controlled using a run config. This will write results and tensorboard logs to <code>logs/</code> (this is configurable). Monitor or review training progress using <code>tensorboard --logdir=logs</code>.</p> <p><code>caveat batch --help</code></p> <p>Train and report on a batch of runs using a special batch config <code>caveat batch configs/toy_batch.yaml</code>. Batch allows comparison of multiple models and/or hyper-params as per the batch config.</p> <p><code>caveat nrun --help</code></p> <p>Run and report the variance of n of the same run using <code>caveat nrun configs/toy_run.yaml --n 3</code>. The config is as per a regular run config but <code>seed</code> is ignored.</p>"},{"location":"#data","title":"Data","text":"<p>Caveat requires a .csv format to represent a population of activity sequences:</p> pid act start end 0 home 0 390 0 work 390 960 0 home 960 1440 1 home 0 390 1 education 390 960 1 home 960 1440 <ul> <li>pid (person id) field is a unique identifier for each sequence</li> <li>act is a categorical value for the type of activity in the sequence</li> <li>start and end are the start and end times of the activities in the sequence</li> </ul> <p>We commonly refer to these as populations. Times are assumed to be in minutes and should be integers. Valid sequences should be complete, ie the start of an activity should be equal to the end of the previous. The convention is to start at midnight. Such that time can be thought of as minutes since midnight.</p> <p>There is an example toy population with 1000 sequences in <code>caveat/examples/data</code>. There are also example notebooks for:</p> <ul> <li>Generation of a synthetic population</li> <li>Generation of a population from UK travel diaries</li> </ul>"},{"location":"#encoder","title":"Encoder","text":"<p>We are keen to test different encodings (such as continuous sequences versus descretised time-steps). The exact encoding required will depend on the model structure being used.</p> <p>The encoder and it's parameters are defined in the config <code>encoder</code> group.</p> <p>Encoders are defined in the <code>encoders</code> module and should be accessed via <code>caveat.encoders.library</code>.</p> <p>Note that encoders must implement both an encode and decode method so that model outputs can be converted back into the population format for reporting.</p>"},{"location":"#model","title":"Model","text":"<p>The model and it's parameters are defined in the config <code>model</code> group. Models are trained until validation stabilises or until some max number of epochs.</p> <p>Models are defined in <code>models</code> and should be accessed via <code>caveat.models.library</code>. Models and their training should be specified via the config.</p> <p>The <code>data_loader</code>, <code>experiment</code> and <code>trainer</code> hyper-params are also configured by similarly named groups. These groups use the standard pytorch-lightning framework.</p>"},{"location":"#evaluate","title":"Evaluate","text":"<p>Each model (with training weights from the best performing validation step) is used to generate a new \"sythetic\" population.</p> <p>Sythetic populations are compared to the original \"observed\" population.</p> <p>Evaluating the quality of generated populations is subjective. The <code>features</code> module provides functions for extracting features from populations. Such as \"activity durations\". These are then used to make descriptive metrics and distance metrics between the observed and sythetic populations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>caveat is an actively maintained and utilised project.</p>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<p>to report issues, request features, or exchange with our community, just follow the links below.</p> <p>Is something not working?</p> <p> Report a bug</p> <p>Missing information in our docs?</p> <p> Report a docs issue</p> <p>Want to submit an idea?</p> <p> Request a change</p> <p>Have a question or need help?</p> <p> Ask a question</p>"},{"location":"contributing/#developing-caveat","title":"Developing caveat","text":"<p>To find beginner-friendly existing bugs and feature requests you may like to start out with, take a look at our good first issues.</p>"},{"location":"contributing/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>To create a development environment for caveat, with all libraries required for development and quality assurance installed, it is easiest to install caveat using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li>Open the command line (or the \"miniforge prompt\" in Windows).</li> <li>Download (a.k.a., clone) the caveat repository: <code>git clone git@github.com:fredshone/caveat.git</code></li> <li>Change into the <code>caveat</code> directory: <code>cd caveat</code></li> <li>Create the caveat mamba environment: <code>mamba create -n caveat -c conda-forge --file requirements/base.txt --file requirements/dev.txt</code></li> <li>Activate the caveat mamba environment: <code>mamba activate caveat</code></li> <li>Install the caveat package into the environment, in editable mode and ignoring dependencies (we have dealt with those when creating the mamba environment): <code>pip install --no-deps -e .</code></li> </ol> <p>All together:</p> <pre><code>git clone git@github.com:fredshone/caveat.git\ncd caveat\nmamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch --file requirements/base.txt --file requirements/dev.txt\nmamba activate caveat\npip install --no-deps -e .\n</code></pre>"},{"location":"contributing/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>To run the example notebooks you will need to add a ipython kernel: <code>ipython kernel install --user --name=caveat</code>.</p>"},{"location":"contributing/#windoes-and-cuda","title":"Windoes and CUDA","text":"<p>If you want to get a cuda enabled windows install you can try the following mamba create: <pre><code>mamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch -c nvidia --file requirements/cuda_base.txt --file requirements/dev.txt\n</code></pre> Or lake a look here.</p> <p>If installing directly with pip, you can install these libraries using the <code>dev</code> option, i.e., <code>pip install -e '.[dev]'</code> Either way, you should add your environment as a jupyter kernel, so the example notebooks can run in the tests: <code>ipython kernel install --user --name=caveat</code> If you plan to make changes to the code then please make regular use of the following tools to verify the codebase while you work:</p> <ul> <li><code>pre-commit</code>: run <code>pre-commit install</code> in your command line to load inbuilt checks that will run every time you commit your changes. The checks are: 1. check no large files have been staged, 2. lint python files for major errors, 3. format python files to conform with the PEP8 standard. You can also run these checks yourself at any time to ensure staged changes are clean by calling <code>pre-commit</code>.</li> <li><code>pytest</code> - run the unit test suite and check test coverage.</li> </ul> <p>Note</p> <p>If you already have an environment called <code>caveat</code> on your system (e.g., for a stable installation of the package), you will need to chose a different environment name. You will then need to add this as a pytest argument when running the tests: <code>pytest --nbmake-kernel=[my-env-name]</code>.</p>"},{"location":"contributing/#rapid-fire-testing","title":"Rapid-fire testing","text":"<p>The following options allow you to strip down the test suite to the bare essentials: 1. The test suite includes unit tests and integration tests (in the form of jupyter notebooks found in the <code>examples</code> directory). The integration tests can be slow, so if you want to avoid them during development, you should run <code>pytest tests/</code>. 2. You can avoid generating coverage reports, by adding the <code>--no-cov</code> argument: <code>pytest --no-cov</code>. 3. By default, the tests run with up to two parallel threads, to increase this to e.g. 4 threads: <code>pytest -n4</code>.</p> <p>All together:</p> <pre><code>pytest tests/ --no-cov -n4\n</code></pre> <p>Note</p> <p>You cannot debug failing tests and have your tests run in parallel, you will need to set <code>-n0</code> if using the <code>--pdb</code> flag</p>"},{"location":"contributing/#memory-profiling","title":"Memory profiling","text":"<p>Note</p> <p>When you open a pull request (PR), one of the GitHub actions will run memory profiling for you. This means you don't have to do any profiling locally. However, if you can, it is still good practice to do so as you will catch issues earlier.</p> <p>caveat can be memory intensive; we like to ensure that any development to the core code does not exacerbate this. If you are running on a UNIX device (i.e., not on Windows), you can test whether any changes you have made adversely impact memory and time performance as follows:</p> <ol> <li>Install memray in your <code>caveat</code> mamba environment: <code>mamba install memray pytest-memray</code>.</li> <li>Run the memory profiling integration test: <code>pytest -p memray -m \"high_mem\" --no-cov</code>.</li> <li>Optionally, to visualise the memory allocation, run <code>pytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]</code> - where you must define <code>[my_path]</code> and <code>[my_prefix]</code> - followed by <code>memray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin</code>. You will then find the HTML report at <code>[my_path]/memray-flamegraph-[my_prefix]-tests-test_100_memory_profiling.py-test_mem.html</code>.</li> </ol> <p>All together:</p> <pre><code>mamba install memray pytest-memray\npytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]\nmemray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin\n</code></pre> <p>For more information on using memray, refer to their documentation.</p>"},{"location":"contributing/#submitting-changes","title":"Submitting changes","text":"<p>To contribute changes:</p> <ol> <li>Fork the project on GitHub.</li> <li>Create a feature branch to work on in your fork (<code>git checkout -b new-fix-or-feature</code>).</li> <li>Test your changes using <code>pytest</code>.</li> <li>Commit your changes to the feature branch (you should have <code>pre-commit</code> installed to ensure your code is correctly formatted when you commit changes).</li> <li>Push the branch to GitHub (<code>git push origin new-fix-or-feature</code>).</li> <li>On GitHub, create a new pull request from the feature branch.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<p>Before submitting a pull request, check whether you have:</p> <ul> <li>Added your changes to <code>CHANGELOG.md</code>.</li> <li>Added or updated documentation for your changes.</li> <li>Added tests if you implemented new functionality.</li> </ul> <p>When opening a pull request, please provide a clear summary of your changes!</p>"},{"location":"contributing/#commit-messages","title":"Commit messages","text":"<p>Please try to write clear commit messages. One-line messages are fine for small changes, but bigger changes should look like this:</p> <pre><code>A brief summary of the commit (max 50 characters)\n\nA paragraph or bullet-point list describing what changed and its impact,\ncovering as many lines as needed.\n</code></pre>"},{"location":"contributing/#code-conventions","title":"Code conventions","text":"<p>Start reading our code and you'll get the hang of it.</p> <p>We mostly follow the official Style Guide for Python Code (PEP8).</p> <p>We have chosen to use the uncompromising code formatter <code>black</code> and the linter <code>ruff</code>. When run from the root directory of this repo, <code>pyproject.toml</code> should ensure that formatting and linting fixes are in line with our custom preferences (e.g., 100 character maximum line length). The philosophy behind using <code>black</code> is to have uniform style throughout the project dictated by code. Since <code>black</code> is designed to minimise diffs, and make patches more human readable, this also makes code reviews more efficient. To make this a smooth experience, you should run <code>pre-commit install</code> after setting up your development environment, so that <code>black</code> makes all the necessary fixes to your code each time you commit, and so that <code>ruff</code> will highlight any errors in your code. If you prefer, you can also set up your IDE to run these two tools whenever you save your files, and to have <code>ruff</code> highlight erroneous code directly as you type. Take a look at their documentation for more information on configuring this.</p> <p>We require all new contributions to have docstrings for all modules, classes and methods. When adding docstrings, we request you use the Google docstring style.</p>"},{"location":"contributing/#release-checklist","title":"Release checklist","text":""},{"location":"contributing/#pre-release","title":"Pre-release","text":"<ul> <li> Make sure all unit and integration tests pass (This is best done by creating a pre-release pull request).</li> <li> Re-run tutorial Jupyter notebooks (<code>pytest examples/ --overwrite</code>).</li> <li> Make sure documentation builds without errors (<code>mike deploy [version]</code>, where <code>[version]</code> is the current minor release of the form <code>X.Y</code>).</li> <li> Make sure the changelog is up-to-date, especially that new features and backward incompatible changes are clearly marked.</li> </ul>"},{"location":"contributing/#create-release","title":"Create release","text":"<ul> <li> Bump the version number in <code>caveat/__init__.py</code></li> <li> Update the changelog with final version number of the form <code>vX.Y.Z</code>, release date, and github <code>compare</code> link (at the bottom of the page).</li> <li> Commit with message <code>Release vX.Y.Z</code>, then add a <code>vX.Y.Z</code> tag.</li> <li> Create a release pull request to verify that the conda package builds successfully.</li> <li> Once the PR is approved and merged, create a release through the GitHub web interface, using the same tag, titling it <code>Release vX.Y.Z</code> and include all the changelog elements that are not flagged as internal.</li> </ul>"},{"location":"contributing/#post-release","title":"Post-release","text":"<ul> <li> Update the changelog, adding a new <code>[Unreleased]</code> heading.</li> <li> Update <code>caveat/__init__.py</code> to the next version appended with <code>.dev0</code>, in preparation for the next main commit.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#setting-up-a-user-environment","title":"Setting up a user environment","text":"<p>As a <code>caveat</code> user, it is easiest to install using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li> <p>Open the command line (or the \"miniforge prompt\" in Windows).</p> </li> <li> <p>Create the caveat mamba environment: <code>mamba create -n caveat -c conda-forge -c city-modelling-lab caveat</code></p> </li> <li>Activate the caveat mamba environment: <code>mamba activate caveat</code></li> </ol> <p>All together:</p> <pre><code>mamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch\n</code></pre>"},{"location":"installation/#running-the-example-notebooks","title":"Running the example notebooks","text":"<p>If you have followed the non-developer installation instructions above, you will need to install <code>jupyter</code> into your <code>caveat</code> environment to run the example notebooks:</p> <pre><code>mamba install -n caveat jupyter\n</code></pre> <p>With Jupyter installed, it's easiest to then add the environment as a jupyter kernel:</p> <pre><code>mamba activate caveat\nipython kernel install --user --name=caveat\njupyter notebook\n</code></pre>"},{"location":"installation/#choosing-a-different-environment-name","title":"Choosing a different environment name","text":"<p>If you would like to use a different name to <code>caveat</code> for your mamba environment, the installation becomes (where <code>[my-env-name]</code> is your preferred name for the environment):</p> <pre><code>mamba create -n [my-env-name] -c conda-forge --file requirements/base.txt\nmamba activate [my-env-name]\nipython kernel install --user --name=[my-env-name]\n</code></pre>"},{"location":"installation/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>The install instructions are slightly different to create a development environment compared to a user environment:</p> <pre><code>git clone git@github.com:fredshone/caveat.git\ncd caveat\nmamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch --file requirements/base.txt --file requirements/dev.txt\nmamba activate caveat\npip install --no-deps -e .\n</code></pre>"},{"location":"installation/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>To run the example notebooks you will need to add a ipython kernel: <code>ipython kernel install --user --name=caveat</code>.</p>"},{"location":"installation/#windoes-and-cuda","title":"Windoes and CUDA","text":"<p>If you want to get a cuda enabled windows install you can try the following mamba create: <pre><code>mamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch -c nvidia --file requirements/cuda_base.txt --file requirements/dev.txt\n</code></pre> Or lake a look here.</p> <p>For more detailed installation instructions specific to developing the caveat codebase, see our development documentation.</p>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>This page provides documentation for our command line tools.</p>"},{"location":"api/cli/#caveat","title":"caveat","text":"<p>Console script for caveat.</p> <p>Usage:</p> <pre><code>caveat [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> boolean Show the version and exit. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-batch","title":"caveat batch","text":"<p>Train and report on a batch of encoders and models as per the given configuration file.</p> <p>Usage:</p> <pre><code>caveat batch [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-nrun","title":"caveat nrun","text":"<p>Train and report variance on n identical runs with varying seeds.</p> <p>Usage:</p> <pre><code>caveat nrun [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--n</code> integer N/A <code>5</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-report","title":"caveat report","text":"<p>Report on the given observed population and logs directory.</p> <p>Usage:</p> <pre><code>caveat report [OPTIONS] OBSERVED_PATH LOGS_DIR\n</code></pre> <p>Options:</p> Name Type Description Default <code>--name</code> text N/A <code>synthetic.csv</code> <code>--verbose</code> boolean N/A <code>False</code> <code>--head</code> integer N/A <code>10</code> <code>--batch</code>, <code>-b</code> boolean N/A <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-run","title":"caveat run","text":"<p>Train and report on an encoder and model as per the given configuration file.</p> <p>Usage:</p> <pre><code>caveat run [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"examples/1_synthetic_population_generation/","title":"Introduction to Synthetic Population Generation","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nimport pandas as pd\n\nfrom caveat.data.synth import ActivityGen\nfrom caveat.data.utils import generate_population, trace_to_pam\nfrom caveat.describe.times import (\n    joint_time_distributions_plot,\n    times_distributions_plot,\n)\nfrom caveat.describe.transitions import sequence_prob_plot\n</pre> from pathlib import Path  import pandas as pd  from caveat.data.synth import ActivityGen from caveat.data.utils import generate_population, trace_to_pam from caveat.describe.times import (     joint_time_distributions_plot,     times_distributions_plot, ) from caveat.describe.transitions import sequence_prob_plot In\u00a0[\u00a0]: Copied! <pre>write_path = Path(\"tmp/synthetic_population.csv\")\n</pre> write_path = Path(\"tmp/synthetic_population.csv\") In\u00a0[\u00a0]: Copied! <pre># Example\ngenerator = ActivityGen()\ngenerator.build()\n\ntrace = generator.run()\nplan = trace_to_pam(trace, generator.map)\nplan.plot()\n</pre> # Example generator = ActivityGen() generator.build()  trace = generator.run() plan = trace_to_pam(trace, generator.map) plan.plot() In\u00a0[\u00a0]: Copied! <pre>population = generate_population(gen=generator, size=100)\npopulation.act = population.act.map(generator.map)\npopulation = population[[\"pid\", \"act\", \"start\", \"end\", \"duration\"]]\npopulation\n</pre> population = generate_population(gen=generator, size=100) population.act = population.act.map(generator.map) population = population[[\"pid\", \"act\", \"start\", \"end\", \"duration\"]] population Out[\u00a0]: pid act start end duration 0 0 home 0 390 390 1 0 shop 390 405 15 2 0 work 405 945 540 3 0 shop 945 960 15 4 0 home 960 1440 480 ... ... ... ... ... ... 512 98 home 1155 1440 285 513 99 home 0 405 405 514 99 work 405 930 525 515 99 leisure 930 1035 105 516 99 home 1035 1440 405 <p>517 rows \u00d7 5 columns</p> In\u00a0[\u00a0]: Copied! <pre>write_path.parent.mkdir(exist_ok=True)\npopulation.to_csv(write_path, index=False)\n</pre> write_path.parent.mkdir(exist_ok=True) population.to_csv(write_path, index=False) In\u00a0[\u00a0]: Copied! <pre>def describe_col(population, col: str) -&gt; pd.DataFrame:\n    description = population.groupby(\"act\")[col].describe()[\n        [\"count\", \"mean\", \"std\", \"min\", \"max\"]\n    ]\n    description[\"attribute\"] = col\n    return description\n\n\ndef describe_cols(population, cols: list[str]) -&gt; pd.DataFrame:\n    description = pd.concat(\n        [describe_col(population, c) for c in cols], ignore_index=False\n    )\n    description = description.reset_index().set_index([\"attribute\", \"act\"])\n    return description\n\n\ndescribe_cols(population, [\"start\", \"end\", \"duration\"]).round()\n</pre> def describe_col(population, col: str) -&gt; pd.DataFrame:     description = population.groupby(\"act\")[col].describe()[         [\"count\", \"mean\", \"std\", \"min\", \"max\"]     ]     description[\"attribute\"] = col     return description   def describe_cols(population, cols: list[str]) -&gt; pd.DataFrame:     description = pd.concat(         [describe_col(population, c) for c in cols], ignore_index=False     )     description = description.reset_index().set_index([\"attribute\", \"act\"])     return description   describe_cols(population, [\"start\", \"end\", \"duration\"]).round() Out[\u00a0]: count mean std min max attribute act start education 45.0 830.0 209.0 420.0 1155.0 home 224.0 569.0 518.0 0.0 1305.0 leisure 57.0 808.0 298.0 375.0 1245.0 shop 91.0 678.0 308.0 375.0 1185.0 work 100.0 420.0 44.0 375.0 585.0 end education 45.0 947.0 201.0 480.0 1170.0 home 224.0 934.0 504.0 375.0 1440.0 leisure 57.0 873.0 312.0 390.0 1305.0 shop 91.0 707.0 303.0 390.0 1200.0 work 100.0 925.0 71.0 765.0 1035.0 duration education 45.0 118.0 52.0 15.0 240.0 home 224.0 365.0 83.0 30.0 525.0 leisure 57.0 65.0 23.0 15.0 135.0 shop 91.0 30.0 18.0 15.0 75.0 work 100.0 506.0 62.0 375.0 630.0 In\u00a0[\u00a0]: Copied! <pre>def time_distributions(population: pd.DataFrame, mapping: dict):\n    starts = {k: [] for k in mapping.values()}\n    ends = {k: [] for k in mapping.values()}\n    durations = {k: [] for k in mapping.values()}\n    for act, acts in population.groupby(\"act\"):\n        starts[act] = list(acts.start)\n        ends[act] = list(acts.end)\n        durations[act] = list(acts.duration)\n    return starts, ends, durations\n</pre> def time_distributions(population: pd.DataFrame, mapping: dict):     starts = {k: [] for k in mapping.values()}     ends = {k: [] for k in mapping.values()}     durations = {k: [] for k in mapping.values()}     for act, acts in population.groupby(\"act\"):         starts[act] = list(acts.start)         ends[act] = list(acts.end)         durations[act] = list(acts.duration)     return starts, ends, durations In\u00a0[\u00a0]: Copied! <pre>starts, ends, durations = time_distributions(population, generator.map)\n</pre> starts, ends, durations = time_distributions(population, generator.map) In\u00a0[\u00a0]: Copied! <pre>_ = times_distributions_plot(population, ys={})\n</pre> _ = times_distributions_plot(population, ys={}) In\u00a0[\u00a0]: Copied! <pre>_ = joint_time_distributions_plot(population, ys={})\n</pre> _ = joint_time_distributions_plot(population, ys={}) In\u00a0[\u00a0]: Copied! <pre>_ = sequence_prob_plot(population, ys={}, figsize=(8, 6))\n</pre> _ = sequence_prob_plot(population, ys={}, figsize=(8, 6)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/1_synthetic_population_generation/#introduction-to-synthetic-population-generation","title":"Introduction to Synthetic Population Generation\u00b6","text":""},{"location":"examples/2_NTS_population_generation/","title":"Generate Population from National Transport Survey Data","text":"In\u00a0[2]: Copied! <pre>from pathlib import Path\n\nimport pandas as pd\nfrom pam import read\nfrom pam.core import Population\nfrom pam.utils import datetime_to_matsim_time\n\nfrom caveat.describe.times import (\n    joint_time_distributions_plot,\n    times_distributions_plot,\n)\nfrom caveat.describe.transitions import sequence_prob_plot\n</pre> from pathlib import Path  import pandas as pd from pam import read from pam.core import Population from pam.utils import datetime_to_matsim_time  from caveat.describe.times import (     joint_time_distributions_plot,     times_distributions_plot, ) from caveat.describe.transitions import sequence_prob_plot In\u00a0[3]: Copied! <pre>trips_csv = \"data/dummyNTS/trips.tab\"\nyear = 2021\nwrite_dir = Path(\"tmp\")\n</pre> trips_csv = \"data/dummyNTS/trips.tab\" year = 2021 write_dir = Path(\"tmp\") In\u00a0[4]: Copied! <pre>travel_diaries = pd.read_csv(\n    trips_csv,\n    sep=\"\\t\",\n    usecols=[\n        \"TripID\",\n        \"JourSeq\",\n        \"DayID\",\n        \"IndividualID\",\n        \"HouseholdID\",\n        \"MainMode_B04ID\",\n        \"TripPurpFrom_B01ID\",\n        \"TripPurpTo_B01ID\",\n        \"TripStart\",\n        \"TripEnd\",\n        \"TripOrigGOR_B02ID\",\n        \"TripDestGOR_B02ID\",\n        \"W5\",\n        \"SurveyYear\",\n    ],\n)\n\ntravel_diaries = travel_diaries.rename(\n    columns={  # rename data\n        \"TripID\": \"tid\",\n        \"JourSeq\": \"seq\",\n        \"DayID\": \"day\",\n        \"IndividualID\": \"iid\",\n        \"HouseholdID\": \"hid\",\n        \"TripOrigGOR_B02ID\": \"ozone\",\n        \"TripDestGOR_B02ID\": \"dzone\",\n        \"TripPurpFrom_B01ID\": \"oact\",\n        \"TripPurpTo_B01ID\": \"dact\",\n        \"MainMode_B04ID\": \"mode\",\n        \"TripStart\": \"tst\",\n        \"TripEnd\": \"tet\",\n        \"W5\": \"freq\",\n        \"SurveyYear\": \"year\",\n    }\n)\n\ntravel_diaries = travel_diaries[travel_diaries.year == year]\n\ntravel_diaries.tst = pd.to_numeric(travel_diaries.tst, errors=\"coerce\")\ntravel_diaries.tet = pd.to_numeric(travel_diaries.tet, errors=\"coerce\")\ntravel_diaries.ozone = pd.to_numeric(travel_diaries.ozone, errors=\"coerce\")\ntravel_diaries.dzone = pd.to_numeric(travel_diaries.dzone, errors=\"coerce\")\ntravel_diaries.freq = pd.to_numeric(travel_diaries.freq, errors=\"coerce\")\n\ntravel_diaries[\"did\"] = travel_diaries.groupby(\"iid\")[\"day\"].transform(\n    lambda x: pd.factorize(x)[0] + 1\n)\ntravel_diaries[\"pid\"] = [\n    f\"{i}-{d}\" for i, d in zip(travel_diaries.iid, travel_diaries.did)\n]\n\ntravel_diaries = travel_diaries.loc[\n    travel_diaries.groupby(\"pid\")\n    .filter(lambda x: pd.isnull(x).sum().sum() &lt; 1)\n    .index\n]\n# travel_diaries.freq = travel_diaries.freq / travel_diaries.groupby(\"iid\").day.transform(\"nunique\")\ntravel_diaries.loc[travel_diaries.tet == 0, \"tet\"] = 1440\n\ntravel_diaries = travel_diaries.drop(\n    [\"tid\", \"iid\", \"day\", \"year\", \"did\"], axis=1\n)\n\nmode_mapping = {\n    1: \"walk\",\n    2: \"bike\",\n    3: \"car\",  #'Car/van driver'\n    4: \"car\",  #'Car/van driver'\n    5: \"car\",  #'Motorcycle',\n    6: \"car\",  #'Other private transport',\n    7: \"pt\",  # Bus in London',\n    8: \"pt\",  #'Other local bus',\n    9: \"pt\",  #'Non-local bus',\n    10: \"pt\",  #'London Underground',\n    11: \"pt\",  #'Surface Rail',\n    12: \"car\",  #'Taxi/minicab',\n    13: \"pt\",  #'Other public transport',\n    -10: \"DEAD\",\n    -8: \"NA\",\n}\n\npurp_mapping = {\n    1: \"work\",\n    2: \"work\",  #'In course of work',\n    3: \"education\",\n    4: \"shop\",  #'Food shopping',\n    5: \"shop\",  #'Non food shopping',\n    6: \"medical\",  #'Personal business medical',\n    7: \"other\",  #'Personal business eat/drink',\n    8: \"other\",  #'Personal business other',\n    9: \"other\",  #'Eat/drink with friends',\n    10: \"visit\",  #'Visit friends',\n    11: \"other\",  #'Other social',\n    12: \"other\",  #'Entertain/ public activity',\n    13: \"other\",  #'Sport: participate',\n    14: \"home\",  #'Holiday: base',\n    15: \"other\",  #'Day trip/just walk',\n    16: \"other\",  #'Other non-escort',\n    17: \"escort\",  #'Escort home',\n    18: \"escort\",  #'Escort work',\n    19: \"escort\",  #'Escort in course of work',\n    20: \"escort\",  #'Escort education',\n    21: \"escort\",  #'Escort shopping/personal business',\n    22: \"escort\",  #'Other escort',\n    23: \"home\",  #'Home',\n    -10: \"DEAD\",\n    -8: \"NA\",\n}\n\ntravel_diaries[\"mode\"] = travel_diaries[\"mode\"].map(mode_mapping)\ntravel_diaries[\"oact\"] = travel_diaries[\"oact\"].map(purp_mapping)\ntravel_diaries[\"dact\"] = travel_diaries[\"dact\"].map(purp_mapping)\ntravel_diaries.tst = travel_diaries.tst.astype(int)\ntravel_diaries.tet = travel_diaries.tet.astype(int)\n\ntravel_diaries.head()\n</pre> travel_diaries = pd.read_csv(     trips_csv,     sep=\"\\t\",     usecols=[         \"TripID\",         \"JourSeq\",         \"DayID\",         \"IndividualID\",         \"HouseholdID\",         \"MainMode_B04ID\",         \"TripPurpFrom_B01ID\",         \"TripPurpTo_B01ID\",         \"TripStart\",         \"TripEnd\",         \"TripOrigGOR_B02ID\",         \"TripDestGOR_B02ID\",         \"W5\",         \"SurveyYear\",     ], )  travel_diaries = travel_diaries.rename(     columns={  # rename data         \"TripID\": \"tid\",         \"JourSeq\": \"seq\",         \"DayID\": \"day\",         \"IndividualID\": \"iid\",         \"HouseholdID\": \"hid\",         \"TripOrigGOR_B02ID\": \"ozone\",         \"TripDestGOR_B02ID\": \"dzone\",         \"TripPurpFrom_B01ID\": \"oact\",         \"TripPurpTo_B01ID\": \"dact\",         \"MainMode_B04ID\": \"mode\",         \"TripStart\": \"tst\",         \"TripEnd\": \"tet\",         \"W5\": \"freq\",         \"SurveyYear\": \"year\",     } )  travel_diaries = travel_diaries[travel_diaries.year == year]  travel_diaries.tst = pd.to_numeric(travel_diaries.tst, errors=\"coerce\") travel_diaries.tet = pd.to_numeric(travel_diaries.tet, errors=\"coerce\") travel_diaries.ozone = pd.to_numeric(travel_diaries.ozone, errors=\"coerce\") travel_diaries.dzone = pd.to_numeric(travel_diaries.dzone, errors=\"coerce\") travel_diaries.freq = pd.to_numeric(travel_diaries.freq, errors=\"coerce\")  travel_diaries[\"did\"] = travel_diaries.groupby(\"iid\")[\"day\"].transform(     lambda x: pd.factorize(x)[0] + 1 ) travel_diaries[\"pid\"] = [     f\"{i}-{d}\" for i, d in zip(travel_diaries.iid, travel_diaries.did) ]  travel_diaries = travel_diaries.loc[     travel_diaries.groupby(\"pid\")     .filter(lambda x: pd.isnull(x).sum().sum() &lt; 1)     .index ] # travel_diaries.freq = travel_diaries.freq / travel_diaries.groupby(\"iid\").day.transform(\"nunique\") travel_diaries.loc[travel_diaries.tet == 0, \"tet\"] = 1440  travel_diaries = travel_diaries.drop(     [\"tid\", \"iid\", \"day\", \"year\", \"did\"], axis=1 )  mode_mapping = {     1: \"walk\",     2: \"bike\",     3: \"car\",  #'Car/van driver'     4: \"car\",  #'Car/van driver'     5: \"car\",  #'Motorcycle',     6: \"car\",  #'Other private transport',     7: \"pt\",  # Bus in London',     8: \"pt\",  #'Other local bus',     9: \"pt\",  #'Non-local bus',     10: \"pt\",  #'London Underground',     11: \"pt\",  #'Surface Rail',     12: \"car\",  #'Taxi/minicab',     13: \"pt\",  #'Other public transport',     -10: \"DEAD\",     -8: \"NA\", }  purp_mapping = {     1: \"work\",     2: \"work\",  #'In course of work',     3: \"education\",     4: \"shop\",  #'Food shopping',     5: \"shop\",  #'Non food shopping',     6: \"medical\",  #'Personal business medical',     7: \"other\",  #'Personal business eat/drink',     8: \"other\",  #'Personal business other',     9: \"other\",  #'Eat/drink with friends',     10: \"visit\",  #'Visit friends',     11: \"other\",  #'Other social',     12: \"other\",  #'Entertain/ public activity',     13: \"other\",  #'Sport: participate',     14: \"home\",  #'Holiday: base',     15: \"other\",  #'Day trip/just walk',     16: \"other\",  #'Other non-escort',     17: \"escort\",  #'Escort home',     18: \"escort\",  #'Escort work',     19: \"escort\",  #'Escort in course of work',     20: \"escort\",  #'Escort education',     21: \"escort\",  #'Escort shopping/personal business',     22: \"escort\",  #'Other escort',     23: \"home\",  #'Home',     -10: \"DEAD\",     -8: \"NA\", }  travel_diaries[\"mode\"] = travel_diaries[\"mode\"].map(mode_mapping) travel_diaries[\"oact\"] = travel_diaries[\"oact\"].map(purp_mapping) travel_diaries[\"dact\"] = travel_diaries[\"dact\"].map(purp_mapping) travel_diaries.tst = travel_diaries.tst.astype(int) travel_diaries.tet = travel_diaries.tet.astype(int)  travel_diaries.head() Out[4]: hid seq mode oact dact freq tst tet ozone dzone pid 0 1 1 car home visit 0.989618 675 683 7 7 1-1 1 1 2 car visit other 1.002945 720 735 7 7 1-1 2 1 3 car other visit 0.989618 770 780 7 7 1-1 3 1 4 car visit home 0.989618 1110 1130 7 7 1-1 4 1 1 car home visit 0.999891 760 770 7 7 1-2 In\u00a0[5]: Copied! <pre>pam_population = read.load_travel_diary(\n    trips=travel_diaries, trip_freq_as_person_freq=True\n)\nprint(pam_population.stats)\npam_population.fix_plans()\nprint(pam_population.stats)\npam_population.random_person().plot()\n</pre> pam_population = read.load_travel_diary(     trips=travel_diaries, trip_freq_as_person_freq=True ) print(pam_population.stats) pam_population.fix_plans() print(pam_population.stats) pam_population.random_person().plot() <pre>Using from-to activity parser using 'oact' and 'dact' columns\nAdding pid-&gt;hh mapping to persons_attributes from trips.\n\n        Unable to load household area ('hzone') - not found in trips_diary or unable to build from attributes.\n        Pam will try to infer home location from activities, but this behaviour is not recommended.\n        \nUsing freq of 'None' for all trips.\n Person pid:2-5 hid:1 plan does not start with 'home' activity: work\n Person pid:2-6 hid:1 plan does not start with 'home' activity: work\n Person pid:2-7 hid:1 plan does not start with 'home' activity: work\n Person pid:3-4 hid:1 plan does not start with 'home' activity: education\n</pre> <pre>{'num_households': 3, 'num_people': 39, 'num_activities': 188, 'num_legs': 149}\n{'num_households': 3, 'num_people': 39, 'num_activities': 175, 'num_legs': 136}\n</pre> In\u00a0[6]: Copied! <pre>def dt_to_min(dt) -&gt; int:\n    h, m, s = datetime_to_matsim_time(dt).split(\":\")\n    return (int(h) * 60) + int(m)\n\n\ndef pam_to_population(population: Population) -&gt; pd.DataFrame:\n    \"\"\"write trace of population. Ignoring trips.\"\"\"\n    record = []\n    for uid, (hid, pid, person) in enumerate(population.people()):\n        for i in range(0, len(person.plan) - 1, 2):\n            record.append(\n                [\n                    uid,\n                    hid,\n                    person.plan[i].act,\n                    dt_to_min(person.plan[i].start_time),\n                    dt_to_min(person.plan[i + 1].end_time),\n                ]\n            )\n        record.append(\n            [\n                uid,\n                hid,\n                person.plan[-1].act,\n                dt_to_min(person.plan[-1].start_time),\n                dt_to_min(person.plan[-1].end_time),\n            ]\n        )\n\n    df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])\n    df[\"duration\"] = df.end - df.start\n    return df\n\n\npopulation = pam_to_population(population=pam_population)\n\npopulation.describe()\n</pre> def dt_to_min(dt) -&gt; int:     h, m, s = datetime_to_matsim_time(dt).split(\":\")     return (int(h) * 60) + int(m)   def pam_to_population(population: Population) -&gt; pd.DataFrame:     \"\"\"write trace of population. Ignoring trips.\"\"\"     record = []     for uid, (hid, pid, person) in enumerate(population.people()):         for i in range(0, len(person.plan) - 1, 2):             record.append(                 [                     uid,                     hid,                     person.plan[i].act,                     dt_to_min(person.plan[i].start_time),                     dt_to_min(person.plan[i + 1].end_time),                 ]             )         record.append(             [                 uid,                 hid,                 person.plan[-1].act,                 dt_to_min(person.plan[-1].start_time),                 dt_to_min(person.plan[-1].end_time),             ]         )      df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])     df[\"duration\"] = df.end - df.start     return df   population = pam_to_population(population=pam_population)  population.describe() Out[6]: pid hid start end duration count 175.000000 175.000000 175.000000 175.000000 175.000000 mean 17.520000 1.685714 686.988571 1007.902857 320.914286 std 11.502044 0.921375 416.041844 301.367566 262.947656 min 0.000000 1.000000 0.000000 348.000000 6.000000 25% 8.000000 1.000000 465.000000 785.000000 52.000000 50% 17.000000 1.000000 805.000000 950.000000 320.000000 75% 28.000000 3.000000 967.000000 1308.500000 527.500000 max 38.000000 3.000000 1325.000000 1440.000000 962.000000 In\u00a0[7]: Copied! <pre>_ = times_distributions_plot(population, ys={})\n</pre> _ = times_distributions_plot(population, ys={}) In\u00a0[8]: Copied! <pre>_ = joint_time_distributions_plot(population, ys={})\n</pre> _ = joint_time_distributions_plot(population, ys={}) In\u00a0[9]: Copied! <pre>_ = sequence_prob_plot(population, ys={}, figsize=(8, 6))\n</pre> _ = sequence_prob_plot(population, ys={}, figsize=(8, 6)) In\u00a0[10]: Copied! <pre>write_path = write_dir / \"nts_toy_population.csv\"\nwrite_path.parent.mkdir(exist_ok=True)\npopulation.to_csv(write_path, index=False)\n</pre> write_path = write_dir / \"nts_toy_population.csv\" write_path.parent.mkdir(exist_ok=True) population.to_csv(write_path, index=False) In\u00a0[11]: Copied! <pre>def pam_to_population_home_based_plans_only(\n    population: Population,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert population to trace. Only home-based plans are included.\"\"\"\n    record = []\n    for uid, (hid, pid, person) in enumerate(population.people()):\n        person_record = []\n        for i in range(0, len(person.plan) - 1, 2):\n            person_record.append(\n                [\n                    uid,\n                    hid,\n                    person.plan[i].act,\n                    dt_to_min(person.plan[i].start_time),\n                    dt_to_min(person.plan[i + 1].end_time),\n                ]\n            )\n        person_record.append(\n            [\n                uid,\n                hid,\n                person.plan[-1].act,\n                dt_to_min(person.plan[-1].start_time),\n                dt_to_min(person.plan[-1].end_time),\n            ]\n        )\n        if (person_record[0][2] == \"home\") and (person_record[-1][2] == \"home\"):\n            record.extend(person_record)\n\n    df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])\n    df[\"duration\"] = df.end - df.start\n    return df\n\n\nhome_population = pam_to_population_home_based_plans_only(pam_population)\n\nwrite_path = write_dir / \"nts_toy_home_population.csv\"\nwrite_path.parent.mkdir(exist_ok=True)\nhome_population.to_csv(write_path, index=False)\n</pre> def pam_to_population_home_based_plans_only(     population: Population, ) -&gt; pd.DataFrame:     \"\"\"Convert population to trace. Only home-based plans are included.\"\"\"     record = []     for uid, (hid, pid, person) in enumerate(population.people()):         person_record = []         for i in range(0, len(person.plan) - 1, 2):             person_record.append(                 [                     uid,                     hid,                     person.plan[i].act,                     dt_to_min(person.plan[i].start_time),                     dt_to_min(person.plan[i + 1].end_time),                 ]             )         person_record.append(             [                 uid,                 hid,                 person.plan[-1].act,                 dt_to_min(person.plan[-1].start_time),                 dt_to_min(person.plan[-1].end_time),             ]         )         if (person_record[0][2] == \"home\") and (person_record[-1][2] == \"home\"):             record.extend(person_record)      df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])     df[\"duration\"] = df.end - df.start     return df   home_population = pam_to_population_home_based_plans_only(pam_population)  write_path = write_dir / \"nts_toy_home_population.csv\" write_path.parent.mkdir(exist_ok=True) home_population.to_csv(write_path, index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/2_NTS_population_generation/#generate-population-from-national-transport-survey-data","title":"Generate Population from National Transport Survey Data\u00b6","text":"<p>Provided example is toy data. You can access UK travel survey from 2002-2021 from the UK Data Service.</p>"},{"location":"examples/3_features_and_correctness/","title":"Features and Correctness","text":"In\u00a0[3]: Copied! <pre>import random\n\nimport pandas as pd\n\nfrom caveat.describe import features\nfrom caveat.describe.times import (\n    joint_time_distributions_plot,\n    times_distributions_plot,\n)\nfrom caveat.describe.transitions import sequence_prob_plot\nfrom caveat.distance import mape, emd\nfrom caveat.features import participation, times\n</pre> import random  import pandas as pd  from caveat.describe import features from caveat.describe.times import (     joint_time_distributions_plot,     times_distributions_plot, ) from caveat.describe.transitions import sequence_prob_plot from caveat.distance import mape, emd from caveat.features import participation, times In\u00a0[4]: Copied! <pre># create some fake data\nraw = pd.read_csv(\"data/synthetic_population.csv\")\n\n\ndef down_sample(df, p):\n    n_samples = int(len(df.pid.unique()) * p)\n    sample_ids = random.sample(list(df.pid.unique()), n_samples)\n    sampled = df[df.pid.isin(sample_ids)]\n    return sampled\n\n\nobserved = down_sample(raw, 0.2)\n\na = down_sample(observed, 0.2)\nb = down_sample(raw, 0.2)\nsynthetic = {\"a\": a, \"b\": b}\n</pre> # create some fake data raw = pd.read_csv(\"data/synthetic_population.csv\")   def down_sample(df, p):     n_samples = int(len(df.pid.unique()) * p)     sample_ids = random.sample(list(df.pid.unique()), n_samples)     sampled = df[df.pid.isin(sample_ids)]     return sampled   observed = down_sample(raw, 0.2)  a = down_sample(observed, 0.2) b = down_sample(raw, 0.2) synthetic = {\"a\": a, \"b\": b} <p>For example we can extract the start times for each activity and report the averages:</p> In\u00a0[5]: Copied! <pre>starts = times.start_times_by_act(observed)\nfeatures.average(starts)\n</pre> starts = times.start_times_by_act(observed) features.average(starts) Out[5]: <pre>education    845.806452\nhome         546.814988\nleisure      812.727273\nshop         624.824561\nwork         422.089552\ndtype: float64</pre> <p>Note that the starts feature is a dictionary of tuples. Where the first value describes the 'support' of the feature and the second the frequncy of each observation.</p> <p>We can take a better look at the distributions by plotting them:</p> In\u00a0[6]: Copied! <pre>fig = times_distributions_plot(observed, None)\n</pre> fig = times_distributions_plot(observed, None) In\u00a0[7]: Copied! <pre>participation_rates = participation.participation_rates_by_act(observed)\nprint(features.average(participation_rates))\n</pre> participation_rates = participation.participation_rates_by_act(observed) print(features.average(participation_rates)) <pre>education    0.465\nhome         2.135\nleisure      0.605\nshop         0.855\nwork         1.005\ndtype: float64\n</pre> In\u00a0[8]: Copied! <pre>fig = sequence_prob_plot(observed, synthetic, figsize=(12, 4))\n</pre> fig = sequence_prob_plot(observed, synthetic, figsize=(12, 4)) In\u00a0[9]: Copied! <pre>participation_rates = participation.participation_rates_by_seq_act(observed)\nprint(features.average(participation_rates).head(10))\n</pre> participation_rates = participation.participation_rates_by_seq_act(observed) print(features.average(participation_rates).head(10)) <pre>0home         1.000\n1leisure      0.150\n1shop         0.500\n1work         0.350\n2education    0.235\n2home         0.035\n2leisure      0.100\n2shop         0.075\n2work         0.555\n3education    0.190\ndtype: float64\n</pre> <p>Or by the enumeration of that type of activity in each sequence:</p> In\u00a0[10]: Copied! <pre>participation_rates = participation.participation_rates_by_act_enum(observed)\nprint(features.average(participation_rates).head(10))\n</pre> participation_rates = participation.participation_rates_by_act_enum(observed) print(features.average(participation_rates).head(10)) <pre>education0    0.455\neducation1    0.010\nhome0         1.000\nhome1         1.000\nhome2         0.135\nleisure0      0.570\nleisure1      0.035\nshop0         0.730\nshop1         0.125\nwork0         1.000\ndtype: float64\n</pre> <p>In these examples we use additional segmentation to get more information about the sequence. For example we can differentiate between the participation in (i) education as the third activity (2education) versus the fourth activity (3education), or (ii) the first education activity (education0) versus the second education activity (education1).</p> <p>In all cases we use weighted averaging to combine segmented features into single metrics. Where weighting is ussually the number of each feature in the observed population of sequences.</p> In\u00a0[11]: Copied! <pre>start_durations = times.start_and_duration_by_act_bins(observed, bin_size=10)\n# average 2d averages each dimension and then sums so that we can return an float\nfeatures.average2d(start_durations)\n</pre> start_durations = times.start_and_duration_by_act_bins(observed, bin_size=10) # average 2d averages each dimension and then sums so that we can return an float features.average2d(start_durations) Out[11]: <pre>education    967.849462\nhome         932.693208\nleisure      874.545455\nshop         665.029240\nwork         935.970149\ndtype: float64</pre> In\u00a0[12]: Copied! <pre>fig = joint_time_distributions_plot(observed, None, figsize=(12, 4))\n</pre> fig = joint_time_distributions_plot(observed, None, figsize=(12, 4)) In\u00a0[13]: Copied! <pre>fig = times_distributions_plot(observed, synthetic)\n</pre> fig = times_distributions_plot(observed, synthetic) <p>To make a quantitave comparison between populations of sequences we primarilly use Wassersetin \"earth movers\" distance. This measure the amount of \"work\" required to make one distribuion match another.</p> In\u00a0[14]: Copied! <pre>x = times.start_times_by_act(observed)\nya = times.start_times_by_act(synthetic[\"a\"])\nyb = times.start_times_by_act(synthetic[\"b\"])\nprint(\"synthetic population A: \", emd(x[\"home\"], ya[\"home\"]))\nprint(\"synthetic population B: \", emd(x[\"home\"], yb[\"home\"]))\n</pre> x = times.start_times_by_act(observed) ya = times.start_times_by_act(synthetic[\"a\"]) yb = times.start_times_by_act(synthetic[\"b\"]) print(\"synthetic population A: \", emd(x[\"home\"], ya[\"home\"])) print(\"synthetic population B: \", emd(x[\"home\"], yb[\"home\"])) <pre>synthetic population A:  8.073906649964593\nsynthetic population B:  5.224775224775256\n</pre> <p>In this case we might proclaim population B to be better. In practice we will use a lot more features and there will generally be trade-offs between them.</p> <p>For probability features (in particular participation) we also sometime use absolute percentage error. This is particularly useful for highlighting the participation in uncommon activities which are often problematic for generative models.</p> In\u00a0[15]: Copied! <pre>x = participation.participation_prob_by_act(observed)\nya = participation.participation_prob_by_act(synthetic[\"a\"])\nyb = participation.participation_prob_by_act(synthetic[\"b\"])\nprint(\"synthetic population A: \", mape(x[\"leisure\"], ya[\"shop\"]))\nprint(\"synthetic population B: \", mape(x[\"leisure\"], yb[\"shop\"]))\n</pre> x = participation.participation_prob_by_act(observed) ya = participation.participation_prob_by_act(synthetic[\"a\"]) yb = participation.participation_prob_by_act(synthetic[\"b\"]) print(\"synthetic population A: \", mape(x[\"leisure\"], ya[\"shop\"])) print(\"synthetic population B: \", mape(x[\"leisure\"], yb[\"shop\"])) <pre>synthetic population A:  0.2875000000000001\nsynthetic population B:  0.2137931034482759\n</pre> <p>By this metric, population A appears better.</p>"},{"location":"examples/3_features_and_correctness/#features-and-correctness","title":"Features and Correctness\u00b6","text":"<p>We want to be able to describe our populations of sequences and compare them. To do this we extract various distributions, but we call them features. These features are designed such that they can be used to describe and measure correctness as a distance.</p> <p>We define features as belonging to one of the following \"domains\":</p> <ul> <li>Structural features are designed to check for \"structural zeros\", ie outputs that should not be possible.<ul> <li>eg, if each sequence starts with a \"home\" activity</li> <li>eg, if each sequence has a total duration of 24 hours</li> </ul> </li> <li>Participation features check for the occurance of activity types in a sequence, they can be presented as rates or probabilities.<ul> <li>eg, if each sequence participates in the \"work\" activity or not</li> </ul> </li> <li>Transition features describe the ordering of activities within sequences.<ul> <li>eg, how many times a sequence transitions from \"home\" to \"work\"</li> </ul> </li> <li>times/scheduling features describe when activities take place.<ul> <li>eg, the start time of all \"shop\" activities</li> </ul> </li> </ul> <p>We also use frequency features to describe the aggregate probability of an activity taking place in a given time bin. For example, \"X% of agents are at work between 10am and 11am\".</p> <p>We also use uniqueness as a measure of diversity within a population of sequences.</p>"},{"location":"examples/3_features_and_correctness/#feature-structure","title":"Feature Structure\u00b6","text":"<p>Features are commonly segmented into a dictionary of keys and values, where the key describes the segment. Features are commonly segmented by activity or transition type:</p> <pre><code>participation_probability = {\n    \"home\": [1,1,1],\n    \"work\": [1,0,0],\n    \"shop\": [0,1,0]\n}\n</code></pre> <p>In the above example, each feature segment records if each of the 3 segements contained a \"home\", \"work\" or \"shop\" activity.</p> <p>In practice we compress this representation into frequency counts, represented by a tuple of the (i) possible values and (ii) their frequncies, in this simple case we get:</p> <pre><code>participation_probability = {\n    \"home\": ([0, 1], [0, 3]),\n    \"work\": ([0, 1], [2, 1]),\n    \"shop\": ([0, 1], [2, 1]),\n}\n</code></pre>"},{"location":"examples/3_features_and_correctness/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Features can be described or plotted using functions in the describe module:</p>"},{"location":"examples/3_features_and_correctness/#feature-segmentation","title":"Feature Segmentation\u00b6","text":"<p>We can use more interesting types of segmentation to extact more descriptive features. For example we can enumerate activity type by it's location in the sequence:</p>"},{"location":"examples/3_features_and_correctness/#dimensions","title":"Dimensions\u00b6","text":"<p>Start times are a one dimensional feature, but we can also consider multi-demnsional features:</p>"},{"location":"examples/3_features_and_correctness/#distances","title":"Distances\u00b6","text":"<p>When comparing features we can generally see complex distributions:</p>"},{"location":"examples/4_creativity/","title":"Creativity","text":"In\u00a0[1]: Copied! <pre>import random\n\nimport pandas as pd\n\nfrom caveat.features import creativity\n</pre> import random  import pandas as pd  from caveat.features import creativity In\u00a0[2]: Copied! <pre># create some fake data\nraw = pd.read_csv(\"data/synthetic_population.csv\")\n\n\ndef down_sample(df, p):\n    n_samples = int(len(df.pid.unique()) * p)\n    sample_ids = random.sample(list(df.pid.unique()), n_samples)\n    sampled = df[df.pid.isin(sample_ids)]\n    return sampled\n\n\nobserved = down_sample(raw, 0.2)\n\na = down_sample(observed, 0.5)\nb = down_sample(raw, 0.2)\nsynthetic = {\"a\": a, \"b\": b}\n</pre> # create some fake data raw = pd.read_csv(\"data/synthetic_population.csv\")   def down_sample(df, p):     n_samples = int(len(df.pid.unique()) * p)     sample_ids = random.sample(list(df.pid.unique()), n_samples)     sampled = df[df.pid.isin(sample_ids)]     return sampled   observed = down_sample(raw, 0.2)  a = down_sample(observed, 0.5) b = down_sample(raw, 0.2) synthetic = {\"a\": a, \"b\": b} In\u00a0[3]: Copied! <pre>observed_hash = creativity.hash_population(observed)\na_hash = creativity.hash_population(a)\nb_hash = creativity.hash_population(b)\n</pre> observed_hash = creativity.hash_population(observed) a_hash = creativity.hash_population(a) b_hash = creativity.hash_population(b) In\u00a0[4]: Copied! <pre>print(\n    f\"Observed population of size {len(observed)} has diversity of {creativity.diversity(observed, observed_hash)}\"\n)\nprint(\n    f\"Synthetic population A of size {len(a)} has diversity of {creativity.diversity(a, a_hash)}\"\n)\nprint(\n    f\"Synthetic population B of size {len(b)} has diversity of {creativity.diversity(b, b_hash)}\"\n)\n</pre> print(     f\"Observed population of size {len(observed)} has diversity of {creativity.diversity(observed, observed_hash)}\" ) print(     f\"Synthetic population A of size {len(a)} has diversity of {creativity.diversity(a, a_hash)}\" ) print(     f\"Synthetic population B of size {len(b)} has diversity of {creativity.diversity(b, b_hash)}\" ) <pre>Observed population of size 1010 has diversity of 0.925\nSynthetic population A of size 512 has diversity of 0.91\nSynthetic population B of size 1024 has diversity of 0.935\n</pre> <p>In all cases we have high diversity. Note that that the size of a population also has an impact on diversity. we expect it to be easier to generate a diverse population if it is smaller.</p> In\u00a0[5]: Copied! <pre>print(\n    f\"Synthetic population A of size {len(a)} has novelty of {creativity.novelty(observed_hash, a, a_hash)}\"\n)\nprint(\n    f\"Synthetic population B of size {len(b)} has novelty of {creativity.novelty(observed_hash, b, b_hash)}\"\n)\n</pre> print(     f\"Synthetic population A of size {len(a)} has novelty of {creativity.novelty(observed_hash, a, a_hash)}\" ) print(     f\"Synthetic population B of size {len(b)} has novelty of {creativity.novelty(observed_hash, b, b_hash)}\" ) <pre>Synthetic population A of size 512 has novelty of 0.0\nSynthetic population B of size 1024 has novelty of 0.67\n</pre> <p>In our example, population A is sampled from the observed population and therefore has 0 novelty. Population B is sampled from the same larger population (intended to represent the \"real\" population) as the observed so has some novelty.</p>"},{"location":"examples/4_creativity/#creativity","title":"Creativity\u00b6","text":"<p>In addition to correctness we also desire a model to have creativity.</p> <p>Consider a model that generates a new population by sampling randomly from the training population. This model would have excellent or even perfect correctness. However this model would be unable to generate up-sampled populations of sequences without sampling the same data multiple times. More generally this model would be unable to generate sequences not seen in the training data, which in practice is only a small sample of the true population.</p> <p>We therefore also measure and value creativity, which we define as a combination of diversity and novelty.</p>"},{"location":"examples/4_creativity/#diversity","title":"Diversity\u00b6","text":"<p>We consider the diversity of the populations of sequences by counting the number of unique sequences as a proportion of the total number of sequences. We can compare the diversity of two populations in this manner. We consider higher diversity to be good. Ideally as diverse or more so than the training population.</p>"},{"location":"examples/4_creativity/#novelty","title":"Novelty\u00b6","text":"<p>We consider the novelty of a model as how well it can generate sequences not observed in the training population. We measure novelty by counting the number of unique sequences not seen in the training population as a proportion of the total number of sequences in the population. By only considering unique sequences we are are effectively combining our measure of diversity with novelty, and we more generally refer to this metric as creativity.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":""},{"location":"CHANGELOG/#removed","title":"Removed","text":""},{"location":"CHANGELOG/#v010-2023-10-04","title":"[v0.1.0] - 2023-10-04","text":"<p>Initial release.</p>"},{"location":"reference/caveat/data/loader/","title":"caveat.data.loader","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule","title":"<code>DataModule(data, val_split=0.1, train_batch_size=128, val_batch_size=128, test_batch_size=128, num_workers=0, pin_memory=False, **kwargs)</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Torch DataModule.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Data</p> <p> TYPE: <code>Dataset</code> </p> <code>val_split</code> <p>description. Defaults to 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>train_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>val_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>test_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>num_workers</code> <p>description. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>pin_memory</code> <p>description. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>caveat/data/loader.py</code> <pre><code>def __init__(\n    self,\n    data: Dataset,\n    val_split: float = 0.1,\n    train_batch_size: int = 128,\n    val_batch_size: int = 128,\n    test_batch_size: int = 128,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    **kwargs,\n):\n    \"\"\"Torch DataModule.\n\n    Args:\n        data (Dataset): Data\n        val_split (float, optional): _description_. Defaults to 0.1.\n        train_batch_size (int, optional): _description_. Defaults to 128.\n        val_batch_size (int, optional): _description_. Defaults to 128.\n        test_batch_size (int, optional): _description_. Defaults to 128.\n        num_workers (int, optional): _description_. Defaults to 0.\n        pin_memory (bool, optional): _description_. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.data = data\n    self.val_split = val_split\n    self.train_batch_size = train_batch_size\n    self.val_batch_size = val_batch_size\n    self.test_batch_size = test_batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n    self.mapping = None\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.data","title":"<code>data = data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.mapping","title":"<code>mapping = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.num_workers","title":"<code>num_workers = num_workers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.pin_memory","title":"<code>pin_memory = pin_memory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.test_batch_size","title":"<code>test_batch_size = test_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.train_batch_size","title":"<code>train_batch_size = train_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_batch_size","title":"<code>val_batch_size = val_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_split","title":"<code>val_split = val_split</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.setup","title":"<code>setup(stage=None)</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n        self.data, [1 - self.val_split, self.val_split]\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def test_dataloader(self) -&gt; Union[DataLoader, list[DataLoader]]:\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.train_batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def val_dataloader(self) -&gt; Union[DataLoader, list[DataLoader]]:\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.val_batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.predict_dataloader","title":"<code>predict_dataloader(num_samples, latent_dim, batch_size=256, num_workers=4)</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def predict_dataloader(\n    num_samples, latent_dim, batch_size: int = 256, num_workers: int = 4\n):\n    z = torch.randn(num_samples, latent_dim)\n    return DataLoader(\n        z,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/","title":"caveat.data.synth","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen","title":"<code>ActivityGen()</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def __init__(self):\n    self.map = {i: s for i, s in enumerate(self.possible_states)}\n    self.steps = self.duration // self.step_size\n    self.transition_weights = None\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.duration","title":"<code>duration = 24 * 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.initial_state","title":"<code>initial_state = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.map","title":"<code>map = {i: sfor (i, s) in enumerate(self.possible_states)}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_sensitivity","title":"<code>max_duration_sensitivity = np.array([0.1, 0.1, 0.1, 0.1, 0.1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_tollerance","title":"<code>max_duration_tollerance = np.array([12 * 60, 6 * 60, 60, 360, 120])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_sensitivity","title":"<code>min_duration_sensitivity = np.array([1, 1.2, 1, 1.2, 1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_tollerance","title":"<code>min_duration_tollerance = np.array([180, 420, 60, 120, 60])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.pivot_adjustment","title":"<code>pivot_adjustment = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.possible_states","title":"<code>possible_states = ['home', 'work', 'shop', 'education', 'leisure']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repetition_sensitivity","title":"<code>repetition_sensitivity = np.array([1, 2, 1, 2, 1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repetition_tollerance","title":"<code>repetition_tollerance = np.array([10, 1, 1, 1, 2])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.step_size","title":"<code>step_size = 15</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.steps","title":"<code>steps = self.duration // self.step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_config","title":"<code>transition_config = {'home': {'home': [(0, 100), (5, 100), (11, 0.1), (23, 100), (24, 100)], 'work': [(0, 0), (6, 0), (9, 0.2), (11, 0.1), (17, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 2), (11, 1), (20, 0), (24, 0)], 'education': [(0, 0), (7.5, 0), (8.5, 5), (11, 0.01), (17, 0.01), (20, 0), (24, 0)], 'leisure': [(0, 0), (6, 0), (9, 2), (16, 0.1), (22, 0), (24, 0)]}, 'work': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.5), (17, 1), (24, 100)], 'work': [(0, 100), (12, 100), (20, 0), (24, 0)], 'shop': [(0, 0), (12, 0), (13, 0.1), (14, 0), (18, 0.1), (19, 0), (24, 0)], 'education': [(0, 0), (12, 0), (13, 0.1), (14, 0), (16, 0), (17, 0.1), (19, 0), (24, 0)], 'leisure': [(0, 0), (15, 0), (16, 0.1), (17, 0.2), (24, 0)]}, 'shop': {'home': [(0, 0.3), (23, 1), (24, 1)], 'work': [(0, 0.1), (14, 0.1), (15, 0), (24, 0)], 'shop': [(0, 10), (15, 10), (16, 0), (24, 0)], 'education': [(0, 0.1), (15, 0), (24, 0)], 'leisure': [(0, 0.2), (15, 0), (24, 0)]}, 'education': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.1), (17, 100), (24, 100)], 'work': [(0, 0), (12, 1), (15, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 0.1), (11, 0.3), (23, 0), (24, 0)], 'education': [(0, 100), (12, 100), (17, 100), (18, 0), (24, 0)], 'leisure': [(0, 0), (6, 0), (9, 0.1), (16, 0.1), (17, 0), (24, 0)]}, 'leisure': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.1), (17, 100), (24, 100)], 'work': [(0, 1), (12, 1), (23, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 0.1), (11, 0.3), (23, 0), (24, 0)], 'education': [(0, 0), (24, 0)], 'leisure': [(0, 100), (19, 100), (23, 0), (24, 0)]}}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_weights","title":"<code>transition_weights = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.build","title":"<code>build()</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def build(self):\n    num_states = len(self.possible_states)\n    self.transition_weights = np.zeros((num_states, num_states, self.steps))\n    for i in range(num_states):\n        in_state = self.possible_states[i]\n        state_transitions = self.transition_config[in_state]\n        for j in range(num_states):\n            out_state = self.possible_states[j]\n            pivots = state_transitions[out_state]\n            self.transition_weights[i][j] = interpolate_from_pivots(\n                pivots, self.steps, self.pivot_adjustment, self.step_size\n            )\n\n    self.transition_weights = np.transpose(\n        self.transition_weights, (0, 2, 1)\n    )  # ie [in_state, minute, out_state]\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_adjustment","title":"<code>max_duration_adjustment(activity_durations)</code>","text":"<p>Penalise current activity based on duration.</p> PARAMETER  DESCRIPTION <code>activity_durations</code> <p>activity durations</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def max_duration_adjustment(self, activity_durations: np.array) -&gt; np.array:\n    \"\"\"Penalise current activity based on duration.\n\n    Args:\n        activity_durations (np.array): activity durations\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return 1 / (\n        np.clip((activity_durations - self.max_duration_tollerance), 1, None)\n        ** self.max_duration_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_adjustment","title":"<code>min_duration_adjustment(activity_durations)</code>","text":"<p>Penalise current activity based on duration.</p> PARAMETER  DESCRIPTION <code>activity_durations</code> <p>activity durations</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def min_duration_adjustment(self, activity_durations: np.array) -&gt; np.array:\n    \"\"\"Penalise current activity based on duration.\n\n    Args:\n        activity_durations (np.array): activity durations\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return (\n        np.clip(((self.min_duration_tollerance - activity_durations)), 1, None)\n        ** self.min_duration_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repeat_adjustment","title":"<code>repeat_adjustment(activity_counts)</code>","text":"<p>Penalise activities based on how often they have been done.</p> PARAMETER  DESCRIPTION <code>activity_counts</code> <p>counts of activity repetitions</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def repeat_adjustment(self, activity_counts: np.array) -&gt; np.array:\n    \"\"\"Penalise activities based on how often they have been done.\n\n    Args:\n        activity_counts (np.array): counts of activity repetitions\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return 1 / (\n        np.clip((activity_counts - self.repetition_tollerance), 1, None)\n        ** self.repetition_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.run","title":"<code>run()</code>","text":"<p>summary</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def run(self):\n    \"\"\"_summary_\"\"\"\n    trace = []  # [(act, start, end, dur), (act, start, end, dur), ...]\n    state = self.initial_state\n    activity_counts = np.zeros((len(self.possible_states)))\n    activity_counts[state] += 1\n    activity_durations = np.zeros((len(self.possible_states)))\n    activity_durations[state] += self.step_size\n\n    for step in range(1, self.steps):\n        new_state = np.random.choice(\n            len(self.possible_states),\n            p=self.transition_probabilities(state, step, activity_counts, activity_durations),\n        )\n        if new_state != state:\n            time = step * self.step_size\n            if not trace:  # first transition\n                prev_end = 0\n            else:\n                prev_end = trace[-1][2]\n            trace.append((state, prev_end, time, time - prev_end))\n\n            # update state\n            state = new_state\n            activity_counts[state] += 1\n            activity_durations[state] = 0  # reset\n\n        activity_durations[state] += self.step_size\n\n    # close\n    prev_end = trace[-1][2]\n    trace.append((state, prev_end, self.duration, self.duration - prev_end))\n    return trace\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_probabilities","title":"<code>transition_probabilities(state, step, activity_counts, activity_durations)</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def transition_probabilities(\n    self, state, step, activity_counts: np.array, activity_durations: np.array\n):\n    p = self.transition_weights[state][step]\n    p = (\n        p\n        * self.repeat_adjustment(activity_counts)\n        * self.min_duration_adjustment(activity_durations)\n        * self.max_duration_adjustment(activity_durations)\n    )\n    return p / sum(p)\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.interpolate_from_pivots","title":"<code>interpolate_from_pivots(pivots, size=1440, pivot_adjustment=60, step_size=1)</code>","text":"<p>Create a descretised array of shape 'size' based on given 'pivots'.</p> PARAMETER  DESCRIPTION <code>pivots</code> <p>description</p> <p> TYPE: <code>list[tuple[float, float]]</code> </p> <code>size</code> <p>description. Defaults to 1440</p> <p> TYPE: <code>int</code> DEFAULT: <code>1440</code> </p> <code>pivot_adjustment</code> <p>description. Defaults to 60</p> <p> TYPE: <code>int</code> DEFAULT: <code>60</code> </p> <code>step_size</code> <p>Defaults to 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: bins</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def interpolate_from_pivots(\n    pivots: list[tuple[float, float]],\n    size: int = 1440,\n    pivot_adjustment: int = 60,\n    step_size: int = 1,\n) -&gt; np.array:\n    \"\"\"Create a descretised array of shape 'size' based on given 'pivots'.\n\n    Args:\n        pivots (list[tuple[float, float]]): _description_\n        size (int, optional): _description_. Defaults to 1440\n        pivot_adjustment (int, optional): _description_. Defaults to 60\n        step_size (int, optional): Defaults to 1\n\n    Returns:\n        np.array: bins\n    \"\"\"\n    bins = np.zeros((size), dtype=np.float64)\n    for k in range(len(pivots) - 1):\n        a_pivot, a_value = pivots[k]\n        b_pivot, b_value = pivots[k + 1]\n        a_pivot = int(a_pivot * pivot_adjustment / step_size)\n        b_pivot = int(b_pivot * pivot_adjustment / step_size)\n        a = (a_pivot, a_value)\n        b = (b_pivot, b_value)\n        bins[slice(a_pivot, b_pivot)] = interpolate_pivot(a, b)\n    return bins\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.interpolate_pivot","title":"<code>interpolate_pivot(a, b)</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def interpolate_pivot(a: tuple[int, float], b: tuple[int, float]) -&gt; np.array:\n    a_pivot, a_value = a\n    b_pivot, b_value = b\n    return np.linspace(a_value, b_value, abs(b_pivot - a_pivot), endpoint=False)\n</code></pre>"},{"location":"reference/caveat/data/utils/","title":"caveat.data.utils","text":""},{"location":"reference/caveat/data/utils/#caveat.data.utils.gen_person","title":"<code>gen_person(gen, pid)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def gen_person(gen, pid) -&gt; pd.DataFrame:\n    trace = gen.run()\n    return trace_to_df(trace, pid=pid)\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.gen_persons","title":"<code>gen_persons(gen, pids)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def gen_persons(gen, pids) -&gt; pd.DataFrame:\n    return pd.concat([gen_person(gen, pid) for pid in pids], ignore_index=True)\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.generate_population","title":"<code>generate_population(gen, size, cores=None)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def generate_population(gen, size: int, cores: int = None):\n    if cores is None:\n        cores = mp.cpu_count()\n\n    batches = list(split(range(size), cores))\n\n    pools = mp.Pool(cores)\n    results = [pools.apply_async(gen_persons, args=(gen, pids)) for pids in batches]\n    pools.close()\n    pools.join()\n    results = [r.get() for r in results]\n    pop = pd.concat(results, ignore_index=True)\n    return pop\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.split","title":"<code>split(a, n)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.trace_to_df","title":"<code>trace_to_df(trace, **kwargs)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def trace_to_df(trace: list[tuple], **kwargs) -&gt; pd.DataFrame:\n    df = pd.DataFrame(trace, columns=[\"act\", \"start\", \"end\", \"duration\"])\n    for k, v in kwargs.items():\n        df[k] = v\n    return df\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.trace_to_pam","title":"<code>trace_to_pam(trace, mapping)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def trace_to_pam(trace: list[tuple], mapping: dict):\n    plan = Plan()\n    for act, start, end, duration in trace:\n        name = mapping[act]\n        plan.add(Activity(act=name, start_time=mtdt(start), end_time=mtdt(end)))\n        plan.add(Trip(mode=\"car\", start_time=mtdt(end), end_time=mtdt(end)))\n    return plan\n</code></pre>"},{"location":"reference/caveat/data/validate/","title":"caveat.data.validate","text":""},{"location":"reference/caveat/data/validate/#caveat.data.validate.load_and_validate","title":"<code>load_and_validate(data_path)</code>","text":"Source code in <code>caveat/data/validate.py</code> <pre><code>def load_and_validate(data_path: Path) -&gt; pd.DataFrame:\n    data = pd.read_csv(data_path)\n    if data.empty:\n        raise UserWarning(f\"No data found in {data_path}.\")\n    validate(data)\n    return data\n</code></pre>"},{"location":"reference/caveat/data/validate/#caveat.data.validate.validate","title":"<code>validate(data)</code>","text":"Source code in <code>caveat/data/validate.py</code> <pre><code>def validate(data: pd.DataFrame):\n    required_cols = {\"pid\", \"act\", \"start\", \"end\"}\n    found = set(data.columns)\n    missing = required_cols - found\n    if missing:\n        raise UserWarning(\n            f\"\"\"\n    Input data is missing required columns.\n    Required: {required_cols}.\n    Found: {found}.\n    Please add missing: {missing}.\n    \"\"\"\n        )\n    data.act = data.act.astype(\"category\")\n    data.start = data.start.astype(\"int\")\n    data.end = data.end.astype(\"int\")\n\n    data[\"duration\"] = data.end - data.start\n</code></pre>"},{"location":"reference/caveat/describe/features/","title":"caveat.describe.features","text":""},{"location":"reference/caveat/describe/features/#caveat.describe.features.actual","title":"<code>actual(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def actual(features: dict[str, float]) -&gt; Series:\n    return Series(features)\n</code></pre>"},{"location":"reference/caveat/describe/features/#caveat.describe.features.average","title":"<code>average(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def average(features: dict[str, tuple[ndarray, ndarray]]) -&gt; Series:\n    return Series(\n        {\n            k: np.average(v, axis=0, weights=w).sum()\n            for k, (v, w) in features.items()\n        }\n    )\n</code></pre>"},{"location":"reference/caveat/describe/features/#caveat.describe.features.average2d","title":"<code>average2d(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def average2d(features: dict[str, tuple[ndarray, ndarray]]) -&gt; Series:\n    return Series(\n        {\n            k: np.average(v, axis=0, weights=w).sum().sum()\n            for k, (v, w) in features.items()\n        }\n    )\n</code></pre>"},{"location":"reference/caveat/describe/features/#caveat.describe.features.average_weight","title":"<code>average_weight(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def average_weight(features: dict[str, ndarray]) -&gt; Series:\n    return Series({k: w.mean() for k, (v, w) in features.items()})\n</code></pre>"},{"location":"reference/caveat/describe/features/#caveat.describe.features.feature_length","title":"<code>feature_length(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def feature_length(features: dict[str, ndarray]) -&gt; Series:\n    return Series({k: len(v) for k, (v, w) in features.items()})\n</code></pre>"},{"location":"reference/caveat/describe/features/#caveat.describe.features.feature_weight","title":"<code>feature_weight(features)</code>","text":"Source code in <code>caveat/describe/features.py</code> <pre><code>def feature_weight(features: dict[str, ndarray]) -&gt; Series:\n    return Series({k: w.sum() for k, (v, w) in features.items()})\n</code></pre>"},{"location":"reference/caveat/describe/frequency/","title":"caveat.describe.frequency","text":""},{"location":"reference/caveat/describe/frequency/#caveat.describe.frequency.frequency_plots","title":"<code>frequency_plots(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/describe/frequency.py</code> <pre><code>def frequency_plots(\n        observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n):\n    if ys is None:\n        ys = dict()\n    acts = list(observed.act.value_counts(ascending=False).index)\n    class_map = {n: i for i, n in enumerate(acts)}\n\n    fig, axs = plt.subplots(\n        sharex=True,\n        sharey=True,\n        nrows=1, ncols=len(ys) + 1,\n        constrained_layout=True,\n        figsize=kwargs.pop(\"figsize\", (15, 4))\n        )\n\n    if not ys:\n        ax = axs\n    else:\n        ax = axs[0]\n\n    plot_agg_acts(\"observed\", observed, class_map, ax=ax, legend=True)\n\n    # now deal with ys\n    for i, (name, y) in enumerate(ys.items()):\n        ax = axs[i + 1]\n        plot_agg_acts(name, y, class_map, ax=ax, legend=False)\n\n    return fig\n</code></pre>"},{"location":"reference/caveat/describe/frequency/#caveat.describe.frequency.plot_agg_acts","title":"<code>plot_agg_acts(name, population, class_map, duration=1440, step=10, ax=None, legend=True)</code>","text":"Source code in <code>caveat/describe/frequency.py</code> <pre><code>def plot_agg_acts(\n    name: str, population: DataFrame, class_map: dict, duration: int = 1440, step: int = 10, ax = None, legend=True\n):\n    bins = activity_bins(\n        population, duration=duration, step=step, class_map=class_map\n    )\n    columns = list(class_map.keys())\n    totals = bins.sum(0)\n    sorted_cols = [x for _, x in sorted(zip(totals, columns))]\n    df = DataFrame(bins, columns=columns)[sorted_cols]\n    df.index = [\n        datetime(2021, 11, 1, 0) + timedelta(minutes=i * step)\n        for i in range(len(df.index))\n    ]\n    fig = df.plot(kind=\"bar\", stacked=True, width=1, ax=ax, legend=legend)\n    ax = fig.axes\n    labels = [\" \" for _ in range(len(df.index))]\n    labels[:: int(60 / step)] = [x.strftime(\"%H:%M\") for x in df.index][\n        :: int(60 / step)\n    ]\n    ax.set_xticklabels(labels)\n    ax.set_title(name.title(), fontstyle=\"italic\")\n    return ax\n</code></pre>"},{"location":"reference/caveat/describe/times/","title":"caveat.describe.times","text":""},{"location":"reference/caveat/describe/times/#caveat.describe.times.joint_time_distributions_plot","title":"<code>joint_time_distributions_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/describe/times.py</code> <pre><code>def joint_time_distributions_plot(\n    observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n) -&gt; Figure:\n    if ys is None:\n        ys = dict()\n    acts = list(observed.act.value_counts(ascending=False).index)\n\n    fig = plt.figure(\n        constrained_layout=True, figsize=kwargs.pop(\"figsize\", (15, 4))\n    )\n\n    subfigs = fig.subfigures(nrows=len(ys) + 1, ncols=1)\n    # deal with observed first\n    if not ys:\n        subfig = subfigs\n    else:\n        subfig = subfigs[0]\n    subfig.suptitle(\"Observed\", fontstyle=\"italic\")\n    axs = subfig.subplots(nrows=1, ncols=len(acts), sharex=True, sharey=True)\n    _joint_time_plot(\"obseved\", observed, axs, acts)\n\n    # act column titles\n    for ax, act in zip(axs, acts):\n        ax.set_title(act.title(), fontsize=\"large\")\n\n    # now deal with ys\n    for i, (name, y) in enumerate(ys.items()):\n        subfig = subfigs[i + 1]\n        subfig.suptitle(name.title(), fontstyle=\"italic\")\n        axs = subfig.subplots(\n            nrows=1, ncols=len(acts), sharex=True, sharey=True\n        )\n        _joint_time_plot(name, y, axs, acts)\n\n    # xlabel on bottom row\n    for ax in axs:\n        ax.set(xlabel=\"Start times\\n(minutes)\")\n\n    return fig\n</code></pre>"},{"location":"reference/caveat/describe/times/#caveat.describe.times.times_distributions_plot","title":"<code>times_distributions_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/describe/times.py</code> <pre><code>def times_distributions_plot(\n    observed: DataFrame, ys: Optional[dict[str, DataFrame]], **kwargs\n) -&gt; Figure:\n    fig, axs = plt.subplots(\n        3,\n        observed.act.nunique(),\n        figsize=kwargs.pop(\"figsize\", (12, 5)),\n        sharex=True,\n        sharey=False,\n        tight_layout=True,\n    )\n    acts = list(observed.act.value_counts(ascending=False).index)\n    _times_plot(\"observed\", observed, acts, axs=axs)\n    if ys is None:\n        return fig\n    for name, y in ys.items():\n        _times_plot(name, y, acts, axs=axs)\n    return fig\n</code></pre>"},{"location":"reference/caveat/describe/transitions/","title":"caveat.describe.transitions","text":""},{"location":"reference/caveat/describe/transitions/#caveat.describe.transitions.sequence_prob_plot","title":"<code>sequence_prob_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/describe/transitions.py</code> <pre><code>def sequence_prob_plot(\n    observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n) -&gt; Figure:\n    acts = list(observed.act.value_counts(ascending=False).index)\n    if kwargs.pop(\"cmap\", None) is None:\n        cmap = plt.cm.Set3\n    colors = cmap.colors\n    factor = (len(acts) // len(colors)) + 1\n    cmap = dict(zip(acts, colors * factor))\n\n    n_plots = len(ys) + 2\n    ratios = [1 for _ in range(n_plots)]\n    ratios[-1] = 0.5\n\n    fig, axs = plt.subplots(\n        1,\n        n_plots,\n        figsize=kwargs.pop(\"figsize\", (12, 5)),\n        sharex=True,\n        sharey=True,\n        tight_layout=True,\n        gridspec_kw={\"width_ratios\": ratios},\n    )\n    acts = list(observed.act.value_counts(ascending=False).index)\n    _probs_plot(observed, acts, ax=axs[0], cmap=cmap)\n    axs[0].set_title(\"Observed\", fontstyle=\"italic\")\n    if ys is None:\n        return fig\n    for i, (name, y) in enumerate(ys.items()):\n        _probs_plot(y, acts, ax=axs[i + 1], cmap=cmap)\n        axs[i + 1].set_title(name.title(), fontstyle=\"italic\")\n\n    elements = [Patch(facecolor=cmap[act], label=act.title()) for act in acts]\n    axs[-1].axis(\"off\")\n    axs[-1].legend(handles=elements, loc=\"center left\", frameon=False)\n\n    return fig\n</code></pre>"},{"location":"reference/caveat/distance/scalar/","title":"caveat.distance.scalar","text":""},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.abs_av_diff","title":"<code>abs_av_diff(a, b)</code>","text":"Source code in <code>caveat/distance/scalar.py</code> <pre><code>def abs_av_diff(\n    a: tuple[np.ndarray, np.ndarray], b: tuple[np.ndarray, np.ndarray]\n) -&gt; float:\n    # TODO test this\n    # unpack\n    ak, aw = a\n    bk, bw = b\n    a_average = (ak * aw).sum() / aw.sum()\n    b_average = (bk * bw).sum() / bw.sum()\n\n    return np.abs(a_average - b_average)\n</code></pre>"},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.clamp","title":"<code>clamp(x)</code>","text":"Source code in <code>caveat/distance/scalar.py</code> <pre><code>def clamp(x):\n    if x &gt; 1.0:\n        return 1.0\n    return x\n</code></pre>"},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.mae","title":"<code>mae(a, b)</code>","text":"Source code in <code>caveat/distance/scalar.py</code> <pre><code>def mae(\n    a: tuple[np.ndarray, np.ndarray], b: tuple[np.ndarray, np.ndarray]\n) -&gt; float:\n    # TODO test this\n    # requires and b have same support.\n    # unpack\n    _, aw = a\n    _, bw = b\n    return (np.abs(aw - bw)).mean()\n</code></pre>"},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.mape","title":"<code>mape(a, b)</code>","text":"<p>Calculate mean average percentage error between distributions a and b.</p> <p>Clipped at 1.0.</p> PARAMETER  DESCRIPTION <code>a</code> <p>Distribution a.</p> <p> TYPE: <code>tuple[ndarray, ndarray]</code> </p> <code>b</code> <p>Distribution b.</p> <p> TYPE: <code>tuple[ndarray, ndarray]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>MAPE.</p> <p> TYPE: <code>float</code> </p> Source code in <code>caveat/distance/scalar.py</code> <pre><code>def mape(\n    a: tuple[np.ndarray, np.ndarray], b: tuple[np.ndarray, np.ndarray]\n) -&gt; float:\n    \"\"\"Calculate mean average percentage error between distributions a and b.\n\n    Clipped at 1.0.\n\n    Args:\n        a (tuple[np.ndarray, np.ndarray]): Distribution a.\n        b (tuple[np.ndarray, np.ndarray]): Distribution b.\n\n    Returns:\n        float: MAPE.\n    \"\"\"\n    # TODO test this\n    # unpack\n    ak, aw = a\n    bk, bw = b\n    # calc weighted average\n    akw = (ak * aw).sum() / aw.sum()\n    bkw = (bk * bw).sum() / bw.sum()\n    diff = np.abs(akw - bkw)\n    if diff == 0:\n        return 0.0\n    if bkw == 0:\n        return clamp(diff / akw)\n    return clamp(diff / bkw)\n</code></pre>"},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.mape_scalar","title":"<code>mape_scalar(a, b)</code>","text":"Source code in <code>caveat/distance/scalar.py</code> <pre><code>def mape_scalar(a, b):\n    return np.abs((a - b) / a).mean()\n</code></pre>"},{"location":"reference/caveat/distance/scalar/#caveat.distance.scalar.mse","title":"<code>mse(a, b)</code>","text":"Source code in <code>caveat/distance/scalar.py</code> <pre><code>def mse(\n    a: tuple[np.ndarray, np.ndarray], b: tuple[np.ndarray, np.ndarray]\n) -&gt; float:\n    # requires and b have same support.\n    # unpack\n    _, aw = a\n    _, bw = b\n    return ((aw - bw) ** 2).mean()\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/","title":"caveat.distance.wasserstein","text":""},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance","title":"<code>SinkhornDistance(eps, max_iter, reduction='none')</code>","text":"<p>             Bases: <code>Module</code></p> <p>https://dfdazac.github.io/sinkhorn.html Given two empirical measures each with :math:<code>P_1</code> locations :math:<code>x\\in\\mathbb{R}^{D_1}</code> and :math:<code>P_2</code> locations :math:<code>y\\in\\mathbb{R}^{D_2}</code>, outputs an approximation of the regularized OT cost for point clouds.</p> PARAMETER  DESCRIPTION <code>eps</code> <p>regularization coefficient</p> <p> TYPE: <code>float</code> </p> <code>max_iter</code> <p>maximum number of Sinkhorn iterations</p> <p> TYPE: <code>int</code> </p> <code>reduction</code> <p>Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Default: 'none'</p> <p> TYPE: <code>string</code> DEFAULT: <code>'none'</code> </p> Shape <ul> <li>Input: :math:<code>(N, P_1, D_1)</code>, :math:<code>(N, P_2, D_2)</code></li> <li>Output: :math:<code>(N)</code> or :math:<code>()</code>, depending on <code>reduction</code></li> </ul> Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def __init__(self, eps, max_iter, reduction=\"none\"):\n    super(SinkhornDistance, self).__init__()\n    self.eps = eps\n    self.max_iter = max_iter\n    self.reduction = reduction\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.eps","title":"<code>eps = eps</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.max_iter","title":"<code>max_iter = max_iter</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.reduction","title":"<code>reduction = reduction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.M","title":"<code>M(C, u, v)</code>","text":"<p>Modified cost for logarithmic updates</p> Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def M(self, C, u, v):\n    \"Modified cost for logarithmic updates\"\n    \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n    return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.ave","title":"<code>ave(u, u1, tau)</code>  <code>staticmethod</code>","text":"<p>Barycenter subroutine, used by kinetic acceleration through extrapolation.</p> Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>@staticmethod\ndef ave(u, u1, tau):\n    \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n    return tau * u + (1 - tau) * u1\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.SinkhornDistance.forward","title":"<code>forward(x, y)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def forward(self, x, y):\n    # The Sinkhorn algorithm takes as input three variables :\n    print(\"calc cost matrix\")\n    C = self._cost_matrix(x, y)  # Wasserstein cost function\n    x_points = x.shape[-2]\n    y_points = y.shape[-2]\n    if x.dim() == 2:\n        batch_size = 1\n    else:\n        batch_size = x.shape[0]\n\n    # both marginals are fixed with equal weights\n    print(\"mu\")\n    mu = (\n        torch.empty(\n            batch_size, x_points, dtype=torch.float, requires_grad=False\n        )\n        .fill_(1.0 / x_points)\n        .squeeze()\n    )\n    print(\"nu\")\n    nu = (\n        torch.empty(\n            batch_size, y_points, dtype=torch.float, requires_grad=False\n        )\n        .fill_(1.0 / y_points)\n        .squeeze()\n    )\n\n    u = torch.zeros_like(mu)\n    v = torch.zeros_like(nu)\n    # To check if algorithm terminates because of threshold\n    # or max iterations reached\n    actual_nits = 0\n    # Stopping criterion\n    thresh = 1e-1\n\n    # Sinkhorn iterations\n    for i in range(self.max_iter):\n        print(i, \"start\")\n        u1 = u  # useful to check the update\n        u = (\n            self.eps\n            * (\n                torch.log(mu + 1e-8)\n                - torch.logsumexp(self.M(C, u, v), dim=-1)\n            )\n            + u\n        )\n        v = (\n            self.eps\n            * (\n                torch.log(nu + 1e-8)\n                - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)\n            )\n            + v\n        )\n        err = (u - u1).abs().sum(-1).mean()\n        print(i, err.item())\n\n        actual_nits += 1\n        if err.item() &lt; thresh:\n            print(f\"Sinkhorn converged at iteration {i}\")\n            break\n\n    U, V = u, v\n    # Transport plan pi = diag(a)*K*diag(b)\n    pi = torch.exp(self.M(C, U, V))\n    # Sinkhorn distance\n    cost = torch.sum(pi * C, dim=(-2, -1))\n\n    if self.reduction == \"mean\":\n        cost = cost.mean()\n    elif self.reduction == \"sum\":\n        cost = cost.sum()\n\n    return cost, pi, C\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.emd","title":"<code>emd(a, b)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def emd(\n    a: tuple[np.array, np.array], b: tuple[np.array, np.array, int]\n) -&gt; float:\n    if a[0].ndim == 1:\n        return emd1d(a, b)\n    elif a[0].ndim == 2:\n        return emd2d(a, b)\n    else:\n        raise ValueError(\"Only 1d and 2d features are supported\")\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.emd1d","title":"<code>emd1d(a, b)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def emd1d(\n    a: tuple[np.array, np.array], b: tuple[np.array, np.array, int]\n) -&gt; float:\n    ak, aw = a\n    bk, bw = b\n    aw = aw / aw.sum()\n    bw = bw / bw.sum()\n    return emd2_1d(ak, bk, aw, bw, metric=\"cityblock\")\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.emd2d","title":"<code>emd2d(a, b)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def emd2d(\n    a: tuple[np.array, np.array], b: tuple[np.array, np.array, int]\n) -&gt; float:\n    ak, aw = a\n    bk, bw = b\n    aw = aw / aw.sum()\n    bw = bw / bw.sum()\n    d = dist(ak, bk, metric=\"cityblock\")\n    return emd2(aw, bw, d, check_marginals=False)\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.sinkhorn","title":"<code>sinkhorn(x, y, eps=0.01, max_iter=10, reduction=None)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def sinkhorn(\n    x: list[list], y: list[list], eps=0.01, max_iter=10, reduction=None\n):\n    x = torch.tensor(x, dtype=torch.float)\n    y = torch.tensor(y, dtype=torch.float)\n    model = SinkhornDistance(eps=eps, max_iter=max_iter, reduction=reduction)\n    return model(x, y)[0].item()\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.sliced_wasserstein","title":"<code>sliced_wasserstein(x, y, num_proj=100)</code>","text":"<p>https://stats.stackexchange.com/questions/404775/calculate-earth-movers-distance-for-two-grayscale-images</p> Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def sliced_wasserstein(x: list[list], y: list[list], num_proj=100):\n    \"\"\"\n    https://stats.stackexchange.com/questions/404775/calculate-earth-movers-distance-for-two-grayscale-images\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    dim = x.shape[1]\n    ests = []\n    for _ in range(num_proj):\n        # sample uniformly from the unit sphere\n        dir = np.random.randn(dim)\n        dir /= np.linalg.norm(dir)\n\n        # project the data\n        x_proj = x @ dir\n        y_proj = y @ dir\n\n        # compute 1d wasserstein\n        ests.append(wasserstein_distance(x_proj, y_proj))\n    return np.mean(ests)\n</code></pre>"},{"location":"reference/caveat/distance/wasserstein/#caveat.distance.wasserstein.wasserstein","title":"<code>wasserstein(x, y)</code>","text":"Source code in <code>caveat/distance/wasserstein.py</code> <pre><code>def wasserstein(x: list[list], y: list[list]):\n    return wasserstein_distance(x, y)\n</code></pre>"},{"location":"reference/caveat/encoders/base/","title":"caveat.encoders.base","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncodedPlans","title":"<code>BaseEncodedPlans()</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Base encoded sequence Dataset.</p> Source code in <code>caveat/encoders/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Base encoded sequence Dataset.\"\"\"\n    super(BaseEncodedPlans, self).__init__()\n    self.encodings: int\n    self.encoding_weights: Tensor\n    self.masks: Tensor\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncodedPlans.encoding_weights","title":"<code>encoding_weights: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncodedPlans.encodings","title":"<code>encodings: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncodedPlans.masks","title":"<code>masks: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncodedPlans.shape","title":"<code>shape()</code>","text":"Source code in <code>caveat/encoders/base.py</code> <pre><code>def shape(self):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder","title":"<code>BaseEncoder()</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>caveat/encoders/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    super(BaseEncoder, self).__init__()\n    self.encodings = None\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder.encodings","title":"<code>encodings = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder.decode","title":"<code>decode(encoded)</code>","text":"Source code in <code>caveat/encoders/base.py</code> <pre><code>def decode(self, encoded: Tensor) -&gt; DataFrame:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder.encode","title":"<code>encode(input)</code>","text":"Source code in <code>caveat/encoders/base.py</code> <pre><code>def encode(self, input: DataFrame) -&gt; BaseEncodedPlans:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/","title":"caveat.encoders.descrete","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans","title":"<code>DescreteEncodedPlans(data, duration, step_size, class_map)</code>","text":"<p>             Bases: <code>BaseEncodedPlans</code></p> <p>Torch Dataset for descretised sequence data.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Population of sequences.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def __init__(\n    self, data: pd.DataFrame, duration: int, step_size: int, class_map: dict\n):\n    \"\"\"Torch Dataset for descretised sequence data.\n\n    Args:\n        data (Tensor): Population of sequences.\n    \"\"\"\n    self.encodings = data.act.nunique()\n    # calc weightings\n    weights = data.groupby(\"act\", observed=True).duration.sum().to_dict()\n    weights = np.array([weights[k] for k in sorted(weights.keys())])\n    self.encoding_weights = torch.from_numpy(1 / weights).float()\n    self.encoded = descretise_population(\n        data, duration=duration, step_size=step_size, class_map=class_map\n    )\n    self.mask = torch.ones((1, self.encoded.shape[-1]))\n    self.size = len(self.encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.encoded","title":"<code>encoded = descretise_population(data, duration=duration, step_size=step_size, class_map=class_map)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.encoding_weights","title":"<code>encoding_weights = torch.from_numpy(1 / weights).float()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.encodings","title":"<code>encodings = data.act.nunique()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.mask","title":"<code>mask = torch.ones((1, self.encoded.shape[-1]))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.size","title":"<code>size = len(self.encoded)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncodedPlans.shape","title":"<code>shape()</code>","text":"Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def shape(self):\n    return self.encoded[0].shape\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder","title":"<code>DescreteEncoder(duration=1440, step_size=10, **kwargs)</code>","text":"<p>             Bases: <code>BaseEncoder</code></p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def __init__(self, duration: int = 1440, step_size: int = 10, **kwargs):\n    self.duration = duration\n    self.step_size = step_size\n    self.steps = duration // step_size\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.duration","title":"<code>duration = duration</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.step_size","title":"<code>step_size = step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.steps","title":"<code>steps = duration // step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.decode","title":"<code>decode(encoded)</code>","text":"<p>Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:</p> <p>pid | act | start | end</p> <p>pid is taken as sample enumeration.</p> PARAMETER  DESCRIPTION <code>encoded</code> <p>description</p> <p> TYPE: <code>Tensor</code> </p> <code>mapping</code> <p>description</p> <p> TYPE: <code>dict</code> </p> <code>length</code> <p>Length of plan in minutes.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def decode(self, encoded: Tensor) -&gt; pd.DataFrame:\n    \"\"\"Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:\n\n    pid | act | start | end\n\n    pid is taken as sample enumeration.\n\n    Args:\n        encoded (Tensor): _description_\n        mapping (dict): _description_\n        length (int): Length of plan in minutes.\n\n    Returns:\n        pd.DataFrame: _description_\n    \"\"\"\n    encoded = torch.argmax(encoded, dim=-1)\n    decoded = []\n\n    for pid in range(len(encoded)):\n        current_act = None\n        act_start = 0\n\n        for step, act_idx in enumerate(encoded[pid, 0]):\n            if int(act_idx) != current_act and current_act is not None:\n                decoded.append(\n                    [\n                        pid,\n                        self.index_to_acts[current_act],\n                        int(act_start * self.step_size),\n                        int(step * self.step_size),\n                    ]\n                )\n                act_start = step\n            current_act = int(act_idx)\n        decoded.append(\n            [\n                pid,\n                self.index_to_acts[current_act],\n                int(act_start * self.step_size),\n                self.duration,\n            ]\n        )\n\n    return pd.DataFrame(decoded, columns=[\"pid\", \"act\", \"start\", \"end\"])\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.encode","title":"<code>encode(data)</code>","text":"Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def encode(self, data: pd.DataFrame) -&gt; BaseEncodedPlans:\n    self.index_to_acts = {i: a for i, a in enumerate(data.act.unique())}\n    self.acts_to_index = {a: i for i, a in self.index_to_acts.items()}\n    return DescreteEncodedPlans(\n        data,\n        duration=self.duration,\n        step_size=self.step_size,\n        class_map=self.acts_to_index,\n    )\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.descretise_population","title":"<code>descretise_population(data, duration, step_size, class_map)</code>","text":"<p>Convert given population of activity traces into vector [N, L] of classes. N is the population size. l is time steps.</p> PARAMETER  DESCRIPTION <code>data</code> <p>description</p> <p> TYPE: <code>DataFrame</code> </p> <code>duration</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>step_size</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.tensor: [N, L]</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def descretise_population(\n    data: pd.DataFrame, duration: int, step_size: int, class_map: dict\n) -&gt; torch.Tensor:\n    \"\"\"Convert given population of activity traces into vector [N, L] of classes.\n    N is the population size.\n    l is time steps.\n\n    Args:\n        data (pd.DataFrame): _description_\n        duration (int): _description_\n        step_size (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        torch.tensor: [N, L]\n    \"\"\"\n    persons = data.pid.nunique()\n    steps = duration // step_size\n    encoded = np.zeros((persons, steps), dtype=np.int8)\n\n    for pid, (_, trace) in enumerate(data.groupby(\"pid\")):\n        trace_encoding = descretise_trace(\n            acts=trace.act,\n            starts=trace.start,\n            ends=trace.end,\n            length=duration,\n            class_map=class_map,\n        )\n        trace_encoding = down_sample(trace_encoding, step_size)\n        encoded[pid] = trace_encoding  # [N, L]\n    return torch.from_numpy(encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.descretise_trace","title":"<code>descretise_trace(acts, starts, ends, length, class_map)</code>","text":"<p>Create categorical encoding from ranges with step of 1.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>description</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>starts</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>ends</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>length</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def descretise_trace(\n    acts: Iterable[str],\n    starts: Iterable[int],\n    ends: Iterable[int],\n    length: int,\n    class_map: dict,\n) -&gt; np.ndarray:\n    \"\"\"Create categorical encoding from ranges with step of 1.\n\n    Args:\n        acts (Iterable[str]): _description_\n        starts (Iterable[int]): _description_\n        ends (Iterable[int]): _description_\n        length (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    encoding = np.zeros((length), dtype=np.int8)\n    for act, start, end in zip(acts, starts, ends):\n        encoding[start:end] = class_map[act]\n    return encoding\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.down_sample","title":"<code>down_sample(array, step)</code>","text":"<p>Down-sample by steppiong through given array. todo: Methodology will down sample based on first classification. If we are down sampling a lot (for example from minutes to hours), we would be better of, samplig based on majority class.</p> PARAMETER  DESCRIPTION <code>array</code> <p>description</p> <p> TYPE: <code>array</code> </p> <code>step</code> <p>description</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def down_sample(array: np.ndarray, step: int) -&gt; np.ndarray:\n    \"\"\"Down-sample by steppiong through given array.\n    todo:\n    Methodology will down sample based on first classification.\n    If we are down sampling a lot (for example from minutes to hours),\n    we would be better of, samplig based on majority class.\n\n    Args:\n        array (np.array): _description_\n        step (int): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    return array[::step]\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/","title":"caveat.encoders.descrete_one_hot","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot","title":"<code>DescreteEncoderOneHot(duration=1440, step_size=10, **kwargs)</code>","text":"<p>             Bases: <code>BaseEncoder</code></p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def __init__(self, duration: int = 1440, step_size: int = 10, **kwargs):\n    self.duration = duration\n    self.step_size = step_size\n    self.steps = duration // step_size\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot.duration","title":"<code>duration = duration</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot.step_size","title":"<code>step_size = step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot.steps","title":"<code>steps = duration // step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot.decode","title":"<code>decode(encoded)</code>","text":"<p>Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:</p> <p>pid | act | start | end</p> <p>pid is taken as sample enumeration.</p> PARAMETER  DESCRIPTION <code>encoded</code> <p>description</p> <p> TYPE: <code>Tensor</code> </p> <code>mapping</code> <p>description</p> <p> TYPE: <code>dict</code> </p> <code>length</code> <p>Length of plan in minutes.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: description</p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def decode(self, encoded: Tensor) -&gt; pd.DataFrame:\n    \"\"\"Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:\n\n    pid | act | start | end\n\n    pid is taken as sample enumeration.\n\n    Args:\n        encoded (Tensor): _description_\n        mapping (dict): _description_\n        length (int): Length of plan in minutes.\n\n    Returns:\n        pd.DataFrame: _description_\n    \"\"\"\n    encoded = torch.argmax(encoded, dim=-1)\n    decoded = []\n\n    for pid in range(len(encoded)):\n        current_act = None\n        act_start = 0\n\n        for step, act_idx in enumerate(encoded[pid, 0]):\n            if int(act_idx) != current_act and current_act is not None:\n                decoded.append(\n                    [\n                        pid,\n                        self.index_to_acts[current_act],\n                        int(act_start * self.step_size),\n                        int(step * self.step_size),\n                    ]\n                )\n                act_start = step\n            current_act = int(act_idx)\n        decoded.append(\n            [\n                pid,\n                self.index_to_acts[current_act],\n                int(act_start * self.step_size),\n                self.duration,\n            ]\n        )\n\n    return pd.DataFrame(decoded, columns=[\"pid\", \"act\", \"start\", \"end\"])\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteEncoderOneHot.encode","title":"<code>encode(data)</code>","text":"Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def encode(self, data: pd.DataFrame) -&gt; BaseEncodedPlans:\n    self.index_to_acts = {i: a for i, a in enumerate(data.act.unique())}\n    self.acts_to_index = {a: i for i, a in self.index_to_acts.items()}\n    return DescreteOneHotEncodedPlans(\n        data,\n        duration=self.duration,\n        step_size=self.step_size,\n        acts_to_index=self.acts_to_index,\n    )\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans","title":"<code>DescreteOneHotEncodedPlans(data, duration, step_size, acts_to_index)</code>","text":"<p>             Bases: <code>BaseEncodedPlans</code></p> <p>Torch Dataset for descretised sequence data.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Population of sequences.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    duration: int,\n    step_size: int,\n    acts_to_index: dict,\n):\n    \"\"\"Torch Dataset for descretised sequence data.\n\n    Args:\n        data (Tensor): Population of sequences.\n    \"\"\"\n    self.encodings = data.act.nunique()\n    # calc weightings based on durations\n    weights = data.groupby(\"act\", observed=True).duration.sum().to_dict()\n    weights = np.array([weights[k] for k in acts_to_index.keys()])\n    self.encoding_weights = torch.from_numpy(1 / weights).float()\n    self.encoded = descretise_population(\n        data,\n        duration=duration,\n        step_size=step_size,\n        class_map=acts_to_index,\n    )\n    self.masks = torch.ones((1, self.encoded.shape[2]))\n    self.size = len(self.encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.encoded","title":"<code>encoded = descretise_population(data, duration=duration, step_size=step_size, class_map=acts_to_index)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.encoding_weights","title":"<code>encoding_weights = torch.from_numpy(1 / weights).float()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.encodings","title":"<code>encodings = data.act.nunique()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.masks","title":"<code>masks = torch.ones((1, self.encoded.shape[2]))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.size","title":"<code>size = len(self.encoded)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.DescreteOneHotEncodedPlans.shape","title":"<code>shape()</code>","text":"Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def shape(self):\n    return self.encoded[0].shape\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.descretise_population","title":"<code>descretise_population(data, duration, step_size, class_map)</code>","text":"<p>Convert given population of activity traces into vector [P, C, H, W]. P is the population size. C (channel) is length 1. H is time steps. W is a one-hot encoding of activity type.</p> PARAMETER  DESCRIPTION <code>data</code> <p>description</p> <p> TYPE: <code>DataFrame</code> </p> <code>duration</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>step_size</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.tensor: [P, C, H, W]</p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def descretise_population(\n    data: pd.DataFrame, duration: int, step_size: int, class_map: dict\n) -&gt; torch.Tensor:\n    \"\"\"Convert given population of activity traces into vector [P, C, H, W].\n    P is the population size.\n    C (channel) is length 1.\n    H is time steps.\n    W is a one-hot encoding of activity type.\n\n    Args:\n        data (pd.DataFrame): _description_\n        duration (int): _description_\n        step_size (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        torch.tensor: [P, C, H, W]\n    \"\"\"\n    persons = data.pid.nunique()\n    num_classes = len(class_map)\n    steps = duration // step_size\n    encoded = np.zeros((persons, steps, num_classes, 1), dtype=np.float32)\n\n    for pid, (_, trace) in enumerate(data.groupby(\"pid\")):\n        trace_encoding = descretise_trace(\n            acts=trace.act,\n            starts=trace.start,\n            ends=trace.end,\n            length=duration,\n            class_map=class_map,\n        )\n        trace_encoding = down_sample(trace_encoding, step_size)\n        trace_encoding = one_hot(trace_encoding, num_classes)\n        trace_encoding = trace_encoding.reshape(steps, num_classes, 1)\n        encoded[pid] = trace_encoding  # [B, H, W, C]\n    encoded = encoded.transpose(0, 3, 1, 2)  # [B, C, H, W]\n    return torch.from_numpy(encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.descretise_trace","title":"<code>descretise_trace(acts, starts, ends, length, class_map)</code>","text":"<p>Create categorical encoding from ranges with step of 1.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>description</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>starts</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>ends</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>length</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def descretise_trace(\n    acts: Iterable[str],\n    starts: Iterable[int],\n    ends: Iterable[int],\n    length: int,\n    class_map: dict,\n) -&gt; np.ndarray:\n    \"\"\"Create categorical encoding from ranges with step of 1.\n\n    Args:\n        acts (Iterable[str]): _description_\n        starts (Iterable[int]): _description_\n        ends (Iterable[int]): _description_\n        length (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    encoding = np.zeros((length), dtype=np.int8)\n    for act, start, end in zip(acts, starts, ends):\n        encoding[start:end] = class_map[act]\n    return encoding\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.down_sample","title":"<code>down_sample(array, step)</code>","text":"<p>Down-sample by steppiong through given array. todo: Methodology will down sample based on first classification. If we are down sampling a lot (for example from minutes to hours), we would be better of, samplig based on majority class.</p> PARAMETER  DESCRIPTION <code>array</code> <p>description</p> <p> TYPE: <code>array</code> </p> <code>step</code> <p>description</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def down_sample(array: np.ndarray, step: int) -&gt; np.ndarray:\n    \"\"\"Down-sample by steppiong through given array.\n    todo:\n    Methodology will down sample based on first classification.\n    If we are down sampling a lot (for example from minutes to hours),\n    we would be better of, samplig based on majority class.\n\n    Args:\n        array (np.array): _description_\n        step (int): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    return array[::step]\n</code></pre>"},{"location":"reference/caveat/encoders/descrete_one_hot/#caveat.encoders.descrete_one_hot.one_hot","title":"<code>one_hot(target, num_classes)</code>","text":"<p>One hot encoding of given categorical array.</p> PARAMETER  DESCRIPTION <code>target</code> <p>description</p> <p> TYPE: <code>array</code> </p> <code>num_classes</code> <p>description</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete_one_hot.py</code> <pre><code>def one_hot(target: np.ndarray, num_classes: int) -&gt; np.ndarray:\n    \"\"\"One hot encoding of given categorical array.\n\n    Args:\n        target (np.array): _description_\n        num_classes (int): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    return np.eye(num_classes)[target]\n</code></pre>"},{"location":"reference/caveat/encoders/seq/","title":"caveat.encoders.seq","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans","title":"<code>SequenceEncodedPlans(data, max_length, acts_to_index, norm_duration, sos=0, eos=1)</code>","text":"<p>             Bases: <code>BaseEncodedPlans</code></p> <p>Torch Dataset for sequence data.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Population of sequences.</p> <p> TYPE: <code>DataFrame</code> </p> <code>max_length</code> <p>Max length of sequences.</p> <p> TYPE: <code>int</code> </p> <code>acts_to_index</code> <p>Mapping of activity to index.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>caveat/encoders/seq.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    max_length: int,\n    acts_to_index: dict,\n    norm_duration: int,\n    sos: int = 0,\n    eos: int = 1,\n):\n    \"\"\"Torch Dataset for sequence data.\n\n    Args:\n        data (DataFrame): Population of sequences.\n        max_length (int): Max length of sequences.\n        acts_to_index (dict): Mapping of activity to index.\n    \"\"\"\n    self.max_length = max_length\n    self.sos = sos\n    self.eos = eos\n    self.encodings = len(acts_to_index)\n    self.encoded, self.masks, self.encoding_weights = self._encode(\n        data, max_length, acts_to_index, norm_duration\n    )\n    self.size = len(self.encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.encodings","title":"<code>encodings = len(acts_to_index)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.eos","title":"<code>eos = eos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.max_length","title":"<code>max_length = max_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.size","title":"<code>size = len(self.encoded)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.sos","title":"<code>sos = sos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncodedPlans.shape","title":"<code>shape()</code>","text":"Source code in <code>caveat/encoders/seq.py</code> <pre><code>def shape(self):\n    return self.encoded[0].shape\n</code></pre>"},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncoder","title":"<code>SequenceEncoder(max_length=12, duration=1440, **kwargs)</code>","text":"<p>             Bases: <code>BaseEncoder</code></p> Source code in <code>caveat/encoders/seq.py</code> <pre><code>def __init__(\n    self, max_length: int = 12, duration: int = 1440, **kwargs\n):\n    self.max_length = max_length\n    self.duration = duration\n</code></pre>"},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncoder.duration","title":"<code>duration = duration</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncoder.max_length","title":"<code>max_length = max_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncoder.decode","title":"<code>decode(encoded)</code>","text":"<p>Decode a sequences ([N, max_length, encoding]) into DataFrame of 'traces', eg:</p> <p>pid | act | start | end</p> <p>enumeration of seq is used for pid.</p> PARAMETER  DESCRIPTION <code>encoded</code> <p>description</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: description</p> Source code in <code>caveat/encoders/seq.py</code> <pre><code>def decode(self, encoded: Tensor) -&gt; pd.DataFrame:\n    \"\"\"Decode a sequences ([N, max_length, encoding]) into DataFrame of 'traces', eg:\n\n    pid | act | start | end\n\n    enumeration of seq is used for pid.\n\n    Args:\n        encoded (Tensor): _description_\n\n    Returns:\n        pd.DataFrame: _description_\n    \"\"\"\n    encoded, durations = torch.split(encoded, [self.encodings, 1], dim=-1)\n    encoded = encoded.argmax(dim=-1).numpy()\n    decoded = []\n\n    for pid in range(len(encoded)):\n        act_start = 0\n        for act_idx, duration in zip(encoded[pid], durations[pid]):\n            if int(act_idx) == self.sos:\n                continue\n            if int(act_idx) == self.eos:\n                break\n            duration = int(duration * self.duration)\n            decoded.append(\n                [\n                    pid,\n                    self.index_to_acts[int(act_idx)],\n                    act_start,\n                    act_start + duration,\n                ]\n            )\n            act_start += duration\n\n    return pd.DataFrame(decoded, columns=[\"pid\", \"act\", \"start\", \"end\"])\n</code></pre>"},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.SequenceEncoder.encode","title":"<code>encode(data)</code>","text":"Source code in <code>caveat/encoders/seq.py</code> <pre><code>def encode(self, data: pd.DataFrame) -&gt; BaseEncodedPlans:\n    self.sos = 0\n    self.eos = 1\n    self.index_to_acts = {i + 2: a for i, a in enumerate(data.act.unique())}\n    self.index_to_acts[0] = \"&lt;SOS&gt;\"\n    self.index_to_acts[1] = \"&lt;EOS&gt;\"\n    self.acts_to_index = {a: i for i, a in self.index_to_acts.items()}\n    self.encodings = len(self.index_to_acts)\n    # encoding takes place in SequenceDataset\n    return SequenceEncodedPlans(\n        data, self.max_length, self.acts_to_index, self.duration\n    )\n</code></pre>"},{"location":"reference/caveat/encoders/seq/#caveat.encoders.seq.encode_sequence","title":"<code>encode_sequence(acts, durations, max_length, encoding_width, sos, eos)</code>","text":"<p>Create sequence encoding from ranges.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>durations</code> <p>description</p> <p> TYPE: <code>Iterable[float]</code> </p> <code>max_length</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>encoding_width</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>Tuple[ndarray, ndarray]</code> <p>np.array: description</p> Source code in <code>caveat/encoders/seq.py</code> <pre><code>def encode_sequence(\n    acts: list[int],\n    durations: list[float],\n    max_length: int,\n    encoding_width: int,\n    sos: int,\n    eos: int,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create sequence encoding from ranges.\n\n    Args:\n        acts (Iterable[int]): _description_\n        durations (Iterable[float]): _description_\n        max_length (int): _description_\n        encoding_width (dict): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    encoding = np.zeros((max_length, encoding_width), dtype=np.float32)\n    mask = np.zeros((max_length))\n    # SOS\n    encoding[0][0] = sos\n    # mask includes sos\n    mask[0] = 1\n    for i in range(1, max_length):\n        if i &lt; len(acts) + 1:\n            encoding[i][0] = acts[i - 1]\n            encoding[i][1] = durations[i - 1]\n            mask[i] = 1\n        elif i &lt; len(acts) + 2:\n            encoding[i][0] = eos\n            # mask includes first eos\n            mask[i] = 1\n        else:\n            encoding[i][0] = eos\n    return encoding, mask\n</code></pre>"},{"location":"reference/caveat/experiment/","title":"caveat.experiment","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment","title":"<code>Experiment(model, LR=0.005, weight_decay=0.0, scheduler_gamma=0.95, kld_weight=0.00025, duration_weight=1.0)</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>caveat/experiment.py</code> <pre><code>def __init__(\n    self,\n    model: BaseVAE,\n    LR: float = 0.005,\n    weight_decay: float = 0.0,\n    scheduler_gamma: float = 0.95,\n    kld_weight: float = 0.00025,\n    duration_weight: float = 1.0,\n) -&gt; None:\n    super(Experiment, self).__init__()\n    self.model = model\n    self.LR = LR\n    self.weight_decay = weight_decay\n    self.scheduler_gamma = scheduler_gamma\n    self.kld_weight = kld_weight\n    self.duration_weight = duration_weight\n    self.curr_device = None\n    self.save_hyperparameters(ignore=[\"model\"])\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.LR","title":"<code>LR = LR</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.curr_device","title":"<code>curr_device = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.duration_weight","title":"<code>duration_weight = duration_weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.kld_weight","title":"<code>kld_weight = kld_weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.scheduler_gamma","title":"<code>scheduler_gamma = scheduler_gamma</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.weight_decay","title":"<code>weight_decay = weight_decay</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def configure_optimizers(self):\n    optims = []\n    scheds = []\n\n    optimizer = optim.Adam(\n        self.model.parameters(), lr=self.LR, weight_decay=self.weight_decay\n    )\n    optims.append(optimizer)\n\n    if self.scheduler_gamma is not None:\n        scheduler = optim.lr_scheduler.ExponentialLR(\n            optims[0], gamma=self.scheduler_gamma\n        )\n        scheds.append(scheduler)\n    return optims, scheds\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.forward","title":"<code>forward(batch, **kwargs)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def forward(self, batch: Tensor, **kwargs) -&gt; List[Tensor]:\n    return self.model(batch, **kwargs)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.on_validation_end","title":"<code>on_validation_end()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def on_validation_end(self) -&gt; None:\n    self.regenerate_val_batch()\n    self.sample_sequences(100)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.predict_step","title":"<code>predict_step(batch)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def predict_step(self, batch):\n    return self.model.predict_step(batch, self.curr_device)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_batch","title":"<code>regenerate_batch(x, name)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_batch(self, x: Tensor, name: str):\n    y_probs = self.model.generate(x, self.curr_device).squeeze()\n    image = unpack(x, y_probs, self.curr_device)\n    div = torch.ones_like(y_probs)\n    images = torch.cat((image.squeeze(), div, y_probs), dim=-1)\n    vutils.save_image(\n        pre_process(images.data),\n        Path(\n            self.logger.log_dir,\n            name,\n            f\"recons_{self.logger.name}_epoch_{self.current_epoch}.png\",\n        ),\n        normalize=False,\n        nrow=1,\n        pad_value=1,\n    )\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_test_batch","title":"<code>regenerate_test_batch()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_test_batch(self):\n    x, _ = next(iter(self.trainer.datamodule.test_dataloader()))\n    x = x.to(self.curr_device)\n    self.regenerate_batch(x, name=\"test_reconstructions\")\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_val_batch","title":"<code>regenerate_val_batch()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_val_batch(self):\n    x, _ = next(iter(self.trainer.datamodule.val_dataloader()))\n    x = x.to(self.curr_device)\n    self.regenerate_batch(x, name=\"reconstructions\")\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.sample_sequences","title":"<code>sample_sequences(num_samples, name='samples')</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def sample_sequences(self, num_samples: int, name: str = \"samples\") -&gt; None:\n    z = torch.randn(num_samples, self.model.latent_dim)\n    y_probs = self.model.predict_step(z, self.curr_device)\n    vutils.save_image(\n        pre_process(y_probs.cpu().data),\n        Path(\n            self.logger.log_dir,\n            name,\n            f\"{self.logger.name}_epoch_{self.current_epoch}.png\",\n        ),\n        normalize=False,\n        nrow=1,\n        pad_value=1,\n    )\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    batch, mask = batch\n    self.curr_device = batch.device\n\n    results = self.forward(batch, target=batch)\n    train_loss = self.model.loss_function(\n        *results,\n        mask=mask,\n        kld_weight=self.kld_weight,\n        duration_weight=self.duration_weight,\n        batch_idx=batch_idx,\n    )\n    self.log_dict(\n        {key: val.item() for key, val in train_loss.items()}, sync_dist=True\n    )\n\n    return train_loss[\"loss\"]\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.validation_step","title":"<code>validation_step(batch, batch_idx, optimizer_idx=0)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def validation_step(self, batch, batch_idx, optimizer_idx=0):\n    batch, mask = batch\n    self.curr_device = batch.device\n\n    results = self.forward(batch)\n    val_loss = self.model.loss_function(\n        *results,\n        mask=mask,\n        kld_weight=self.kld_weight,\n        duration_weight=self.duration_weight,\n        optimizer_idx=optimizer_idx,\n        batch_idx=batch_idx,\n    )\n\n    self.log_dict(\n        {f\"val_{key}\": val.item() for key, val in val_loss.items()},\n        sync_dist=True,\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n    )\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.pre_process","title":"<code>pre_process(images)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def pre_process(images):\n    # hack for dealing with outputs encoded as [N, h, w]\n    # need to add channel dim and rearrange to [N, C, h, w]\n    # todo remove C/3d encoder\n    s = images.shape\n    if len(s) == 3:\n        # need to add dim and move channel to front\n        return images.reshape(s[0], s[1], s[2], 1).permute(0, 3, 1, 2)\n    return images\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.unpack","title":"<code>unpack(x, y, current_device)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def unpack(x, y, current_device):\n    if x.dim() == 2:\n        # assume cat encoding and unpack into image\n        channels = y.shape[-1]\n        eye = torch.eye(channels)\n        eye = eye.to(current_device)\n        ximage = eye[x.long()].squeeze()\n        return ximage\n\n    elif x.shape[-1] == 2:\n        # assume cat encoding and unpack into image\n        channels = y.shape[-1] - 1\n        acts, durations = x.split([1, 1], dim=-1)\n        eye = torch.eye(channels)\n        eye = eye.to(current_device)\n        ximage = eye[acts.long()].squeeze()\n        ximage = torch.cat((ximage, durations), dim=-1)\n        return ximage\n    return x\n</code></pre>"},{"location":"reference/caveat/features/creativity/","title":"caveat.features.creativity","text":""},{"location":"reference/caveat/features/creativity/#caveat.features.creativity.conservatism","title":"<code>conservatism(observed_hashed, synthetic, synthetic_hashed)</code>","text":"<p>Measure the conservatism of a population as 1-novelty.</p> PARAMETER  DESCRIPTION <code>observed_hashed</code> <p>Hashed observed population.</p> <p> TYPE: <code>set[str]</code> </p> <code>synthetic</code> <p>Synthetic population.</p> <p> TYPE: <code>DataFrame</code> </p> <code>synthetic_hashed</code> <p>Hashed synthetic population.</p> <p> TYPE: <code>set[str]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Conservatism of the synthetic population.</p> <p> TYPE: <code>float</code> </p> Source code in <code>caveat/features/creativity.py</code> <pre><code>def conservatism(\n    observed_hashed: set[str], synthetic: DataFrame, synthetic_hashed: set[str]\n) -&gt; float:\n    \"\"\"Measure the conservatism of a population as 1-novelty.\n\n    Args:\n        observed_hashed (set[str]): Hashed observed population.\n        synthetic (DataFrame): Synthetic population.\n        synthetic_hashed (set[str]): Hashed synthetic population.\n\n    Returns:\n        float: Conservatism of the synthetic population.\n    \"\"\"\n    return 1 - novelty(observed_hashed, synthetic, synthetic_hashed)\n</code></pre>"},{"location":"reference/caveat/features/creativity/#caveat.features.creativity.diversity","title":"<code>diversity(population, hashed)</code>","text":"<p>Measure the internal diversity of a population of sequences. This is the ratio of unique sequences to the total number of sequences.</p> PARAMETER  DESCRIPTION <code>population</code> <p>Input population of sequences.</p> <p> TYPE: <code>DataFrame</code> </p> <code>hashed</code> <p>Hashed population of sequences.</p> <p> TYPE: <code>set[str]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Diversity of the population.</p> <p> TYPE: <code>float</code> </p> Source code in <code>caveat/features/creativity.py</code> <pre><code>def diversity(population: DataFrame, hashed: set[str]) -&gt; float:\n    \"\"\"Measure the internal diversity of a population of sequences. This is the ratio of unique\n    sequences to the total number of sequences.\n\n    Args:\n        population (DataFrame): Input population of sequences.\n        hashed (set[str]): Hashed population of sequences.\n\n    Returns:\n        float: Diversity of the population.\n    \"\"\"\n    n = population.pid.nunique()\n    unique = len(hashed)\n    return unique / n\n</code></pre>"},{"location":"reference/caveat/features/creativity/#caveat.features.creativity.hash_population","title":"<code>hash_population(population)</code>","text":"<p>Hash a population of sequences. We first create strings of combined activities and durations. Then create a python set of these strings. This will remove duplicates.</p> PARAMETER  DESCRIPTION <code>population</code> <p>Input population of sequences.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>set[str]</code> <p>set[str]: set of hashed sequences.</p> Source code in <code>caveat/features/creativity.py</code> <pre><code>def hash_population(population: DataFrame) -&gt; set[str]:\n    \"\"\"Hash a population of sequences. We first create strings of combined activities and durations.\n    Then create a python set of these strings. This will remove duplicates.\n\n    Args:\n        population (DataFrame): Input population of sequences.\n\n    Returns:\n        set[str]: set of hashed sequences.\n    \"\"\"\n    act_hash = population.act.astype(str) + population.duration.astype(str)\n    return set(act_hash.groupby(population.pid).apply(\"\".join))\n</code></pre>"},{"location":"reference/caveat/features/creativity/#caveat.features.creativity.novelty","title":"<code>novelty(observed_hashed, synthetic, synthetic_hashed)</code>","text":"<p>Measure the novelty of a population by comparing it to an observed population.</p> PARAMETER  DESCRIPTION <code>observed_hashed</code> <p>Hashed observed population.</p> <p> TYPE: <code>set[str]</code> </p> <code>synthetic</code> <p>Synthetic population.</p> <p> TYPE: <code>DataFrame</code> </p> <code>synthetic_hashed</code> <p>Hashed synthetic population.</p> <p> TYPE: <code>set[str]</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Novelty of the synthetic population.</p> <p> TYPE: <code>float</code> </p> Source code in <code>caveat/features/creativity.py</code> <pre><code>def novelty(\n    observed_hashed: set[str], synthetic: DataFrame, synthetic_hashed: set[str]\n) -&gt; float:\n    \"\"\"Measure the novelty of a population by comparing it to an observed population.\n\n    Args:\n        observed_hashed (set[str]): Hashed observed population.\n        synthetic (DataFrame): Synthetic population.\n        synthetic_hashed (set[str]): Hashed synthetic population.\n\n    Returns:\n        float: Novelty of the synthetic population.\n    \"\"\"\n    unique = len(synthetic_hashed - observed_hashed)\n    return unique / synthetic.pid.nunique()\n</code></pre>"},{"location":"reference/caveat/features/frequency/","title":"caveat.features.frequency","text":""},{"location":"reference/caveat/features/frequency/#caveat.features.frequency.activity_bins","title":"<code>activity_bins(population, class_map, duration=1440, step=30)</code>","text":"Source code in <code>caveat/features/frequency.py</code> <pre><code>def activity_bins(\n    population: DataFrame, class_map: dict, duration: int = 1440, step: int = 30\n) -&gt; ndarray:\n    return (\n        descretise_population(\n            population, duration=duration, step_size=step, class_map=class_map\n        )\n        .sum(0)\n        .numpy()\n    )[0, :, :]\n</code></pre>"},{"location":"reference/caveat/features/frequency/#caveat.features.frequency.activity_frequencies","title":"<code>activity_frequencies(population, duration=1440, step=10)</code>","text":"Source code in <code>caveat/features/frequency.py</code> <pre><code>def activity_frequencies(\n    population: DataFrame, duration: int = 1440, step: int = 10\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    index_to_acts = {i: a for i, a in enumerate(population.act.unique())}\n    class_map = {a: i for i, a in index_to_acts.items()}\n    bins = (\n        descretise_population(\n            population, duration=duration, step_size=step, class_map=class_map\n        )\n        .mean(0)\n        .numpy()\n    )[0, :, :]\n\n    support = array([i for i in range(0, duration, step)])\n    return {act: (support, bins[:, i]) for act, i in class_map.items()}\n</code></pre>"},{"location":"reference/caveat/features/participation/","title":"caveat.features.participation","text":""},{"location":"reference/caveat/features/participation/#caveat.features.participation.calc_pair_count","title":"<code>calc_pair_count(act_counts, pair)</code>","text":"Source code in <code>caveat/features/participation.py</code> <pre><code>def calc_pair_count(act_counts, pair):\n    a, b = pair\n    if a == b:\n        return (act_counts[a] &gt; 1).sum()\n    return ((act_counts[a] &gt; 0) &amp; (act_counts[b] &gt; 0)).sum()\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.calc_pair_rate","title":"<code>calc_pair_rate(act_counts, pair)</code>","text":"<p>Calculates the participation rate for given activity pairs given activity counts.</p> <p>Parameters: act_counts (DataFrame): DataFrame of activity counts. pair (tuple): Pair of activities to calculate participation rate for.</p> <p>Returns: float: Participation rate of the pair of users.</p> Source code in <code>caveat/features/participation.py</code> <pre><code>def calc_pair_rate(act_counts: DataFrame, pair: tuple) -&gt; float:\n    \"\"\"\n    Calculates the participation rate for given activity pairs given activity counts.\n\n    Parameters:\n    act_counts (DataFrame): DataFrame of activity counts.\n    pair (tuple): Pair of activities to calculate participation rate for.\n\n    Returns:\n    float: Participation rate of the pair of users.\n    \"\"\"\n    a, b = pair\n    if a == b:\n        return (act_counts[a] &gt; 1).mean()\n    return ((act_counts[a] &gt; 0) &amp; (act_counts[b] &gt; 0)).mean()\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.combinations_with_replacement","title":"<code>combinations_with_replacement(targets, length, prev_array=[])</code>","text":"<p>Returns all possible combinations of elements in the input array with replacement, where each combination has a length of tuple_length.</p> PARAMETER  DESCRIPTION <code>targets</code> <p>The input array to generate combinations from.</p> <p> TYPE: <code>list</code> </p> <code>length</code> <p>The length of each combination.</p> <p> TYPE: <code>int</code> </p> <code>prev_array</code> <p>The previous array generated in the recursion. Defaults to [].</p> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> RETURNS DESCRIPTION <code>list</code> <p>A list of all possible combinations of elements in the input array with replacement.</p> <p> TYPE: <code>list[list]</code> </p> Source code in <code>caveat/features/participation.py</code> <pre><code>def combinations_with_replacement(\n    targets: list, length: int, prev_array=[]\n) -&gt; list[list]:\n    \"\"\"\n    Returns all possible combinations of elements in the input array with replacement,\n    where each combination has a length of tuple_length.\n\n    Args:\n        targets (list): The input array to generate combinations from.\n        length (int): The length of each combination.\n        prev_array (list, optional): The previous array generated in the recursion. Defaults to [].\n\n    Returns:\n        list: A list of all possible combinations of elements in the input array with replacement.\n    \"\"\"\n    if len(prev_array) == length:\n        return [prev_array]\n    combs = []\n    for i, val in enumerate(targets):\n        prev_array_extended = prev_array.copy()\n        prev_array_extended.append(val)\n        combs += combinations_with_replacement(\n            targets[i:], length, prev_array_extended\n        )\n    return combs\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.joint_participation_prob","title":"<code>joint_participation_prob(population)</code>","text":"<p>Calculate the participation rate for all pairs of activities in the given population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>dict[str, tuple[ndarray, ndarray]]</code> <p>pandas.Series: A Series containing the participation rate for all pairs of activities.</p> Source code in <code>caveat/features/participation.py</code> <pre><code>def joint_participation_prob(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    \"\"\"\n    Calculate the participation rate for all pairs of activities in the given population.\n\n    Args:\n        population (pandas.DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        pandas.Series: A Series containing the participation rate for all pairs of activities.\n    \"\"\"\n    act_counts = (\n        population.groupby(\"pid\").act.value_counts().unstack(fill_value=0)\n    )\n    acts = list(population.act.unique())\n    pairs = combinations_with_replacement(acts, 2)\n    n = population.pid.nunique()\n    metric = {}\n    for pair in pairs:\n        p = calc_pair_count(act_counts, pair)\n        metric[\"+\".join(pair)] = (array([0, 1]), array([n - p, p]))\n\n    return metric\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_prob_by_act","title":"<code>participation_prob_by_act(population)</code>","text":"<p>Calculate the participations by activity for a given population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>The population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>dict[str, tuple[ndarray, ndarray]]</code> <p>dict[str, tuple[array, array]]: A dictionary containing the participation for each activity.</p> Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_prob_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    \"\"\"\n    Calculate the participations by activity for a given population.\n\n    Args:\n        population (DataFrame): The population data.\n\n    Returns:\n        dict[str, tuple[array, array]]: A dictionary containing the participation for each activity.\n    \"\"\"\n    metrics = population.groupby([\"pid\", \"act\"], observed=False).size() &gt; 0\n    metrics = metrics.groupby(\"act\", observed=False).sum().to_dict()\n    n = population.pid.nunique()\n    compressed = {}\n    for k, v in metrics.items():\n        compressed[k] = (array([0, 1]), array([(n - v), v]))\n    return compressed\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_rates","title":"<code>participation_rates(population)</code>","text":"Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_rates(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    rates = population.groupby(\"pid\").act.count()\n    return weighted_features({\"all\": rates.to_list()})\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_rates_by_act","title":"<code>participation_rates_by_act(population)</code>","text":"Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_rates_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    rates = population.groupby(\"pid\").act.value_counts().unstack().fillna(0)\n    return weighted_features(rates.to_dict(orient=\"list\"))\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_rates_by_act_enum","title":"<code>participation_rates_by_act_enum(population)</code>","text":"Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_rates_by_act_enum(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    act_enum = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n    rates = act_enum.groupby(population.pid).value_counts().unstack().fillna(0)\n    return weighted_features(rates.to_dict(orient=\"list\"))\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_rates_by_seq_act","title":"<code>participation_rates_by_seq_act(population)</code>","text":"Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_rates_by_seq_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.groupby(\"pid\", as_index=False).cumcount().astype(\n        str\n    ) + population.act.astype(str)\n    rates = actseq.groupby(population.pid).value_counts().unstack().fillna(0)\n    return weighted_features(rates.to_dict(orient=\"list\"))\n</code></pre>"},{"location":"reference/caveat/features/structural/","title":"caveat.features.structural","text":""},{"location":"reference/caveat/features/structural/#caveat.features.structural.duration_consistency","title":"<code>duration_consistency(population)</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def duration_consistency(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    durations = population.groupby(\"pid\").duration.sum()\n    return weighted_features({\"total duration\": durations.array})\n</code></pre>"},{"location":"reference/caveat/features/structural/#caveat.features.structural.start_and_end_acts","title":"<code>start_and_end_acts(population, target='home')</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def start_and_end_acts(\n    population: DataFrame, target: str = \"home\"\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    n = len(population.pid.unique())\n    first = (population.groupby(\"pid\").first().act == target).sum()\n    last = (population.groupby(\"pid\").last().act == target).sum()\n    return {\n        f\"first act {target}\": (array([0, 1]), array([(n - first), first])),\n        f\"last act {target}\": (array([0, 1]), array([(n - last), last])),\n    }\n</code></pre>"},{"location":"reference/caveat/features/structural/#caveat.features.structural.time_consistency","title":"<code>time_consistency(population, target=1440)</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def time_consistency(\n    population: DataFrame, target: int = 1440\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    n = len(population.pid.unique())\n    starts = (population.groupby(\"pid\").first().start == 0).sum()\n    ends = (population.groupby(\"pid\").last().end == target).sum()\n    duration = (population.groupby(\"pid\").duration.sum() == target).sum()\n    return {\n        \"starts at 0\": (array([0, 1]), array([(n - starts), starts])),\n        f\"ends at {target}\": (array([0, 1]), array([(n - ends), ends])),\n        f\"duration is {target}\": (\n            array([0, 1]),\n            array([(n - duration), duration]),\n        ),\n    }\n</code></pre>"},{"location":"reference/caveat/features/structural/#caveat.features.structural.trip_consistency","title":"<code>trip_consistency(population)</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def trip_consistency(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/features/times/","title":"caveat.features.times","text":""},{"location":"reference/caveat/features/times/#caveat.features.times.durations_by_act","title":"<code>durations_by_act(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def durations_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    return weighted_features(\n        population.groupby(\"act\", observed=False).duration.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.durations_by_act_plan_enum","title":"<code>durations_by_act_plan_enum(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def durations_by_act_plan_enum(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n    return weighted_features(\n        population.groupby(actseq).duration.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.durations_by_act_plan_seq","title":"<code>durations_by_act_plan_seq(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def durations_by_act_plan_seq(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.groupby(\"pid\", as_index=False).cumcount().astype(\n        str\n    ) + population.act.astype(str)\n    return weighted_features(\n        population.groupby(actseq).duration.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.end_times_by_act","title":"<code>end_times_by_act(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def end_times_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    return weighted_features(\n        population.groupby(\"act\", observed=False).end.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.end_times_by_act_plan_enum","title":"<code>end_times_by_act_plan_enum(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def end_times_by_act_plan_enum(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n    return weighted_features(\n        population.groupby(actseq).end.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.end_times_by_act_plan_seq","title":"<code>end_times_by_act_plan_seq(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def end_times_by_act_plan_seq(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.groupby(\"pid\", as_index=False).cumcount().astype(\n        str\n    ) + population.act.astype(str)\n    return weighted_features(\n        population.groupby(actseq).end.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.start_and_duration_by_act_bins","title":"<code>start_and_duration_by_act_bins(population, bin_size=15)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def start_and_duration_by_act_bins(\n    population: DataFrame, bin_size: int = 15\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    return weighted_features(\n        start_durations_by_act(population), bin_size=bin_size\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.start_durations_by_act","title":"<code>start_durations_by_act(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def start_durations_by_act(population: DataFrame) -&gt; dict[str, ndarray]:\n    sds = population.groupby(\"act\", observed=False).apply(zip_columns).to_dict()\n    return sds\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.start_times_by_act","title":"<code>start_times_by_act(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def start_times_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    return weighted_features(\n        population.groupby(\"act\", observed=False).start.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.start_times_by_act_plan_enum","title":"<code>start_times_by_act_plan_enum(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def start_times_by_act_plan_enum(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n    return weighted_features(\n        population.groupby(actseq).start.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.start_times_by_act_plan_seq","title":"<code>start_times_by_act_plan_seq(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def start_times_by_act_plan_seq(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    actseq = population.groupby(\"pid\", as_index=False).cumcount().astype(\n        str\n    ) + population.act.astype(str)\n    return weighted_features(\n        population.groupby(actseq).start.apply(list).to_dict()\n    )\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.zip_columns","title":"<code>zip_columns(group)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def zip_columns(group) -&gt; ndarray:\n    return array([(s, d) for s, d in zip(group.start, group.duration)])\n</code></pre>"},{"location":"reference/caveat/features/transitions/","title":"caveat.features.transitions","text":""},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.collect_sequence","title":"<code>collect_sequence(acts)</code>","text":"Source code in <code>caveat/features/transitions.py</code> <pre><code>def collect_sequence(acts: Series) -&gt; str:\n    return \"&gt;\".join(acts)\n</code></pre>"},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.full_sequences","title":"<code>full_sequences(population)</code>","text":"Source code in <code>caveat/features/transitions.py</code> <pre><code>def full_sequences(population: DataFrame) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    transitions = population.reset_index()\n    transitions = transitions.set_index([\"index\", \"pid\"])\n    transitions.act = transitions.act.astype(str)\n    transitions = transitions.groupby(\"pid\").act.apply(tour)\n    transitions = (\n        transitions.groupby(\"pid\")\n        .value_counts()\n        .unstack()\n        .fillna(0)\n        .astype(int)\n        .to_dict(orient=\"list\")\n    )\n    return weighted_features(transitions)\n</code></pre>"},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.sequence_probs","title":"<code>sequence_probs(population)</code>","text":"<p>Calculates the sequence probabilities in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame containing the probability of each sequence.</p> <p> TYPE: <code>DataFrame</code> </p> Source code in <code>caveat/features/transitions.py</code> <pre><code>def sequence_probs(population: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the sequence probabilities in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        DataFrame: A DataFrame containing the probability of each sequence.\n    \"\"\"\n    metrics = (\n        population.groupby(\"pid\")\n        .act.apply(collect_sequence)\n        .value_counts(normalize=True)\n    )\n    metrics = metrics.sort_values(ascending=False)\n    metrics.index = MultiIndex.from_tuples(\n        [(\"sequence rate\", acts) for acts in metrics.index]\n    )\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.tour","title":"<code>tour(acts)</code>","text":"<p>Extracts the tour from the given Series of activities.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>A Series containing the activities.</p> <p> TYPE: <code>Series</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string representation of the tour.</p> <p> TYPE: <code>str</code> </p> Source code in <code>caveat/features/transitions.py</code> <pre><code>def tour(acts: Series) -&gt; str:\n    \"\"\"\n    Extracts the tour from the given Series of activities.\n\n    Args:\n        acts (Series): A Series containing the activities.\n\n    Returns:\n        str: A string representation of the tour.\n    \"\"\"\n    return \"&gt;\".join(acts.str[0])\n</code></pre>"},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.transition_3s_by_act","title":"<code>transition_3s_by_act(population)</code>","text":"Source code in <code>caveat/features/transitions.py</code> <pre><code>def transition_3s_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    transitions = population.reset_index()\n    transitions = transitions.set_index([\"index\", \"pid\"])\n    transitions.act = transitions.act.astype(str)\n    transitions = (\n        transitions.act\n        + \"&gt;\"\n        + transitions.act.shift(-1)\n        + \"&gt;\"\n        + transitions.act.shift(-2)\n    )\n    transitions = transitions.drop(transitions.groupby(\"pid\").tail(2).index)\n    transitions = (\n        transitions.groupby(\"pid\")\n        .value_counts()\n        .unstack()\n        .fillna(0)\n        .astype(int)\n        .to_dict(orient=\"list\")\n    )\n    return weighted_features(transitions)\n</code></pre>"},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.transitions_by_act","title":"<code>transitions_by_act(population)</code>","text":"Source code in <code>caveat/features/transitions.py</code> <pre><code>def transitions_by_act(\n    population: DataFrame,\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    transitions = population.reset_index()\n    transitions = transitions.set_index([\"index\", \"pid\"])\n    transitions.act = transitions.act.astype(str)\n    transitions = transitions.act + \"&gt;\" + transitions.act.shift(-1)\n    transitions = transitions.drop(transitions.groupby(\"pid\").tail(1).index)\n    transitions = (\n        transitions.groupby(\"pid\")\n        .value_counts()\n        .unstack()\n        .fillna(0)\n        .astype(int)\n        .to_dict(orient=\"list\")\n    )\n    return weighted_features(transitions)\n</code></pre>"},{"location":"reference/caveat/features/utils/","title":"caveat.features.utils","text":""},{"location":"reference/caveat/features/utils/#caveat.features.utils.bin_values","title":"<code>bin_values(values, bin_size)</code>","text":"<p>Bins the input values based on the given bin size.</p> PARAMETER  DESCRIPTION <code>values</code> <p>Input values to be binned.</p> <p> TYPE: <code>array</code> </p> <code>bin_size</code> <p>Size of each bin.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>array</code> <p>Binned values.</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>caveat/features/utils.py</code> <pre><code>def bin_values(values: array, bin_size: int) -&gt; ndarray:\n    \"\"\"\n    Bins the input values based on the given bin size.\n\n    Args:\n        values (array): Input values to be binned.\n        bin_size (int): Size of each bin.\n\n    Returns:\n        array: Binned values.\n    \"\"\"\n    return (values // bin_size * bin_size) + (bin_size / 2)\n</code></pre>"},{"location":"reference/caveat/features/utils/#caveat.features.utils.compress_feature","title":"<code>compress_feature(feature, bin_size=None)</code>","text":"<p>Compresses a feature by optionally binning its values and returning unique values with counts.</p> PARAMETER  DESCRIPTION <code>feature</code> <p>The feature to compress.</p> <p> TYPE: <code>list</code> </p> <code>bin_size</code> <p>The size of each bin. If None, no binning is performed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>A tuple containing two arrays and the total weight. The first array contains the unique values, and the second  array contains the counts of each value.</p> <p> TYPE: <code>tuple[ndarray, ndarray]</code> </p> Source code in <code>caveat/features/utils.py</code> <pre><code>def compress_feature(\n    feature: list, bin_size: Optional[int] = None\n) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"\n    Compresses a feature by optionally binning its values and returning unique values with counts.\n\n    Args:\n        feature (list): The feature to compress.\n        bin_size (int, optional): The size of each bin. If None, no binning is performed.\n\n    Returns:\n        tuple: A tuple containing two arrays and the total weight. The first array contains the unique\n            values, and the second  array contains the counts of each value.\n    \"\"\"\n    s = array(feature)\n    if bin_size is not None:\n        s = bin_values(s, bin_size)\n    ks, ws = unique(s, axis=0, return_counts=True)\n    return ks, ws\n</code></pre>"},{"location":"reference/caveat/features/utils/#caveat.features.utils.equals","title":"<code>equals(a, b)</code>","text":"Source code in <code>caveat/features/utils.py</code> <pre><code>def equals(\n    a: dict[str, tuple[ndarray, ndarray]], b: dict[str, tuple[ndarray, ndarray]]\n) -&gt; bool:\n    if set(a.keys()) != set(b.keys()):\n        return False\n    for k in a.keys():\n        if not len(a[k][0]) == len(b[k][0]):\n            return False\n        if not len(a[k][1]) == len(b[k][1]):\n            return False\n        if not (a[k][0] == b[k][0]).all():\n            return False\n        if not (a[k][1] == b[k][1]).all():\n            return False\n    return True\n</code></pre>"},{"location":"reference/caveat/features/utils/#caveat.features.utils.weighted_features","title":"<code>weighted_features(features, bin_size=None)</code>","text":"<p>Apply optional binning and value counting to dictionary of features.</p> PARAMETER  DESCRIPTION <code>features</code> <p>A dictionary of features to compress.</p> <p> TYPE: <code>dict[array</code> </p> <code>bin_size</code> <p>The size of the bin to use for compression. Defaults to None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, tuple[ndarray, ndarray]]</code> <p>dict[str, tuple[array, array[int]]]: A dictionary of features and weights.</p> Source code in <code>caveat/features/utils.py</code> <pre><code>def weighted_features(\n    features: dict[str, ndarray], bin_size: Optional[int] = None\n) -&gt; dict[str, tuple[ndarray, ndarray]]:\n    \"\"\"\n    Apply optional binning and value counting to dictionary of features.\n\n    Args:\n        features (dict[array): A dictionary of features to compress.\n        bin_size (Optional[int]): The size of the bin to use for compression. Defaults to None.\n\n    Returns:\n        dict[str, tuple[array, array[int]]]: A dictionary of features and weights.\n    \"\"\"\n    return {\n        k: compress_feature(values, bin_size) for k, values in features.items()\n    }\n</code></pre>"},{"location":"reference/caveat/models/base/","title":"caveat.models.base","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseDecoder","title":"<code>BaseDecoder(**kwargs)</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self, **kwargs):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseEncoder","title":"<code>BaseEncoder(**kwargs)</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self, **kwargs):\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE","title":"<code>BaseVAE(in_shape, encodings, encoding_weights=None, sos=0, **config)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base VAE.</p> PARAMETER  DESCRIPTION <code>in_shape</code> <p>[time_step, activity one-hot encoding].</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>encodings</code> <p>Number of activity encodings.</p> <p> TYPE: <code>int</code> </p> <code>encoding_weights</code> <p>Weights for activity encodings.</p> <p> TYPE: <code>tensor</code> DEFAULT: <code>None</code> </p> <code>sos</code> <p>Start of sequence token. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>config</code> <p>Additional arguments from config.</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple,\n    encodings: int,\n    encoding_weights: Optional[Tensor] = None,\n    sos: int = 0,\n    **config,\n) -&gt; None:\n    \"\"\"Base VAE.\n\n    Args:\n        in_shape (tuple[int, int]): [time_step, activity one-hot encoding].\n        encodings (int): Number of activity encodings.\n        encoding_weights (tensor): Weights for activity encodings.\n        sos (int, optional): Start of sequence token. Defaults to 0.\n        config: Additional arguments from config.\n    \"\"\"\n    super(BaseVAE, self).__init__()\n\n    self.in_shape = in_shape\n    self.encodings = encodings\n    self.encoding_weights = encoding_weights\n\n    self.sos = sos\n    self.teacher_forcing_ratio = config.get(\"teacher_forcing_ratio\", 0)\n    self.kld_weight = config.get(\"kld_weight\", 0.001)\n    print(f\"KLD weight: {self.kld_weight}\")\n    self.duration_weight = config.get(\"duration_weight\", 1)\n    self.use_mask = config.get(\"use_mask\", True)  # defaults to True\n    self.use_weighted_loss = config.get(\n        \"use_weighted_loss\", True\n    )  # defaults to True\n\n    self.NLLL = nn.NLLLoss(weight=encoding_weights)\n    self.MSE = nn.MSELoss()\n    self.hamming = MulticlassHammingDistance(\n        num_classes=encodings, average=\"micro\"\n    )\n\n    self.build(**config)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.MSE","title":"<code>MSE = nn.MSELoss()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.NLLL","title":"<code>NLLL = nn.NLLLoss(weight=encoding_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.duration_weight","title":"<code>duration_weight = config.get('duration_weight', 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.encoding_weights","title":"<code>encoding_weights = encoding_weights</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.encodings","title":"<code>encodings = encodings</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.hamming","title":"<code>hamming = MulticlassHammingDistance(num_classes=encodings, average='micro')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.in_shape","title":"<code>in_shape = in_shape</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.kld_weight","title":"<code>kld_weight = config.get('kld_weight', 0.001)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.sos","title":"<code>sos = sos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.teacher_forcing_ratio","title":"<code>teacher_forcing_ratio = config.get('teacher_forcing_ratio', 0)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.use_mask","title":"<code>use_mask = config.get('use_mask', True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.use_weighted_loss","title":"<code>use_weighted_loss = config.get('use_weighted_loss', True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.build","title":"<code>build(**config)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def build(self, **config):\n    self.latent_dim = config[\"latent_dim\"]\n    self.hidden_size = config[\"hidden_size\"]\n    self.hidden_layers = config[\"hidden_layers\"]\n    self.dropout = config[\"dropout\"]\n    length, _ = self.in_shape\n\n    self.encoder = BaseEncoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        num_layers=self.hidden_layers,\n        dropout=self.dropout,\n    )\n    self.decoder = BaseDecoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        output_size=self.encodings + 1,\n        num_layers=self.hidden_layers,\n        max_length=length,\n        sos=self.sos,\n    )\n    self.unflattened_shape = (self.hidden_layers, self.hidden_size)\n    flat_size_encode = self.hidden_layers * self.hidden_size\n    self.fc_mu = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_var = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_hidden = nn.Linear(self.latent_dim, flat_size_encode)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output batch as tuple of log probs and probs ([N, L, C]).</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output batch as tuple of log probs and probs ([N, L, C]).\n    \"\"\"\n    hidden = self.fc_hidden(z)\n    hidden = hidden.unflatten(1, self.unflattened_shape).permute(\n        1, 0, 2\n    )  # ([2xhidden, N, layers])\n    hidden = hidden.split(\n        self.hidden_layers\n    )  # ([hidden, N, layers, [hidden, N, layers]])\n    batch_size = z.shape[0]\n\n    if target is not None and torch.rand(1) &lt; self.teacher_forcing_ratio:\n        # attempt to use teacher forcing by passing target\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=target\n        )\n    else:\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=None\n        )\n\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.discretized_loss","title":"<code>discretized_loss(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"<p>Loss function for discretized encoding [N, L].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def discretized_loss(\n    self, log_probs, probs, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    \"\"\"Loss function for discretized encoding [N, L].\"\"\"\n\n    recon_argmax = probs.squeeze().argmax(dim=-1)\n\n    # activity encoding\n    recon_act_nlll = self.NLLL(\n        log_probs.squeeze().permute(0, 2, 1), input.long()\n    )\n\n    recon_act_ham = self.hamming(recon_argmax, input.long())\n\n    norm_kld_weight = self.kld_weight\n\n    kld_loss = norm_kld_weight * torch.mean(\n        -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1),\n        dim=0,\n    )\n\n    loss = recon_act_nlll + kld_loss\n\n    return {\n        \"loss\": loss,\n        \"recon_loss\": recon_act_nlll,\n        \"recon_act_nlll_loss\": recon_act_nlll,\n        \"recon_act_ham_loss\": recon_act_ham,\n        \"KLD\": kld_loss,\n        \"norm_kld_weight\": torch.tensor([norm_kld_weight]),\n        \"recon_kld_ratio\": recon_act_nlll / kld_loss,\n    }\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.discretized_loss_encoded","title":"<code>discretized_loss_encoded(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"<p>Computes the loss function for discretized encoding [N, L, C].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def discretized_loss_encoded(\n    self, log_probs, probs, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    \"\"\"Computes the loss function for discretized encoding [N, L, C].\"\"\"\n\n    input_argmax = input.squeeze().argmax(dim=-1)\n    return self.discretized_loss(\n        log_probs, probs, input_argmax, mu, log_var, mask, **kwargs\n    )\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.encode","title":"<code>encode(input)</code>","text":"<p>Encodes the input by passing through the encoder network.</p> PARAMETER  DESCRIPTION <code>input</code> <p>Input sequence batch [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>list[Tensor]</code> <p>list[tensor]: Latent layer input (means and variances) [N, latent_dims].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def encode(self, input: Tensor) -&gt; list[Tensor]:\n    \"\"\"Encodes the input by passing through the encoder network.\n\n    Args:\n        input (tensor): Input sequence batch [N, steps, acts].\n\n    Returns:\n        list[tensor]: Latent layer input (means and variances) [N, latent_dims].\n    \"\"\"\n    # [N, L, C]\n    hidden = self.encoder(input)\n    # [N, flatsize]\n\n    # Split the result into mu and var components\n    mu = self.fc_mu(hidden)\n    log_var = self.fc_var(hidden)\n\n    return [mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.forward","title":"<code>forward(x, target=None, **kwargs)</code>","text":"<p>Forward pass, also return latent parameterization.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input sequences [N, L, Cin].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>List[Tensor]</code> <p>list[tensor]: [Log probs, Probs [N, L, Cout], Input [N, L, Cin], mu [N, latent], var [N, latent]].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def forward(self, x: Tensor, target=None, **kwargs) -&gt; List[Tensor]:\n    \"\"\"Forward pass, also return latent parameterization.\n\n    Args:\n        x (tensor): Input sequences [N, L, Cin].\n\n    Returns:\n        list[tensor]: [Log probs, Probs [N, L, Cout], Input [N, L, Cin], mu [N, latent], var [N, latent]].\n    \"\"\"\n    mu, log_var = self.encode(x)\n    z = self.reparameterize(mu, log_var)\n    log_prob_y, prob_y = self.decode(z, target=target)\n    return [log_prob_y, prob_y, x, mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.generate","title":"<code>generate(x, current_device, **kwargs)</code>","text":"<p>Given an encoder input, return reconstructed output.</p> PARAMETER  DESCRIPTION <code>x</code> <p>[N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def generate(self, x: Tensor, current_device: int, **kwargs) -&gt; Tensor:\n    \"\"\"Given an encoder input, return reconstructed output.\n\n    Args:\n        x (tensor): [N, steps, acts].\n\n    Returns:\n        tensor: [N, steps, acts].\n    \"\"\"\n    prob_samples = self.forward(x)[1]\n    prob_samples = prob_samples.to(current_device)\n    return prob_samples\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.loss_function","title":"<code>loss_function(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"<p>Computes the loss function. Different models are expected to need different loss functions depending on the input data structure. Typically it will either be a sequence encoding [N, L, 2], or discretized encoding [N, L, C] or [N, L].</p> <p>The default is to use the sequence loss function. But child classes can override this method.</p> <p>Returns losses as a dictionary. Which must include the keys \"loss\" and \"recon_loss\".</p> PARAMETER  DESCRIPTION <code>log_probs</code> <p>Log probabilities of the output.</p> <p> TYPE: <code>Tensor</code> </p> <code>probs</code> <p>Probabilities of the output.</p> <p> TYPE: <code>Tensor</code> </p> <code>input</code> <p>Input sequences.</p> <p> TYPE: <code>Tensor</code> </p> <code>mu</code> <p>Latent layer means.</p> <p> TYPE: <code>Tensor</code> </p> <code>log_var</code> <p>Latent layer log variances.</p> <p> TYPE: <code>Tensor</code> </p> <code>mask</code> <p>Input mask.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Losses.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def loss_function(\n    self,\n    log_probs: Tensor,\n    probs: Tensor,\n    input: Tensor,\n    mu: Tensor,\n    log_var: Tensor,\n    mask: Tensor,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"Computes the loss function. Different models are expected to need different loss functions\n    depending on the input data structure. Typically it will either be a sequence encoding [N, L, 2],\n    or discretized encoding [N, L, C] or [N, L].\n\n    The default is to use the sequence loss function. But child classes can override this method.\n\n    Returns losses as a dictionary. Which must include the keys \"loss\" and \"recon_loss\".\n\n    Args:\n        log_probs (Tensor): Log probabilities of the output.\n        probs (Tensor): Probabilities of the output.\n        input (Tensor): Input sequences.\n        mu (Tensor): Latent layer means.\n        log_var (Tensor): Latent layer log variances.\n        mask (Tensor): Input mask.\n\n    Returns:\n        dict: Losses.\n    \"\"\"\n\n    return self.seq_loss(\n        log_probs, probs, input, mu, log_var, mask, **kwargs\n    )\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.pack_encoding","title":"<code>pack_encoding(acts, durations)</code>","text":"<p>Pack the activity and duration into input.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>Activity [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> <code>durations</code> <p>Duration [N, steps, 1].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Input sequences [N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def pack_encoding(self, acts: Tensor, durations: Tensor) -&gt; Tensor:\n    \"\"\"Pack the activity and duration into input.\n\n    Args:\n        acts (tensor): Activity [N, steps, acts].\n        durations (tensor): Duration [N, steps, 1].\n\n    Returns:\n        tensor: Input sequences [N, steps, acts].\n    \"\"\"\n    return torch.cat((acts, durations), dim=-1)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.predict_step","title":"<code>predict_step(z, current_device, **kwargs)</code>","text":"<p>Given samples from the latent space, return the corresponding decoder space map.</p> PARAMETER  DESCRIPTION <code>z</code> <p>[N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> <code>current_device</code> <p>Device to run the model.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def predict_step(self, z: Tensor, current_device: int, **kwargs) -&gt; Tensor:\n    \"\"\"Given samples from the latent space, return the corresponding decoder space map.\n\n    Args:\n        z (tensor): [N, latent_dims].\n        current_device (int): Device to run the model.\n\n    Returns:\n        tensor: [N, steps, acts].\n    \"\"\"\n    z = z.to(current_device)\n    prob_samples = self.decode(z)[1]\n    return prob_samples\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Re-parameterization trick to sample from N(mu, var) from N(0,1).</p> PARAMETER  DESCRIPTION <code>mu</code> <p>Mean of the latent Gaussian [N x latent_dims].</p> <p> TYPE: <code>tensor</code> </p> <code>logvar</code> <p>Standard deviation of the latent Gaussian [N x latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N x latent_dims].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/base.py</code> <pre><code>def reparameterize(self, mu: Tensor, logvar: Tensor) -&gt; Tensor:\n    \"\"\"Re-parameterization trick to sample from N(mu, var) from N(0,1).\n\n    Args:\n        mu (tensor): Mean of the latent Gaussian [N x latent_dims].\n        logvar (tensor): Standard deviation of the latent Gaussian [N x latent_dims].\n\n    Returns:\n        tensor: [N x latent_dims].\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return eps * std + mu\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.seq_loss","title":"<code>seq_loss(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"<p>Loss function for sequence encoding [N, L, 2].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def seq_loss(\n    self, log_probs, probs, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    \"\"\"Loss function for sequence encoding [N, L, 2].\"\"\"\n\n    # unpack act probs and durations\n    target_acts, target_durations = self.unpack_encoding(input)\n    pred_acts, pred_durations = self.unpack_encoding(log_probs)\n\n    if self.use_mask:  # default is to use masking\n        flat_mask = mask.view(-1).bool()\n    else:\n        flat_mask = torch.ones_like(target_acts).view(-1).bool()\n\n    # activity encoding\n    recon_act_nlll = self.NLLL(\n        pred_acts.view(-1, self.encodings)[flat_mask],\n        target_acts.view(-1).long()[flat_mask],\n    )\n\n    # duration encoding\n    recon_dur_mse = self.duration_weight * self.MSE(\n        pred_durations.view(-1)[flat_mask],\n        target_durations.view(-1)[flat_mask],\n    )\n\n    # combined\n    recons_loss = recon_act_nlll + recon_dur_mse\n\n    # hamming distance\n    recon_argmax = torch.argmax(pred_acts, dim=-1)\n    recon_act_ham = self.hamming(recon_argmax, target_acts.squeeze().long())\n\n    norm_kld_weight = self.kld_weight * self.latent_dim\n\n    kld_loss = norm_kld_weight * torch.mean(\n        -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1),\n        dim=0,\n    )\n\n    return {\n        \"loss\": recons_loss + kld_loss,\n        \"recon_loss\": recons_loss.detach(),\n        \"recon_act_nlll_loss\": recon_act_nlll.detach(),\n        \"recon_dur_mse_loss\": recon_dur_mse.detach(),\n        \"recon_act_ham_loss\": recon_act_ham.detach(),\n        \"KLD\": kld_loss.detach(),\n        \"norm_kld_weight\": torch.tensor([norm_kld_weight]).float(),\n        \"recon_act_ratio\": recon_act_nlll + recon_dur_mse,\n        \"recon_kld_ratio\": recons_loss / kld_loss,\n    }\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.unpack_encoding","title":"<code>unpack_encoding(input)</code>","text":"<p>Split the input into activity and duration.</p> PARAMETER  DESCRIPTION <code>input</code> <p>Input sequences [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor]</code> <p>tuple[tensor, tensor]: [activity [N, steps, acts], duration [N, steps, 1]].</p> Source code in <code>caveat/models/base.py</code> <pre><code>def unpack_encoding(self, input: Tensor) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Split the input into activity and duration.\n\n    Args:\n        input (tensor): Input sequences [N, steps, acts].\n\n    Returns:\n        tuple[tensor, tensor]: [activity [N, steps, acts], duration [N, steps, 1]].\n    \"\"\"\n    acts = input[:, :, :-1].contiguous()\n    durations = input[:, :, -1:].contiguous()\n    return acts, durations\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.CustomEmbedding","title":"<code>CustomEmbedding(input_size, hidden_size, dropout=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Embedding that combines activity embedding layer.</p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self, input_size, hidden_size, dropout: float = 0.1):\n    \"\"\"Embedding that combines activity embedding layer.\"\"\"\n    super().__init__()\n    self.embedding = nn.Embedding(input_size, hidden_size - 1)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.CustomEmbedding.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.CustomEmbedding.embedding","title":"<code>embedding = nn.Embedding(input_size, hidden_size - 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.CustomEmbedding.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def forward(self, x):\n    embedded, durations = torch.split(x, [1, 1], dim=-1)\n    embedded = self.dropout(self.embedding(embedded.int())).squeeze(-2)\n    embedded = torch.cat((embedded, durations), dim=-1)\n    return embedded\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.CustomLinearEmbedding","title":"<code>CustomLinearEmbedding(input_size, hidden_size, dropout=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Embedding that combines activity embedding layer and duration using a linear layer.</p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self, input_size, hidden_size, dropout: float = 0.1):\n    \"\"\"Embedding that combines activity embedding layer and duration using a linear layer.\"\"\"\n    super().__init__()\n    self.embedding = nn.Embedding(input_size, hidden_size - 1)\n    self.fc = nn.Linear(hidden_size, hidden_size)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.CustomLinearEmbedding.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.CustomLinearEmbedding.embedding","title":"<code>embedding = nn.Embedding(input_size, hidden_size - 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.CustomLinearEmbedding.fc","title":"<code>fc = nn.Linear(hidden_size, hidden_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.CustomLinearEmbedding.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def forward(self, x):\n    embedded, durations = torch.split(x, [1, 1], dim=-1)\n    embedded = self.dropout(self.embedding(embedded.int())).squeeze(-2)\n    embedded = torch.cat((embedded, durations), dim=-1)\n    embedded = self.fc(embedded)\n    return embedded\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.OneHotEmbedding","title":"<code>OneHotEmbedding(input_size, hidden_size, dropout=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Embedding that combines activity onehot embedding and duration.</p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self, input_size, hidden_size, dropout: float = 0.1):\n    \"\"\"Embedding that combines activity onehot embedding and duration.\"\"\"\n    super().__init__()\n    self.classes = input_size\n    self.fc = nn.Linear(hidden_size, hidden_size)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.OneHotEmbedding.classes","title":"<code>classes = input_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.OneHotEmbedding.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.OneHotEmbedding.fc","title":"<code>fc = nn.Linear(hidden_size, hidden_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.OneHotEmbedding.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def forward(self, x):\n    embedded, durations = torch.split(x, [1, 1], dim=-1)\n    embedded = self.dropout(\n        nn.functional.one_hot(embedded.int(), self.classes)\n    )\n    embedded = torch.cat((embedded, durations), dim=-1)\n    embedded = self.fc(embedded)\n    return embedded\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/","title":"caveat.models.conv.conv2d","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Conv2d","title":"<code>Conv2d(*args, **kwargs)</code>","text":"<p>             Bases: <code>BaseVAE</code></p> <p>Convolution based encoder and decoder, takes pre-encoded input (eg one-hot).</p> Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Convolution based encoder and decoder, takes pre-encoded input (eg one-hot).\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Conv2d.build","title":"<code>build(**config)</code>","text":"Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def build(self, **config):\n    hidden_layers = list\n    latent_dim = int\n    dropout = Optional[float]\n    kernel_size = Optional[Union[tuple[int, int], int]]\n    stride = Optional[Union[tuple[int, int], int]]\n    padding = Optional[Union[tuple[int, int], int]]\n\n    hidden_layers = config[\"hidden_layers\"]\n    latent_dim = config[\"latent_dim\"]\n    dropout = config.get(\"dropout\", 0)\n    kernel_size = config.get(\"kernel_size\", 3)\n    stride = config.get(\"stride\", 2)\n    padding = config.get(\"padding\", 1)\n\n    self.latent_dim = latent_dim\n\n    self.encoder = Encoder(\n        in_shape=self.in_shape,\n        hidden_layers=hidden_layers,\n        dropout=dropout,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n    )\n    self.decoder = Decoder(\n        target_shapes=self.encoder.target_shapes,\n        hidden_layers=hidden_layers,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n    )\n    self.fc_mu = nn.Linear(self.encoder.flat_size, latent_dim)\n    self.fc_var = nn.Linear(self.encoder.flat_size, latent_dim)\n    self.fc_hidden = nn.Linear(latent_dim, self.encoder.flat_size)\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Conv2d.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output sequence batch [N, steps, acts].</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output sequence batch [N, steps, acts].\n    \"\"\"\n    # initialize hidden state as inputs\n    hidden = self.fc_hidden(z)\n    hidden = hidden.view(self.encoder.shape_before_flattening)\n    log_probs, probs = self.decoder(hidden)\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Conv2d.loss_function","title":"<code>loss_function(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def loss_function(\n    self, log_probs, probs, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    return self.discretized_loss_encoded(\n        log_probs, probs, input, mu, log_var, mask, **kwargs\n    )\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Decoder","title":"<code>Decoder(target_shapes, hidden_layers, kernel_size=3, stride=2, padding=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>2d Conv Decoder.</p> PARAMETER  DESCRIPTION <code>target_shapes</code> <p>list of target shapes from encoder.</p> <p> TYPE: <code>list</code> </p> <code>hidden_layers</code> <p>description. Defaults to None.</p> <p> TYPE: <code>list</code> </p> <code>kernel_size</code> <p>description. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>description. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>description. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def __init__(\n    self,\n    target_shapes,\n    hidden_layers: list,\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n):\n    \"\"\"2d Conv Decoder.\n\n    Args:\n        target_shapes (list): list of target shapes from encoder.\n        hidden_layers (list, optional): _description_. Defaults to None.\n        kernel_size (Union[tuple[int, int], int], optional): _description_. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): _description_. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): _description_. Defaults to 1.\n    \"\"\"\n    super(Decoder, self).__init__()\n    modules = []\n    target_shapes.reverse()\n\n    for i in range(len(hidden_layers) - 1):\n        modules.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(\n                    in_channels=target_shapes[i][0],\n                    out_channels=target_shapes[i + 1][0],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    output_padding=calc_output_padding(\n                        target_shapes[i + 1]\n                    ),\n                    bias=False,\n                ),\n                nn.BatchNorm2d(target_shapes[i + 1][0]),\n                nn.LeakyReLU(),\n            )\n        )\n\n    # Final layer with Tanh activation\n    modules.append(\n        nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=target_shapes[-2][0],\n                out_channels=target_shapes[-1][0],\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                output_padding=calc_output_padding(target_shapes[-1]),\n                bias=False,\n            ),\n            nn.BatchNorm2d(target_shapes[-1][0]),\n            nn.Tanh(),\n        )\n    )\n\n    self.decoder = nn.Sequential(*modules)\n    self.prob_activation = nn.Softmax(dim=-1)\n    self.logprob_activation = nn.LogSoftmax(dim=-1)\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Decoder.decoder","title":"<code>decoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Decoder.logprob_activation","title":"<code>logprob_activation = nn.LogSoftmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Decoder.prob_activation","title":"<code>prob_activation = nn.Softmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Decoder.forward","title":"<code>forward(hidden, **kwargs)</code>","text":"Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def forward(self, hidden, **kwargs):\n    y = self.decoder(hidden)\n    return self.logprob_activation(y), self.prob_activation(y)\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder","title":"<code>Encoder(in_shape, hidden_layers, dropout=0.1, kernel_size=3, stride=2, padding=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>2d Convolutions Encoder.</p> PARAMETER  DESCRIPTION <code>in_shape</code> <p>[C, time_step, activity_encoding].</p> <p> TYPE: <code>tuple[int, int, int]</code> </p> <code>hidden_layers</code> <p>description. Defaults to None.</p> <p> TYPE: <code>list</code> </p> <code>dropout</code> <p>dropout. Defaults to 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>kernel_size</code> <p>description. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>description. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>description. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple,\n    hidden_layers: list,\n    dropout: float = 0.1,\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n):\n    \"\"\"2d Convolutions Encoder.\n\n    Args:\n        in_shape (tuple[int, int, int]): [C, time_step, activity_encoding].\n        hidden_layers (list, optional): _description_. Defaults to None.\n        dropout (float): dropout. Defaults to 0.1.\n        kernel_size (Union[tuple[int, int], int], optional): _description_. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): _description_. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): _description_. Defaults to 1.\n    \"\"\"\n    super(Encoder, self).__init__()\n    self.dropout = nn.Dropout(dropout)\n\n    modules = []\n    channels, h, w = in_shape\n    self.target_shapes = [(channels, h, w)]\n\n    for hidden_channels in hidden_layers:\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels=channels,\n                    out_channels=hidden_channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(hidden_channels),\n                nn.LeakyReLU(),\n            )\n        )\n        h, w = conv_size(\n            (h, w), kernel_size=kernel_size, padding=padding, stride=stride\n        )\n        self.target_shapes.append((hidden_channels, h, w))\n        channels = hidden_channels\n\n    self.shape_before_flattening = (-1, channels, h, w)\n    self.encoder = nn.Sequential(*modules)\n    self.flat_size = int(channels * h * w)\n</code></pre>"},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.encoder","title":"<code>encoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.flat_size","title":"<code>flat_size = int(channels * h * w)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.shape_before_flattening","title":"<code>shape_before_flattening = (-1, channels, h, w)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.target_shapes","title":"<code>target_shapes = [(channels, h, w)]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/conv2d/#caveat.models.conv.conv2d.Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/conv/conv2d.py</code> <pre><code>def forward(self, x):\n    y = self.encoder(self.dropout(x))\n    y = y.flatten(start_dim=1)\n    return y\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/","title":"caveat.models.conv.embed_conv","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Decoder","title":"<code>Decoder(target_shapes, hidden_layers, kernel_size=3, stride=2, padding=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>2d Conv Decoder.</p> PARAMETER  DESCRIPTION <code>target_shapes</code> <p>list of target shapes from encoder.</p> <p> TYPE: <code>list</code> </p> <code>hidden_layers</code> <p>description. Defaults to None.</p> <p> TYPE: <code>list</code> </p> <code>kernel_size</code> <p>description. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>description. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>description. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def __init__(\n    self,\n    target_shapes,\n    hidden_layers: list,\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n):\n    \"\"\"2d Conv Decoder.\n\n    Args:\n        target_shapes (list): list of target shapes from encoder.\n        hidden_layers (list, optional): _description_. Defaults to None.\n        kernel_size (Union[tuple[int, int], int], optional): _description_. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): _description_. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): _description_. Defaults to 1.\n    \"\"\"\n    super(Decoder, self).__init__()\n    modules = []\n    target_shapes.reverse()\n\n    for i in range(len(hidden_layers) - 1):\n        modules.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(\n                    in_channels=target_shapes[i][0],\n                    out_channels=target_shapes[i + 1][0],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    output_padding=calc_output_padding(\n                        target_shapes[i + 1]\n                    ),\n                    # bias=False,\n                ),\n                nn.BatchNorm2d(target_shapes[i + 1][0]),\n                nn.LeakyReLU(),\n            )\n        )\n\n    # Final layer with Tanh activation\n    modules.append(\n        nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=target_shapes[-2][0],\n                out_channels=target_shapes[-1][0],\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                output_padding=calc_output_padding(target_shapes[-1]),\n            ),\n            nn.BatchNorm2d(target_shapes[-1][0]),\n            nn.Tanh(),\n        )\n    )\n\n    self.decoder = nn.Sequential(*modules)\n    self.prob_activation = nn.Softmax(dim=-1)\n    self.logprob_activation = nn.LogSoftmax(dim=-1)\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Decoder.decoder","title":"<code>decoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Decoder.logprob_activation","title":"<code>logprob_activation = nn.LogSoftmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Decoder.prob_activation","title":"<code>prob_activation = nn.Softmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Decoder.forward","title":"<code>forward(hidden, **kwargs)</code>","text":"Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def forward(self, hidden, **kwargs):\n    y = self.decoder(hidden)\n    return self.logprob_activation(y), self.prob_activation(y)\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.EmbedConv","title":"<code>EmbedConv(*args, **kwargs)</code>","text":"<p>             Bases: <code>BaseVAE</code></p> <p>Convolution based encoder and decoder with encoder embedding layer.</p> Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Convolution based encoder and decoder with encoder embedding layer.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.EmbedConv.build","title":"<code>build(**config)</code>","text":"Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def build(self, **config):\n    hidden_layers = list\n    latent_dim = int\n    dropout = Optional[float]\n    kernel_size = Optional[Union[tuple[int, int], int]]\n    stride = Optional[Union[tuple[int, int], int]]\n    padding = Optional[Union[tuple[int, int], int]]\n\n    embed_size = config.get(\"embed_size\", self.encodings)\n    hidden_layers = config[\"hidden_layers\"]\n    latent_dim = config[\"latent_dim\"]\n    dropout = config.get(\"dropout\", 0)\n    kernel_size = config.get(\"kernel_size\", 3)\n    stride = config.get(\"stride\", 2)\n    padding = config.get(\"padding\", 1)\n\n    self.latent_dim = latent_dim\n\n    self.encoder = Encoder(\n        input_size=self.encodings,\n        embed_size=embed_size,\n        in_shape=self.in_shape,\n        hidden_layers=hidden_layers,\n        dropout=dropout,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n    )\n    self.decoder = Decoder(\n        target_shapes=self.encoder.target_shapes,\n        hidden_layers=hidden_layers,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n    )\n    self.fc_mu = nn.Linear(self.encoder.flat_size, latent_dim)\n    self.fc_var = nn.Linear(self.encoder.flat_size, latent_dim)\n    self.fc_hidden = nn.Linear(latent_dim, self.encoder.flat_size)\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.EmbedConv.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output sequence batch [N, steps, acts].</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output sequence batch [N, steps, acts].\n    \"\"\"\n    # initialize hidden state as inputs\n    hidden = self.fc_hidden(z)\n    hidden = hidden.view(self.encoder.shape_before_flattening)\n    log_probs, probs = self.decoder(hidden)\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.EmbedConv.loss_function","title":"<code>loss_function(log_probs, probs, input, mu, log_var, mask, **kwargs)</code>","text":"Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def loss_function(\n    self, log_probs, probs, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    return self.discretized_loss(\n        log_probs, probs, input, mu, log_var, mask, **kwargs\n    )\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder","title":"<code>Encoder(input_size, embed_size, in_shape, hidden_layers, dropout=0.1, kernel_size=3, stride=2, padding=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>2d Convolutions Encoder.</p> PARAMETER  DESCRIPTION <code>in_shape</code> <p>[C, time_step, activity_encoding].</p> <p> TYPE: <code>tuple[int, int, int]</code> </p> <code>hidden_layers</code> <p>description. Defaults to None.</p> <p> TYPE: <code>list</code> </p> <code>dropout</code> <p>dropout. Defaults to 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>kernel_size</code> <p>description. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>description. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>description. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    embed_size: int,\n    in_shape: tuple,\n    hidden_layers: list,\n    dropout: float = 0.1,\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n):\n    \"\"\"2d Convolutions Encoder.\n\n    Args:\n        in_shape (tuple[int, int, int]): [C, time_step, activity_encoding].\n        hidden_layers (list, optional): _description_. Defaults to None.\n        dropout (float): dropout. Defaults to 0.1.\n        kernel_size (Union[tuple[int, int], int], optional): _description_. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): _description_. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): _description_. Defaults to 1.\n    \"\"\"\n    super(Encoder, self).__init__()\n    h = in_shape[0]\n    self.embedding = nn.Embedding(input_size, embed_size)\n    w = embed_size\n    channels = 1\n\n    modules = []\n    self.target_shapes = [(channels, h, w)]\n\n    for hidden_channels in hidden_layers:\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels=channels,\n                    out_channels=hidden_channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    # bias=False,\n                ),\n                nn.BatchNorm2d(hidden_channels),\n                nn.LeakyReLU(),\n            )\n        )\n        h, w = conv_size(\n            (h, w), kernel_size=kernel_size, padding=padding, stride=stride\n        )\n        self.target_shapes.append((hidden_channels, h, w))\n        channels = hidden_channels\n\n    self.dropout = nn.Dropout(dropout)\n\n    self.shape_before_flattening = (-1, channels, h, w)\n    self.encoder = nn.Sequential(*modules)\n    self.flat_size = int(channels * h * w)\n</code></pre>"},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.embedding","title":"<code>embedding = nn.Embedding(input_size, embed_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.encoder","title":"<code>encoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.flat_size","title":"<code>flat_size = int(channels * h * w)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.shape_before_flattening","title":"<code>shape_before_flattening = (-1, channels, h, w)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.target_shapes","title":"<code>target_shapes = [(channels, h, w)]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/conv/embed_conv/#caveat.models.conv.embed_conv.Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/conv/embed_conv.py</code> <pre><code>def forward(self, x):\n    y = self.dropout(self.embedding(x.int()))\n    y = y.unsqueeze(1)  # add channel dim for Conv\n    y = self.encoder(y)\n    y = y.flatten(start_dim=1)\n    return y\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/","title":"caveat.models.seq.gru","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder","title":"<code>Decoder(input_size, hidden_size, output_size, num_layers, max_length, dropout=0.0, sos=0)</code>","text":"<p>             Bases: <code>Module</code></p> <p>LSTM Decoder with teacher forcings.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>lstm input size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>lstm hidden size.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>number of lstm layers.</p> <p> TYPE: <code>int</code> </p> <code>max_length</code> <p>max length of sequences.</p> <p> TYPE: <code>int</code> </p> <code>dropout</code> <p>dropout probability. Defaults to 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sos</code> <p>start of sequence encoding index. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def __init__(\n    self,\n    input_size,\n    hidden_size,\n    output_size,\n    num_layers,\n    max_length,\n    dropout: float = 0.0,\n    sos: int = 0,\n):\n    \"\"\"LSTM Decoder with teacher forcings.\n\n    Args:\n        input_size (int): lstm input size.\n        hidden_size (int): lstm hidden size.\n        num_layers (int): number of lstm layers.\n        max_length (int): max length of sequences.\n        dropout (float): dropout probability. Defaults to 0.\n        sos (int): start of sequence encoding index. Defaults to 0.\n    \"\"\"\n    super(Decoder, self).__init__()\n    self.current_device = current_device()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.max_length = max_length\n    self.sos = sos\n\n    self.embedding = CustomEmbedding(\n        input_size, hidden_size, dropout=dropout\n    )\n    self.activate = nn.ReLU()\n    self.rnn = nn.GRU(\n        hidden_size,\n        hidden_size,\n        num_layers,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.fc = nn.Linear(hidden_size, output_size)\n    self.activity_prob_activation = nn.Softmax(dim=-1)\n    self.activity_logprob_activation = nn.LogSoftmax(dim=-1)\n    self.duration_activation = nn.Softmax(dim=1)\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.activate","title":"<code>activate = nn.ReLU()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.activity_logprob_activation","title":"<code>activity_logprob_activation = nn.LogSoftmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.activity_prob_activation","title":"<code>activity_prob_activation = nn.Softmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.current_device","title":"<code>current_device = current_device()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.duration_activation","title":"<code>duration_activation = nn.Softmax(dim=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.embedding","title":"<code>embedding = CustomEmbedding(input_size, hidden_size, dropout=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.fc","title":"<code>fc = nn.Linear(hidden_size, output_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.hidden_size","title":"<code>hidden_size = hidden_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.input_size","title":"<code>input_size = input_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.max_length","title":"<code>max_length = max_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.output_size","title":"<code>output_size = output_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.rnn","title":"<code>rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=False)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.sos","title":"<code>sos = sos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.forward","title":"<code>forward(batch_size, hidden, target=None, **kwargs)</code>","text":"Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def forward(self, batch_size, hidden, target=None, **kwargs):\n    decoder_input = torch.zeros(batch_size, 1, 2, device=hidden.device)\n    decoder_input[:, :, 0] = self.sos  # set as SOS\n    hidden = hidden.contiguous()\n    decoder_hidden = hidden\n    outputs = []\n\n    for i in range(self.max_length):\n        decoder_output, decoder_hidden = self.forward_step(\n            decoder_input, decoder_hidden\n        )\n        outputs.append(decoder_output.squeeze())\n\n        if target is not None:\n            # teacher forcing for next step\n            decoder_input = target[:, i : i + 1, :]  # (slice maintains dim)\n        else:\n            # no teacher forcing use decoder output\n            decoder_input = self.pack(decoder_output)\n\n    outputs = torch.stack(outputs).permute(1, 0, 2)  # [N, steps, acts]\n\n    acts_logits, durations = torch.split(\n        outputs, [self.output_size - 1, 1], dim=-1\n    )\n    acts_probs = self.activity_prob_activation(acts_logits)\n    acts_log_probs = self.activity_logprob_activation(acts_logits)\n    durations = self.duration_activation(durations)\n    log_prob_outputs = torch.cat((acts_log_probs, durations), dim=-1)\n    prob_outputs = torch.cat((acts_probs, durations), dim=-1)\n\n    return log_prob_outputs, prob_outputs\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.forward_step","title":"<code>forward_step(x, hidden)</code>","text":"Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def forward_step(self, x, hidden):\n    # [N, 1, 2]\n    embedded = self.embedding(x)\n    output, hidden = self.rnn(embedded, hidden)\n    prediction = self.fc(output)\n    # [N, 1, encodings+1]\n    return prediction, hidden\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Decoder.pack","title":"<code>pack(x)</code>","text":"Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def pack(self, x):\n    # [N, 1, encodings+1]\n    acts, duration = torch.split(x, [self.output_size - 1, 1], dim=-1)\n    _, topi = acts.topk(1)\n    act = (\n        topi.squeeze(-1).detach().unsqueeze(-1)\n    )  # detach from history as input\n    duration = self.duration_activation(duration)\n    outputs = torch.cat((act, duration), dim=-1)\n    # [N, 1, 2]\n    return outputs\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder","title":"<code>Encoder(input_size, hidden_size, num_layers, dropout=0.0)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Sequence Encoder Template.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>lstm input size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>lstm hidden size.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>number of lstm layers.</p> <p> TYPE: <code>int</code> </p> <code>dropout</code> <p>dropout. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    num_layers: int,\n    dropout: float = 0.0,\n):\n    \"\"\"Sequence Encoder Template.\n\n    Args:\n        input_size (int): lstm input size.\n        hidden_size (int): lstm hidden size.\n        num_layers (int): number of lstm layers.\n        dropout (float): dropout. Defaults to 0.0.\n    \"\"\"\n    super(Encoder, self).__init__()\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.embedding = CustomEmbedding(\n        input_size, hidden_size, dropout=dropout\n    )\n    self.rnn = nn.GRU(\n        hidden_size,\n        hidden_size,\n        num_layers,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.embedding","title":"<code>embedding = CustomEmbedding(input_size, hidden_size, dropout=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.hidden_size","title":"<code>hidden_size = hidden_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.num_layers","title":"<code>num_layers = num_layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.rnn","title":"<code>rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=False)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def forward(self, x):\n    embedded = self.embedding(x)\n    _, hidden = self.rnn(embedded)\n    # [L, N, C])\n    hidden = hidden.permute(1, 0, 2).flatten(start_dim=1)\n    # [N, flatsize]\n    return hidden\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.GRU","title":"<code>GRU(*args, **kwargs)</code>","text":"<p>             Bases: <code>BaseVAE</code></p> <p>RNN based encoder and decoder with encoder embedding layer.</p> Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"RNN based encoder and decoder with encoder embedding layer.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.GRU.build","title":"<code>build(**config)</code>","text":"Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def build(self, **config):\n    self.latent_dim = config[\"latent_dim\"]\n    self.hidden_size = config[\"hidden_size\"]\n    self.hidden_layers = config[\"hidden_layers\"]\n    self.dropout = config[\"dropout\"]\n    length, _ = self.in_shape\n\n    self.encoder = Encoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        num_layers=self.hidden_layers,\n        dropout=self.dropout,\n    )\n    self.decoder = Decoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        output_size=self.encodings + 1,\n        num_layers=self.hidden_layers,\n        max_length=length,\n        dropout=self.dropout,\n        sos=self.sos,\n    )\n    flat_size_encode = self.hidden_layers * self.hidden_size\n    self.fc_mu = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_var = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_hidden = nn.Linear(self.latent_dim, flat_size_encode)\n</code></pre>"},{"location":"reference/caveat/models/seq/gru/#caveat.models.seq.gru.GRU.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output batch as tuple of log probs and probs ([N, L, C]).</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/seq/gru.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output batch as tuple of log probs and probs ([N, L, C]).\n    \"\"\"\n    hidden = self.fc_hidden(z)\n    hidden = hidden.unflatten(\n        1, (self.hidden_layers, self.hidden_size)\n    ).permute(\n        1, 0, 2\n    )  # ([hidden, N, layers])\n    batch_size = z.shape[0]\n\n    if target is not None and torch.rand(1) &lt; self.teacher_forcing_ratio:\n        # attempt to use teacher forcing by passing target\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=target\n        )\n    else:\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=None\n        )\n\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/","title":"caveat.models.seq.lstm","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder","title":"<code>Decoder(input_size, hidden_size, output_size, num_layers, max_length, dropout=0.0, sos=0)</code>","text":"<p>             Bases: <code>Module</code></p> <p>LSTM Decoder with teacher forcings.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>lstm input size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>lstm hidden size.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>number of lstm layers.</p> <p> TYPE: <code>int</code> </p> <code>max_length</code> <p>max length of sequences.</p> <p> TYPE: <code>int</code> </p> <code>dropout</code> <p>dropout probability. Defaults to 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def __init__(\n    self,\n    input_size,\n    hidden_size,\n    output_size,\n    num_layers,\n    max_length,\n    dropout: float = 0.0,\n    sos: int = 0,\n):\n    \"\"\"LSTM Decoder with teacher forcings.\n\n    Args:\n        input_size (int): lstm input size.\n        hidden_size (int): lstm hidden size.\n        num_layers (int): number of lstm layers.\n        max_length (int): max length of sequences.\n        dropout (float): dropout probability. Defaults to 0.\n    \"\"\"\n    super(Decoder, self).__init__()\n    self.current_device = current_device()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.max_length = max_length\n    self.sos = sos\n\n    self.embedding = CustomEmbedding(\n        input_size, hidden_size, dropout=dropout\n    )\n    self.activate = nn.ReLU()\n    self.lstm = nn.LSTM(\n        hidden_size,\n        hidden_size,\n        num_layers,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.fc = nn.Linear(hidden_size, output_size)\n    self.activity_prob_activation = nn.Softmax(dim=-1)\n    self.activity_logprob_activation = nn.LogSoftmax(dim=-1)\n    self.duration_activation = nn.Softmax(dim=1)\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.activate","title":"<code>activate = nn.ReLU()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.activity_logprob_activation","title":"<code>activity_logprob_activation = nn.LogSoftmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.activity_prob_activation","title":"<code>activity_prob_activation = nn.Softmax(dim=-1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.current_device","title":"<code>current_device = current_device()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.duration_activation","title":"<code>duration_activation = nn.Softmax(dim=1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.embedding","title":"<code>embedding = CustomEmbedding(input_size, hidden_size, dropout=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.fc","title":"<code>fc = nn.Linear(hidden_size, output_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.hidden_size","title":"<code>hidden_size = hidden_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.input_size","title":"<code>input_size = input_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.lstm","title":"<code>lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=False)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.max_length","title":"<code>max_length = max_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.output_size","title":"<code>output_size = output_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.sos","title":"<code>sos = sos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.forward","title":"<code>forward(batch_size, hidden, target=None, **kwargs)</code>","text":"Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def forward(self, batch_size, hidden, target=None, **kwargs):\n    hidden, cell = hidden\n    decoder_input = torch.zeros(batch_size, 1, 2, device=hidden.device)\n    decoder_input[:, :, 0] = self.sos  # set as SOS\n    hidden = hidden.contiguous()\n    cell = cell.contiguous()\n    decoder_hidden = (hidden, cell)\n    outputs = []\n\n    for i in range(self.max_length):\n        decoder_output, decoder_hidden = self.forward_step(\n            decoder_input, decoder_hidden\n        )\n        outputs.append(decoder_output.squeeze())\n\n        if target is not None:\n            # teacher forcing for next step\n            decoder_input = target[:, i : i + 1, :]  # (slice maintains dim)\n        else:\n            # no teacher forcing use decoder output\n            decoder_input = self.pack(decoder_output)\n\n    outputs = torch.stack(outputs).permute(1, 0, 2)  # [N, steps, acts]\n\n    acts_logits, durations = torch.split(\n        outputs, [self.output_size - 1, 1], dim=-1\n    )\n    acts_probs = self.activity_prob_activation(acts_logits)\n    acts_log_probs = self.activity_logprob_activation(acts_logits)\n    durations = self.duration_activation(durations)\n    log_prob_outputs = torch.cat((acts_log_probs, durations), dim=-1)\n    prob_outputs = torch.cat((acts_probs, durations), dim=-1)\n\n    return log_prob_outputs, prob_outputs\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.forward_step","title":"<code>forward_step(x, hidden)</code>","text":"Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def forward_step(self, x, hidden):\n    # [N, 1, 2]\n    embedded = self.embedding(x)\n    output, hidden = self.lstm(embedded, hidden)\n    prediction = self.fc(output)\n    # [N, 1, encodings+1]\n    return prediction, hidden\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Decoder.pack","title":"<code>pack(x)</code>","text":"Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def pack(self, x):\n    # [N, 1, encodings+1]\n    acts, duration = torch.split(x, [self.output_size - 1, 1], dim=-1)\n    _, topi = acts.topk(1)\n    act = (\n        topi.squeeze(-1).detach().unsqueeze(-1)\n    )  # detach from history as input\n    duration = self.duration_activation(duration)\n    outputs = torch.cat((act, duration), dim=-1)\n    # [N, 1, 2]\n    return outputs\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder","title":"<code>Encoder(input_size, hidden_size, num_layers, dropout=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>LSTM Encoder.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>lstm input size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>lstm hidden size.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>number of lstm layers.</p> <p> TYPE: <code>int</code> </p> <code>dropout</code> <p>dropout. Defaults to 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    num_layers: int,\n    dropout: float = 0.1,\n):\n    \"\"\"LSTM Encoder.\n\n    Args:\n        input_size (int): lstm input size.\n        hidden_size (int): lstm hidden size.\n        num_layers (int): number of lstm layers.\n        dropout (float): dropout. Defaults to 0.1.\n    \"\"\"\n    super(Encoder, self).__init__()\n    self.hidden_size = hidden_size\n    self.num_layers = num_layers\n    self.embedding = CustomEmbedding(\n        input_size, hidden_size, dropout=dropout\n    )\n    self.lstm = nn.LSTM(\n        hidden_size,\n        hidden_size,\n        num_layers,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.embedding","title":"<code>embedding = CustomEmbedding(input_size, hidden_size, dropout=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.hidden_size","title":"<code>hidden_size = hidden_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.lstm","title":"<code>lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=False)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.num_layers","title":"<code>num_layers = num_layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.Encoder.forward","title":"<code>forward(x)</code>","text":"Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def forward(self, x):\n    embedded = self.embedding(x)\n    _, hidden = self.lstm(embedded)\n    # ([L, N, C], [L, N, C])\n    hidden = torch.cat(hidden).permute(1, 0, 2).flatten(start_dim=1)\n    # [N, flatsize]\n    return hidden\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.LSTM","title":"<code>LSTM(*args, **kwargs)</code>","text":"<p>             Bases: <code>BaseVAE</code></p> <p>RNN based encoder and decoder with encoder embedding layer.</p> Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"RNN based encoder and decoder with encoder embedding layer.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.LSTM.build","title":"<code>build(**config)</code>","text":"Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def build(self, **config):\n    self.latent_dim = config[\"latent_dim\"]\n    self.hidden_size = config[\"hidden_size\"]\n    self.hidden_layers = config[\"hidden_layers\"]\n    self.dropout = config[\"dropout\"]\n    length, _ = self.in_shape\n\n    self.encoder = Encoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        num_layers=self.hidden_layers,\n        dropout=self.dropout,\n    )\n    self.decoder = Decoder(\n        input_size=self.encodings,\n        hidden_size=self.hidden_size,\n        output_size=self.encodings + 1,\n        num_layers=self.hidden_layers,\n        max_length=length,\n        dropout=self.dropout,\n        sos=self.sos,\n    )\n    self.unflattened_shape = (2 * self.hidden_layers, self.hidden_size)\n    flat_size_encode = self.hidden_layers * self.hidden_size * 2\n    self.fc_mu = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_var = nn.Linear(flat_size_encode, self.latent_dim)\n    self.fc_hidden = nn.Linear(self.latent_dim, flat_size_encode)\n\n    if config.get(\"share_embed\", False):\n        self.decoder.embedding.weight = self.encoder.embedding.weight\n</code></pre>"},{"location":"reference/caveat/models/seq/lstm/#caveat.models.seq.lstm.LSTM.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output sequence batch [N, steps, acts].</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/seq/lstm.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output sequence batch [N, steps, acts].\n    \"\"\"\n    # initialize hidden state as inputs\n    h = self.fc_hidden(z)\n\n    # initialize hidden state\n    hidden = h.unflatten(\n        1, (2 * self.hidden_layers, self.hidden_size)\n    ).permute(\n        1, 0, 2\n    )  # ([2xhidden, N, layers])\n    hidden = hidden.split(\n        self.hidden_layers\n    )  # ([hidden, N, layers, [hidden, N, layers]])\n    batch_size = z.shape[0]\n\n    if target is not None and torch.rand(1) &lt; self.teacher_forcing_ratio:\n        # use teacher forcing\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=target\n        )\n    else:\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=None\n        )\n\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/","title":"caveat.models.seq.transformer","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN","title":"<code>AttnDecoderRNN(input_size, hidden_size, output_size, max_length, sos, dropout=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    hidden_size: int,\n    output_size: int,\n    max_length: int,\n    sos: int,\n    dropout: float = 0.1,\n):\n    super(AttnDecoderRNN, self).__init__()\n    self.max_length = max_length\n    self.sos = sos\n    self.embedding = nn.Embedding(input_size, hidden_size - 1)\n    self.attention = BahdanauAttention(hidden_size)\n    self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n    self.out = nn.Linear(hidden_size, output_size)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.attention","title":"<code>attention = BahdanauAttention(hidden_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.dropout","title":"<code>dropout = nn.Dropout(dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.embedding","title":"<code>embedding = nn.Embedding(input_size, hidden_size - 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.gru","title":"<code>gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.max_length","title":"<code>max_length = max_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.out","title":"<code>out = nn.Linear(hidden_size, output_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.sos","title":"<code>sos = sos</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.forward","title":"<code>forward(batch_size, encoder_outputs, encoder_hidden, target=None)</code>","text":"Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def forward(self, batch_size, encoder_outputs, encoder_hidden, target=None):\n    decoder_input = torch.zeros(\n        batch_size, 1, 2, dtype=torch.long, device=self.current_device\n    )\n    decoder_input[:, :, 0] = self.sos\n    hidden, cell = encoder_hidden\n    hidden = hidden.contiguous()\n    cell = cell.contiguous()\n    decoder_hidden = (hidden, cell)\n    decoder_outputs = []\n    attentions = []\n\n    for i in range(self.max_length):\n        decoder_output, decoder_hidden, attn_weights = self.forward_step(\n            decoder_input, decoder_hidden, encoder_outputs\n        )\n        decoder_outputs.append(decoder_output.squeeze())\n        attentions.append(attn_weights.squeeze())\n\n        if target is not None:\n            # teacher forcing for next step\n            decoder_input = target[:, i : i + 1, :]  # (slice maintains dim)\n        else:\n            # no teacher forcing use decoder output\n            decoder_input = self.pack(decoder_output)\n\n    decoder_outputs = torch.stack(decoder_outputs).permute(\n        1, 0, 2\n    )  # [N, steps, acts]\n    attentions = torch.stack(attentions).permute(\n        1, 0, 2\n    )  # [N, steps, encodings]\n\n    acts_logits, durations = torch.split(\n        decoder_outputs, [self.output_size - 1, 1], dim=-1\n    )\n    acts_probs = self.activity_prob_activation(acts_logits)\n    acts_log_probs = self.activity_logprob_activation(acts_logits)\n    durations = self.duration_activation(durations)\n    log_prob_outputs = torch.cat((acts_log_probs, durations), dim=-1)\n    prob_outputs = torch.cat((acts_probs, durations), dim=-1)\n\n    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n\n    return log_prob_outputs, prob_outputs, decoder_hidden, attentions\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.forward_step","title":"<code>forward_step(input, hidden, encoder_outputs)</code>","text":"Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def forward_step(self, input, hidden, encoder_outputs):\n    # input: [N, 1, 2]\n    embedded, durations = torch.split(input, [1, 1], dim=-1)\n    embedded = self.dropout(self.embedding(embedded.int())).squeeze(-2)\n    embedded = torch.cat((embedded, durations), dim=-1)\n\n    query = hidden.permute(1, 0, 2)\n    context, attn_weights = self.attention(query, encoder_outputs)\n    input_gru = torch.cat((embedded, context), dim=2)\n\n    output, hidden = self.gru(input_gru, hidden)\n    output = self.out(output)\n\n    return output, hidden, attn_weights\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.AttnDecoderRNN.pack","title":"<code>pack(x)</code>","text":"Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def pack(self, x):\n    # [N, 1, encodings+1]\n    acts, duration = torch.split(x, [self.output_size - 1, 1], dim=-1)\n    _, topi = acts.topk(1)\n    act = (\n        topi.squeeze(-1).detach().unsqueeze(-1)\n    )  # detach from history as input\n    duration = self.duration_activation(duration)\n    outputs = torch.cat((act, duration), dim=-1)\n    # [N, 1, 2]\n    return outputs\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.BahdanauAttention","title":"<code>BahdanauAttention(hidden_size)</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def __init__(self, hidden_size):\n    super(BahdanauAttention, self).__init__()\n    self.Wa = nn.Linear(hidden_size, hidden_size)\n    self.Ua = nn.Linear(hidden_size, hidden_size)\n    self.Va = nn.Linear(hidden_size, 1)\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.BahdanauAttention.Ua","title":"<code>Ua = nn.Linear(hidden_size, hidden_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.BahdanauAttention.Va","title":"<code>Va = nn.Linear(hidden_size, 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.BahdanauAttention.Wa","title":"<code>Wa = nn.Linear(hidden_size, hidden_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.BahdanauAttention.forward","title":"<code>forward(query, keys)</code>","text":"Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def forward(self, query, keys):\n    scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n    scores = scores.squeeze(2).unsqueeze(1)\n\n    weights = F.softmax(scores, dim=-1)\n    context = torch.bmm(weights, keys)\n\n    return context, weights\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer","title":"<code>Transformer(in_shape, latent_dim, hidden_layers, hidden_size, teacher_forcing_ratio, encodings, encoding_weights, dropout=0.1, **kwargs)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Seq to seq via VAE model.</p> PARAMETER  DESCRIPTION <code>in_shape</code> <p>[time_step, activity one-hot encoding].</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>latent_dim</code> <p>Latent space size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_layers</code> <p>Lstm  layers in encoder and decoder.</p> <p> TYPE: <code>int</code> </p> <code>hidden_size</code> <p>Size of lstm layers.</p> <p> TYPE: <code>int</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple[int, int],\n    latent_dim: int,\n    hidden_layers: int,\n    hidden_size: int,\n    teacher_forcing_ratio: float,\n    encodings: int,\n    encoding_weights: Optional[Tensor],\n    dropout: float = 0.1,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Seq to seq via VAE model.\n\n    Args:\n        in_shape (tuple[int, int]): [time_step, activity one-hot encoding].\n        latent_dim (int): Latent space size.\n        hidden_layers (int): Lstm  layers in encoder and decoder.\n        hidden_size (int): Size of lstm layers.\n    \"\"\"\n    super(Transformer, self).__init__()\n    self.steps, self.width = in_shape\n    self.hidden_layers = hidden_layers\n    self.hidden_size = hidden_size\n    self.latent_dim = latent_dim\n    self.teacher_forcing_ratio = teacher_forcing_ratio\n    self.encodings = encodings\n    self.encoding_weights = encoding_weights\n\n    if (\n        kwargs.get(\"weighted_loss\") is not False\n    ):  # default to use weightings\n        if encoding_weights is None:\n            raise ValueError(\n                \"weighted_loss is True but encoding_weights is None\"\n            )\n        self.NLLL = nn.NLLLoss(weight=encoding_weights)\n    else:\n        self.NLLL = nn.NLLLoss(weight=None)\n    self.MSE = nn.MSELoss()\n    self.hamming = MulticlassHammingDistance(\n        num_classes=encodings, average=\"micro\"\n    )\n\n    self.use_mask = kwargs.get(\"use_mask\", True)  # deafult to use mask\n\n    self.encoder = Encoder(\n        input_size=encodings,\n        hidden_size=self.hidden_size,\n        num_layers=hidden_layers,\n        dropout=dropout,\n    )\n    self.decoder = AttnDecoderRNN(\n        input_size=encodings,\n        hidden_size=self.hidden_size,\n        output_size=encodings + 1,  # act encoding plus one for duration\n        max_length=self.steps,\n        sos=0,\n    )\n    flat_size_encode = self.hidden_layers * self.hidden_size * 2\n    self.fc_mu = nn.Linear(flat_size_encode, latent_dim)\n    self.fc_var = nn.Linear(flat_size_encode, latent_dim)\n    self.fc_hidden = nn.Linear(latent_dim, flat_size_encode)\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.MSE","title":"<code>MSE = nn.MSELoss()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.NLLL","title":"<code>NLLL = nn.NLLLoss(weight=encoding_weights)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.decoder","title":"<code>decoder = AttnDecoderRNN(input_size=encodings, hidden_size=self.hidden_size, output_size=encodings + 1, max_length=self.steps, sos=0)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.encoder","title":"<code>encoder = Encoder(input_size=encodings, hidden_size=self.hidden_size, num_layers=hidden_layers, dropout=dropout)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.encoding_weights","title":"<code>encoding_weights = encoding_weights</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.encodings","title":"<code>encodings = encodings</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.fc_hidden","title":"<code>fc_hidden = nn.Linear(latent_dim, flat_size_encode)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.fc_mu","title":"<code>fc_mu = nn.Linear(flat_size_encode, latent_dim)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.fc_var","title":"<code>fc_var = nn.Linear(flat_size_encode, latent_dim)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.hamming","title":"<code>hamming = MulticlassHammingDistance(num_classes=encodings, average='micro')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.hidden_layers","title":"<code>hidden_layers = hidden_layers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.hidden_size","title":"<code>hidden_size = hidden_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.latent_dim","title":"<code>latent_dim = latent_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.teacher_forcing_ratio","title":"<code>teacher_forcing_ratio = teacher_forcing_ratio</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.use_mask","title":"<code>use_mask = kwargs.get('use_mask', True)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.decode","title":"<code>decode(z, target=None, **kwargs)</code>","text":"<p>Decode latent sample to batch of output sequences.</p> PARAMETER  DESCRIPTION <code>z</code> <p>Latent space batch [N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Output sequence batch [N, steps, acts].</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def decode(self, z: Tensor, target=None, **kwargs) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Decode latent sample to batch of output sequences.\n\n    Args:\n        z (tensor): Latent space batch [N, latent_dims].\n\n    Returns:\n        tensor: Output sequence batch [N, steps, acts].\n    \"\"\"\n    # initialize hidden state as inputs\n    h = self.fc_hidden(z)\n\n    # initialize hidden state\n    hidden = h.unflatten(\n        1, (2 * self.hidden_layers, self.hidden_size)\n    ).permute(\n        1, 0, 2\n    )  # ([2xhidden, N, layers])\n    hidden = hidden.split(\n        self.hidden_layers\n    )  # ([hidden, N, layers, [hidden, N, layers]])\n    batch_size = z.shape[0]\n\n    if target is not None and torch.rand(1) &lt; self.teacher_forcing_ratio:\n        # use teacher forcing\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=target\n        )\n    else:\n        log_probs, probs = self.decoder(\n            batch_size=batch_size, hidden=hidden, target=None\n        )\n\n    return log_probs, probs\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.encode","title":"<code>encode(input)</code>","text":"<p>Encodes the input by passing through the encoder network.</p> PARAMETER  DESCRIPTION <code>input</code> <p>Input sequence batch [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>list[Tensor]</code> <p>list[tensor]: Latent layer input (means and variances) [N, latent_dims].</p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def encode(self, input: Tensor) -&gt; list[Tensor]:\n    \"\"\"Encodes the input by passing through the encoder network.\n\n    Args:\n        input (tensor): Input sequence batch [N, steps, acts].\n\n    Returns:\n        list[tensor]: Latent layer input (means and variances) [N, latent_dims].\n    \"\"\"\n    # batch_size, seq_len, feature_dim = x.shape\n    hidden = self.encoder(input)\n    # flatten last hidden cell layer\n    flat = torch.cat(hidden).permute(1, 0, 2).flatten(start_dim=1)\n\n    # Split the result into mu and var components\n    # of the latent Gaussian distribution\n    mu = self.fc_mu(flat)\n    log_var = self.fc_var(flat)\n\n    return [mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.forward","title":"<code>forward(x, target=None, **kwargs)</code>","text":"<p>Forward pass, also return latent parameterization.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Input sequences [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>List[Tensor]</code> <p>list[tensor]: [Output [N, steps, acts], Input [N, steps, acts], mu [N, latent_dims], var [N, latent_dims]].</p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def forward(self, x: Tensor, target=None, **kwargs) -&gt; List[Tensor]:\n    \"\"\"Forward pass, also return latent parameterization.\n\n    Args:\n        x (tensor): Input sequences [N, steps, acts].\n\n    Returns:\n        list[tensor]: [Output [N, steps, acts], Input [N, steps, acts], mu [N, latent_dims], var [N, latent_dims]].\n    \"\"\"\n    mu, log_var = self.encode(x)\n    z = self.reparameterize(mu, log_var)\n    log_prob_y, prob_y = self.decode(z, target=target)\n    return [log_prob_y, prob_y, x, mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.generate","title":"<code>generate(x, current_device, **kwargs)</code>","text":"<p>Given an encoder input, return reconstructed output.</p> PARAMETER  DESCRIPTION <code>x</code> <p>[N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def generate(self, x: Tensor, current_device: int, **kwargs) -&gt; Tensor:\n    \"\"\"Given an encoder input, return reconstructed output.\n\n    Args:\n        x (tensor): [N, steps, acts].\n\n    Returns:\n        tensor: [N, steps, acts].\n    \"\"\"\n    prob_samples = self.forward(x)[1]\n    prob_samples = prob_samples.to(current_device)\n    return prob_samples\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.loss_function","title":"<code>loss_function(log_probs, _, input, mu, log_var, mask, **kwargs)</code>","text":"<p>Computes the VAE loss function.</p> <p>Splits the input into activity and duration, and the recons into activity and duration.</p> RETURNS DESCRIPTION <code>dict</code> <p>Losses.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def loss_function(\n    self, log_probs, _, input, mu, log_var, mask, **kwargs\n) -&gt; dict:\n    r\"\"\"Computes the VAE loss function.\n\n    Splits the input into activity and duration, and the recons into activity and duration.\n\n    Returns:\n        dict: Losses.\n    \"\"\"\n\n    kld_weight = kwargs[\"kld_weight\"]\n    duration_weight = kwargs[\"duration_weight\"]\n\n    # unpack act probs and durations\n    target_acts, target_durations = self.unpack_encoding(input)\n    pred_acts, pred_durations = self.unpack_encoding(log_probs)\n\n    if self.use_mask:  # default is to use masking\n        flat_mask = mask.view(-1).bool()\n    else:\n        flat_mask = torch.ones_like(target_acts).view(-1).bool()\n\n    # activity encodng\n    recon_acts_nlll = self.NLLL(\n        pred_acts.view(-1, self.encodings)[flat_mask],\n        target_acts.view(-1).long()[flat_mask],\n    )\n\n    # duration encodng\n    recon_dur_mse = duration_weight * self.MSE(\n        pred_durations.view(-1)[flat_mask],\n        target_durations.view(-1)[flat_mask],\n    )\n\n    # combined\n    recons_loss = recon_acts_nlll + recon_dur_mse\n\n    recon_argmax = torch.argmax(pred_acts, dim=-1)\n    recons_ham_loss = self.hamming(\n        recon_argmax, target_acts.squeeze().long()\n    )\n\n    kld_loss = kld_weight * torch.mean(\n        -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1),\n        dim=0,\n    )\n\n    loss = recons_loss + kld_loss\n    return {\n        \"loss\": loss,\n        \"recon_loss\": recons_loss.detach(),\n        \"recon_act_loss\": recon_acts_nlll.detach(),\n        \"recon_dur_loss\": recon_dur_mse.detach(),\n        \"recons_ham_loss\": recons_ham_loss.detach(),\n        \"KLD\": kld_loss.detach(),\n    }\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.pack_encoding","title":"<code>pack_encoding(acts, durations)</code>","text":"<p>Pack the activity and duration into input.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>Activity [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> <code>durations</code> <p>Duration [N, steps, 1].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>Input sequences [N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def pack_encoding(self, acts: Tensor, durations: Tensor) -&gt; Tensor:\n    \"\"\"Pack the activity and duration into input.\n\n    Args:\n        acts (tensor): Activity [N, steps, acts].\n        durations (tensor): Duration [N, steps, 1].\n\n    Returns:\n        tensor: Input sequences [N, steps, acts].\n    \"\"\"\n    return torch.cat((acts, durations), dim=-1)\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.predict_step","title":"<code>predict_step(z, current_device, **kwargs)</code>","text":"<p>Given samples from the latent space, return the corresponding decoder space map.</p> PARAMETER  DESCRIPTION <code>z</code> <p>[N, latent_dims].</p> <p> TYPE: <code>tensor</code> </p> <code>current_device</code> <p>Device to run the model.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N, steps, acts].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def predict_step(self, z: Tensor, current_device: int, **kwargs) -&gt; Tensor:\n    \"\"\"Given samples from the latent space, return the corresponding decoder space map.\n\n    Args:\n        z (tensor): [N, latent_dims].\n        current_device (int): Device to run the model.\n\n    Returns:\n        tensor: [N, steps, acts].\n    \"\"\"\n    z = z.to(current_device)\n    prob_samples = self.decode(z)[1]\n    return prob_samples\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick to sample from N(mu, var) from N(0,1).</p> PARAMETER  DESCRIPTION <code>mu</code> <p>Mean of the latent Gaussian [N x latent_dims].</p> <p> TYPE: <code>tensor</code> </p> <code>logvar</code> <p>Standard deviation of the latent Gaussian [N x latent_dims].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[N x latent_dims].</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def reparameterize(self, mu: Tensor, logvar: Tensor) -&gt; Tensor:\n    \"\"\"Reparameterization trick to sample from N(mu, var) from N(0,1).\n\n    Args:\n        mu (tensor): Mean of the latent Gaussian [N x latent_dims].\n        logvar (tensor): Standard deviation of the latent Gaussian [N x latent_dims].\n\n    Returns:\n        tensor: [N x latent_dims].\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return eps * std + mu\n</code></pre>"},{"location":"reference/caveat/models/seq/transformer/#caveat.models.seq.transformer.Transformer.unpack_encoding","title":"<code>unpack_encoding(input)</code>","text":"<p>Split the input into activity and duration.</p> PARAMETER  DESCRIPTION <code>input</code> <p>Input sequences [N, steps, acts].</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor]</code> <p>tuple[tensor, tensor]: [activity [N, steps, acts], duration [N, steps, 1]].</p> Source code in <code>caveat/models/seq/transformer.py</code> <pre><code>def unpack_encoding(self, input: Tensor) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Split the input into activity and duration.\n\n    Args:\n        input (tensor): Input sequences [N, steps, acts].\n\n    Returns:\n        tuple[tensor, tensor]: [activity [N, steps, acts], duration [N, steps, 1]].\n    \"\"\"\n    acts = input[:, :, :-1].contiguous()\n    durations = input[:, :, -1:].contiguous()\n    return acts, durations\n</code></pre>"},{"location":"reference/caveat/models/utils/","title":"caveat.models.utils","text":""},{"location":"reference/caveat/models/utils/#caveat.models.utils.calc_output_padding","title":"<code>calc_output_padding(size)</code>","text":"<p>Calculate output padding for a transposed convolution such that output dims will match dimensions of inputs to a convolution of given size. For each dimension, padding is set to 1 if even size, otherwise 0.</p> PARAMETER  DESCRIPTION <code>size</code> <p>input size (h, w)</p> <p> TYPE: <code>Union[tuple[int, int, int], int]</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: required padding</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def calc_output_padding(size: Union[tuple[int, int, int], int]) -&gt; np.array:\n    \"\"\"Calculate output padding for a transposed convolution such that output dims will\n    match dimensions of inputs to a convolution of given size.\n    For each dimension, padding is set to 1 if even size, otherwise 0.\n\n    Args:\n        size (Union[tuple[int, int, int], int]): input size (h, w)\n\n    Returns:\n        np.array: required padding\n    \"\"\"\n    if isinstance(size, int):\n        size = (0, size, size)\n    _, h, w = size\n    return (int(h % 2 == 0), int(w % 2 == 0))\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.conv_size","title":"<code>conv_size(size, kernel_size=3, stride=2, padding=1, dilation=1)</code>","text":"<p>Calculate output dimensions for 2d convolution.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Input size, may be integer if symetric.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> </p> <code>kernel_size</code> <p>Kernel_size. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>Stride. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>Input padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>Dilation. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: Output size.</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def conv_size(\n    size: Union[tuple[int, int], int],\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n    dilation: Union[tuple[int, int], int] = 1,\n) -&gt; np.array:\n    \"\"\"Calculate output dimensions for 2d convolution.\n\n    Args:\n        size (Union[tuple[int, int], int]): Input size, may be integer if symetric.\n        kernel_size (Union[tuple[int, int], int], optional): Kernel_size. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): Stride. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): Input padding. Defaults to 1.\n        dilation (Union[tuple[int, int], int], optional): Dilation. Defaults to 1.\n\n    Returns:\n        np.array: Output size.\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    if isinstance(padding, int):\n        padding = (padding, padding)\n    if isinstance(dilation, int):\n        dilation = (dilation, dilation)\n    return (\n        np.array(size)\n        + 2 * np.array(padding)\n        - np.array(dilation) * (np.array(kernel_size) - 1)\n        - 1\n    ) // np.array(stride) + 1\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.hot_argmax","title":"<code>hot_argmax(batch, axis=-1)</code>","text":"<p>Encoded given axis as one-hot based on argmax for that axis.</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Input tensor.</p> <p> TYPE: <code>tensor</code> </p> <code>axis</code> <p>Axis index to encode. Defaults to -1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>One hot encoded tensor.</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/utils.py</code> <pre><code>def hot_argmax(batch: tensor, axis: int = -1) -&gt; tensor:\n    \"\"\"Encoded given axis as one-hot based on argmax for that axis.\n\n    Args:\n        batch (tensor): Input tensor.\n        axis (int, optional): Axis index to encode. Defaults to -1.\n\n    Returns:\n        tensor: One hot encoded tensor.\n    \"\"\"\n    batch = batch.swapaxes(axis, -1)\n    argmax = batch.argmax(axis=-1)\n    eye = torch.eye(batch.shape[-1])\n    eye = eye.to(current_device())\n    batch = eye[argmax]\n    return batch.swapaxes(axis, -1)\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.transconv_size","title":"<code>transconv_size(size, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)</code>","text":"<p>Calculate output dimension for 2d transpose convolution.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Input size, may be integer if symetric.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> </p> <code>kernel_size</code> <p>Kernel size. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>Stride. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>Input padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>Dilation. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>output_padding</code> <p>Output padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: Output size.</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def transconv_size(\n    size: Union[tuple[int, int], int],\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n    dilation: Union[tuple[int, int], int] = 1,\n    output_padding: Union[tuple[int, int], int] = 1,\n) -&gt; np.array:\n    \"\"\"Calculate output dimension for 2d transpose convolution.\n\n    Args:\n        size (Union[tuple[int, int], int]): Input size, may be integer if symetric.\n        kernel_size (Union[tuple[int, int], int], optional): Kernel size. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): Stride. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): Input padding. Defaults to 1.\n        dilation (Union[tuple[int, int], int], optional): Dilation. Defaults to 1.\n        output_padding (Union[tuple[int, int], int], optional): Output padding. Defaults to 1.\n\n    Returns:\n        np.array: Output size.\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    if isinstance(padding, int):\n        padding = (padding, padding)\n    if isinstance(dilation, int):\n        dilation = (dilation, dilation)\n    if isinstance(output_padding, int):\n        output_padding = (output_padding, output_padding)\n    return (\n        (np.array(size) - 1) * np.array(stride)\n        - 2 * np.array(padding)\n        + np.array(dilation) * (np.array(kernel_size) - 1)\n        + np.array(output_padding)\n        + 1\n    )\n</code></pre>"},{"location":"reference/caveat/report/","title":"caveat.report","text":""},{"location":"reference/caveat/report/#caveat.report.frequency_jobs","title":"<code>frequency_jobs = [(('agg. participation', frequency.activity_frequencies), feature_length, ('average freq.', average_weight), ('MAE', mae))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.participation_prob_jobs","title":"<code>participation_prob_jobs = [(('participation', participation.participation_prob_by_act), feature_weight, ('prob.', average), ('MAPE', mape)), (('joint participation', participation.joint_participation_prob), feature_weight, ('prob.', average), ('MAPE', mape))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.participation_rate_jobs","title":"<code>participation_rate_jobs = [(('participation rate', participation.participation_rates), feature_weight, ('av. rate', average), ('EMD', emd)), (('activity participation rates', participation.participation_rates_by_act), feature_weight, ('av. rate', average), ('EMD', emd)), (('enumerated participation rates', participation.participation_rates_by_act_enum), feature_weight, ('av. rate', average), ('EMD', emd))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.structure_jobs","title":"<code>structure_jobs = [(('duration', structural.duration_consistency), feature_weight, ('duration', average), ('MAE', abs_av_diff)), (('home based', structural.start_and_end_acts), feature_weight, ('prob.', average), ('MAE', abs_av_diff))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.time_jobs","title":"<code>time_jobs = [(('start times', times.start_times_by_act), feature_weight, ('average', average), ('EMD', emd)), (('end times', times.end_times_by_act), feature_weight, ('average', average), ('EMD', emd)), (('durations', times.durations_by_act), feature_weight, ('average', average), ('EMD', emd)), (('start-durations', times.start_and_duration_by_act_bins), feature_weight, ('average', average2d), ('EMD', emd))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.transition_jobs","title":"<code>transition_jobs = [(('transition pairs', transitions.transitions_by_act), feature_weight, ('av. rate', average), ('EMD', emd)), (('transition 3s', transitions.transition_3s_by_act), feature_weight, ('av. rate', average), ('EMD', emd))]</code>  <code>module-attribute</code>","text":""},{"location":"reference/caveat/report/#caveat.report.describe","title":"<code>describe(observed, ys, feature, head=None)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def describe(\n    observed: DataFrame,\n    ys: dict[str, DataFrame],\n    feature: Callable,\n    head: Optional[int] = None,\n) -&gt; DataFrame:\n    x_report = feature(observed)\n    x_report.name = \"observed\"\n    report = DataFrame(x_report)\n    for name, y in ys.items():\n        y_report = feature(y)\n        y_report.name = name\n        report = concat([report, y_report], axis=1)\n        report = report.fillna(0)\n    report[\"mean\"] = report[ys.keys()].mean(axis=1)\n    report[\"mean delta\"] = report[\"mean\"] - report.observed\n    report[\"std\"] = report[ys.keys()].std(axis=1)\n    if head is not None:\n        report = report.head(head)\n    return report\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.describe_feature","title":"<code>describe_feature(model, features, describe)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def describe_feature(\n    model: str,\n    features: dict[str, tuple[np.array, np.array]],\n    describe: Callable,\n):\n    feature_description = describe(features)\n    feature_description.name = model\n    return feature_description\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.extract_default","title":"<code>extract_default(features)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def extract_default(features: dict[str, tuple[np.array, np.array]]):\n    # we use a single feature of zeros as required\n    default_sample = next(iter(features.values()))\n    default_shape = list(default_sample[0].shape)\n    default_shape[0] = 1\n    default_support = np.zeros(default_shape)\n    return (default_support, np.array([1]))\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.report","title":"<code>report(observed, sampled, log_dir=None, report_description=True, report_scores=True, head=10, verbose=True, report_creativity=True)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def report(\n    observed: DataFrame,\n    sampled: dict[str, DataFrame],\n    log_dir: Optional[Path] = None,\n    report_description: bool = True,\n    report_scores: bool = True,\n    head: int = 10,\n    verbose: bool = True,\n    report_creativity: bool = True,\n):\n    descriptions = DataFrame()\n    distances = DataFrame()\n\n    if report_creativity:\n        observed_hash = creativity.hash_population(observed)\n        observed_diversity = creativity.diversity(observed, observed_hash)\n        creativity_descriptions = DataFrame(\n            {\n                \"feature count\": [observed.pid.nunique()] * 2,\n                \"observed\": [observed_diversity, 1],\n            }\n        )\n        creativity_distance = creativity_descriptions.copy()\n\n        creativity_descs = []\n        creativity_scs = []\n        for model, y in sampled.items():\n            y_hash = creativity.hash_population(y)\n            y_diversity = creativity.diversity(y, y_hash)\n            creativity_descs.append(\n                Series(\n                    [y_diversity, creativity.novelty(observed_hash, y, y_hash)],\n                    name=model,\n                )\n            )\n            creativity_scs.append(\n                Series(\n                    [\n                        abs(y_diversity - observed_diversity),\n                        creativity.conservatism(observed_hash, y, y_hash),\n                    ],\n                    name=model,\n                )\n            )\n        creativity_descs.append(\n            Series([\"diversity\", \"novelty\"], name=\"description\")\n        )\n        creativity_scs.append(\n            Series([\"abs error\", \"conservatism\"], name=\"distance\")\n        )\n\n        descriptions = concat(\n            [creativity_descriptions, concat(creativity_descs, axis=1)], axis=1\n        )\n        distances = concat(\n            [creativity_distance, concat(creativity_scs, axis=1)], axis=1\n        )\n\n        idx = MultiIndex.from_tuples(\n            [\n                (\"creativity\", \"diversity\", \"all\"),\n                (\"creativity\", \"conservatism\", \"all\"),\n            ],\n            names=[\"domain\", \"feature\", \"segment\"],\n        )\n        descriptions.index = idx\n        distances.index = idx\n\n    for domain, jobs in [\n        (\"structure\", structure_jobs),\n        (\"frequency\", frequency_jobs),\n        (\"participation_probs\", participation_prob_jobs),\n        (\"participation_rates\", participation_rate_jobs),\n        (\"transitions\", transition_jobs),\n        (\"scheduling\", time_jobs),\n    ]:\n        for feature, size, description_job, distance_job in jobs:\n            # unpack tuples\n            feature_name, feature = feature\n            print(feature_name)\n            description_name, describe = description_job\n            distance_name, distance_metric = distance_job\n\n            # build observed features\n            observed_features = feature(observed)\n\n            # need to create a default feature for missing sampled features\n            default = extract_default(observed_features)\n\n            # create an observed feature count and description\n            feature_weight = size(observed_features)\n            feature_weight.name = \"feature count\"\n\n            description_job = describe(observed_features)\n            feature_descriptions = DataFrame(\n                {\"feature count\": feature_weight, \"observed\": description_job}\n            )\n\n            # sort by count and description, drop description and add distance description\n            feature_descriptions = feature_descriptions.sort_values(\n                ascending=False, by=[\"feature count\", \"observed\"]\n            )\n\n            feature_distances = feature_descriptions.copy()\n\n            # iterate through samples\n            for model, y in sampled.items():\n                synth_features = feature(y)\n                feature_descriptions = concat(\n                    [\n                        feature_descriptions,\n                        describe_feature(model, synth_features, describe),\n                    ],\n                    axis=1,\n                )\n\n                # report sampled distances\n                feature_distances = concat(\n                    [\n                        feature_distances,\n                        score_features(\n                            model,\n                            observed_features,\n                            synth_features,\n                            distance_metric,\n                            default,\n                        ),\n                    ],\n                    axis=1,\n                )\n\n            # add domain and feature name to index\n            feature_descriptions[\"description\"] = description_name\n            feature_distances[\"distance\"] = distance_name\n            feature_descriptions.index = MultiIndex.from_tuples(\n                [(domain, feature_name, f) for f in feature_descriptions.index],\n                name=[\"domain\", \"feature\", \"segment\"],\n            )\n            feature_distances.index = MultiIndex.from_tuples(\n                [(domain, feature_name, f) for f in feature_distances.index],\n                name=[\"domain\", \"feature\", \"segment\"],\n            )\n            descriptions = concat([descriptions, feature_descriptions], axis=0)\n            distances = concat([distances, feature_distances], axis=0)\n\n    # remove nans\n    descriptions = descriptions.fillna(0)\n    distances = distances.fillna(0)\n\n    # features\n    features_descriptions = (\n        descriptions.drop(\"description\", axis=1)\n        .groupby([\"domain\", \"feature\"])\n        .apply(weighted_av)\n    )\n    features_descriptions[\"description\"] = (\n        descriptions[\"description\"].groupby([\"domain\", \"feature\"]).first()\n    )\n\n    features_distances = (\n        distances.drop(\"distance\", axis=1)\n        .groupby([\"domain\", \"feature\"])\n        .apply(weighted_av)\n    )\n    features_distances[\"distance\"] = (\n        distances[\"distance\"].groupby([\"domain\", \"feature\"]).first()\n    )\n\n    # rank\n    feature_ranks = features_distances.drop(\n        [\"observed\", \"distance\"], axis=1\n    ).rank(axis=1, method=\"min\")\n    col_ranks = feature_ranks.sum(axis=0)\n    ranked = [i for _, i in sorted(zip(col_ranks, col_ranks.index))]\n    feature_ranks = feature_ranks[ranked]\n\n    # themes\n    domain_descriptions = (\n        features_descriptions.drop(\"description\", axis=1)\n        .groupby(\"domain\")\n        .mean()\n    )\n    domain_distances = (\n        features_distances.drop(\"distance\", axis=1).groupby(\"domain\").mean()\n    )\n\n    # rank\n    domain_ranks = domain_distances.drop(\"observed\", axis=1).rank(\n        axis=1, method=\"min\"\n    )\n    col_ranks = domain_ranks.sum(axis=0)\n    ranked = [i for _, i in sorted(zip(col_ranks, col_ranks.index))]\n    domain_ranks = domain_ranks[ranked]\n\n    if log_dir is not None:\n        descriptions.to_csv(Path(log_dir, \"descriptions.csv\"))\n        features_descriptions.to_csv(Path(log_dir, \"feature_descriptions.csv\"))\n        domain_descriptions.to_csv(Path(log_dir, \"domain_descriptions.csv\"))\n        distances.to_csv(Path(log_dir, \"evaluation.csv\"))\n        features_distances.to_csv(Path(log_dir, \"feature_evaluation.csv\"))\n        domain_distances.to_csv(Path(log_dir, \"domain_evaluation.csv\"))\n\n    if head is not None:\n        descriptions_short = descriptions.groupby([\"domain\", \"feature\"]).head(\n            head\n        )\n        distances_short = distances.groupby([\"domain\", \"feature\"]).head(head)\n        if log_dir is not None:\n            descriptions_short.to_csv(Path(log_dir, \"descriptions_short.csv\"))\n            distances_short.to_csv(Path(log_dir, \"evaluation_short.csv\"))\n    else:\n        descriptions_short = descriptions\n        distances_short = distances\n\n    if verbose:\n        print(\"\\nDescriptions:\")\n        print(\n            descriptions_short.to_markdown(\n                tablefmt=\"fancy_grid\", floatfmt=\".3f\"\n            )\n        )\n        print(\"\\nEvalutions (Distance):\")\n        print(\n            distances_short.to_markdown(tablefmt=\"fancy_grid\", floatfmt=\".3f\")\n        )\n    print(\"\\nFeature Descriptions:\")\n    print(\n        features_descriptions.to_markdown(tablefmt=\"fancy_grid\", floatfmt=\".3f\")\n    )\n    print(\"\\nFeature Evaluations (Distance):\")\n    print(features_distances.to_markdown(tablefmt=\"fancy_grid\", floatfmt=\".3f\"))\n    print(\"\\nFeature Evaluations (Ranked):\")\n    print(feature_ranks.to_markdown(tablefmt=\"fancy_grid\"))\n    print(\"\\nDomain Descriptions:\")\n    print(\n        domain_descriptions.to_markdown(tablefmt=\"fancy_grid\", floatfmt=\".3f\")\n    )\n    print(\"\\nDomain Evaluations (Distance):\")\n    print(domain_distances.to_markdown(tablefmt=\"fancy_grid\", floatfmt=\".3f\"))\n    print(\"\\nDomain Evaluations (Ranked):\")\n    print(domain_ranks.to_markdown(tablefmt=\"fancy_grid\"))\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.score_features","title":"<code>score_features(model, a, b, distance, default)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def score_features(\n    model: str,\n    a: dict[str, tuple[np.array, np.array]],\n    b: dict[str, tuple[np.array, np.array]],\n    distance: Callable,\n    default: tuple[np.array, np.array],\n):\n    index = set(a.keys()) | set(b.keys())\n    metrics = Series(\n        {k: distance(a.get(k, default), b.get(k, default)) for k in index},\n        name=model,\n    )\n    metrics = metrics.fillna(0)\n    metrics = metrics[np.isfinite(metrics)]\n    return metrics\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.weighted_av","title":"<code>weighted_av(report, weight_col='feature count')</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def weighted_av(report: DataFrame, weight_col: str = \"feature count\") -&gt; Series:\n    weights = report[weight_col]\n    total = sum(weights)\n    cols = list(report.columns)\n    cols.remove(weight_col)\n    scores = DataFrame()\n    for c in cols:\n        scores[c] = report[c] * weights / total\n    return scores.sum()\n</code></pre>"},{"location":"reference/caveat/run/","title":"caveat.run","text":""},{"location":"reference/caveat/run/#caveat.run.batch_command","title":"<code>batch_command(batch_config)</code>","text":"<p>Runs a batch of training and reporting runs based on the provided configuration.</p> PARAMETER  DESCRIPTION <code>batch_config</code> <p>A dictionary containing the configuration for each training job.</p> <p> TYPE: <code>dict[dict]</code> </p> RETURNS DESCRIPTION <p>None</p> Source code in <code>caveat/run.py</code> <pre><code>def batch_command(batch_config: dict):\n    \"\"\"\n    Runs a batch of training and reporting runs based on the provided configuration.\n\n    Args:\n        batch_config (dict[dict]): A dictionary containing the configuration for each training job.\n\n    Returns:\n        None\n    \"\"\"\n    global_config = batch_config.pop(\"global\")\n    logger_params = global_config.get(\"logging_params\", {})\n    name = str(\n        logger_params.get(\n            \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        )\n    )\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"), name)\n\n    seed = batch_config.pop(\"seed\", seeder())\n\n    data_path = global_config[\"data_path\"]\n    observed = data.load_and_validate(data_path)\n    print(f\"Loaded {len(observed)} sequences from {data_path}\")\n\n    sampled = {}\n    for name, config in batch_config.items():\n        name = str(name)\n        combined_config = global_config.copy()\n        combined_config.update(config)\n        sampled[name] = train_and_sample(\n            name, observed, combined_config, log_dir, seed\n        )\n\n    report.report(observed, sampled, log_dir)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.build_dataloader","title":"<code>build_dataloader(config, dataset)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def build_dataloader(\n    config: dict, dataset: encoders.BaseEncodedPlans\n) -&gt; data.DataModule:\n    data_loader_params = config.get(\"loader_params\", {})\n    datamodule = data.DataModule(data=dataset, **data_loader_params)\n    datamodule.setup()\n    return datamodule\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.build_encoder","title":"<code>build_encoder(config)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def build_encoder(config: dict) -&gt; encoders.BaseEncoder:\n    encoder_name = config[\"encoder_params\"][\"name\"]\n    data_encoder = encoders.library[encoder_name](**config[\"encoder_params\"])\n    return data_encoder\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.build_experiment","title":"<code>build_experiment(dataset, config)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def build_experiment(\n    dataset: encoders.BaseEncodedPlans, config: dict\n) -&gt; Experiment:\n    model_name = config[\"model_params\"][\"name\"]\n    model = models.library[model_name]\n    model = model(\n        in_shape=dataset.shape(),\n        encodings=dataset.encodings,\n        encoding_weights=dataset.encoding_weights,\n        **config[\"model_params\"],\n    )\n    return Experiment(model, **config[\"experiment_params\"])\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.build_trainer","title":"<code>build_trainer(logger, config)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def build_trainer(logger: TensorBoardLogger, config: dict) -&gt; Trainer:\n    trainer_config = config.get(\"trainer_params\", {})\n    patience = trainer_config.pop(\"patience\", 5)\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=Path(logger.log_dir, \"checkpoints\"),\n        monitor=\"val_recon_loss\",\n        save_top_k=2,\n        save_weights_only=True,\n    )\n    return Trainer(\n        logger=logger,\n        callbacks=[\n            EarlyStopping(\n                monitor=\"val_recon_loss\",\n                patience=patience,\n                stopping_threshold=0.0,\n            ),\n            LearningRateMonitor(),\n            checkpoint_callback,\n        ],\n        **trainer_config,\n    )\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.initiate_logger","title":"<code>initiate_logger(save_dir, name)</code>","text":"<p>Initializes a TensorBoardLogger object for logging training progress.</p> PARAMETER  DESCRIPTION <code>save_dir</code> <p>The directory where the logs will be saved.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the logger.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>TensorBoardLogger</code> <p>The initialized TensorBoardLogger object.</p> <p> TYPE: <code>TensorBoardLogger</code> </p> Source code in <code>caveat/run.py</code> <pre><code>def initiate_logger(save_dir: Union[Path, str], name: str) -&gt; TensorBoardLogger:\n    \"\"\"\n    Initializes a TensorBoardLogger object for logging training progress.\n\n    Args:\n        save_dir (str): The directory where the logs will be saved.\n        name (str): The name of the logger.\n\n    Returns:\n        TensorBoardLogger: The initialized TensorBoardLogger object.\n    \"\"\"\n    tb_logger = TensorBoardLogger(save_dir=save_dir, name=name)\n    Path(f\"{tb_logger.log_dir}/samples\").mkdir(exist_ok=True, parents=True)\n    Path(f\"{tb_logger.log_dir}/reconstructions\").mkdir(\n        exist_ok=True, parents=True\n    )\n    return tb_logger\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.nrun_command","title":"<code>nrun_command(config, n=5)</code>","text":"<p>Repeat a single run while varying the seed, report on variance.</p> PARAMETER  DESCRIPTION <code>config</code> <p>A dictionary containing the configuration parameters.</p> <p> TYPE: <code>dict</code> </p> <code>n</code> <p>The number of times to repeat the run. Defaults to 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> Source code in <code>caveat/run.py</code> <pre><code>def nrun_command(config: dict, n: int = 5):\n    \"\"\"\n    Repeat a single run while varying the seed, report on variance.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters.\n        n (int, optional): The number of times to repeat the run. Defaults to 5.\n    \"\"\"\n    logger_params = config.get(\"logging_params\", {})\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"))\n    name = str(\n        logger_params.get(\n            \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        )\n    )\n    write_path = log_dir / name\n\n    data_path = Path(config[\"data_path\"])\n    observed = data.load_and_validate(data_path)\n\n    sampled = {\n        f\"{name}_{i}\": train_and_sample(\n            f\"{name}_{i}\", observed, config, log_dir\n        )\n        for i in range(n)\n    }\n\n    report.report(observed, sampled, write_path)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.report_command","title":"<code>report_command(observed_path, log_dir, name='synthetic.csv', verbose=False, head=10, batch=False)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def report_command(\n    observed_path: Path,\n    log_dir: Path,\n    name: str = \"synthetic.csv\",\n    verbose: bool = False,\n    head: int = 10,\n    batch: bool = False,\n):\n    observed_path = Path(observed_path)\n    log_dir = Path(log_dir)\n    observed = data.load_and_validate(observed_path)\n    sampled = {}\n    if batch:\n        paths = [p for p in log_dir.iterdir() if p.is_dir()]\n    else:\n        paths = [log_dir]\n\n    for experiment in paths:\n        # get most recent version\n        version = sorted([d for d in experiment.iterdir() if d.is_dir()])[-1]\n        path = experiment / version.name / name\n        sampled[experiment.name] = data.load_and_validate(path)\n\n    report.report(\n        observed=observed,\n        sampled=sampled,\n        log_dir=log_dir,\n        report_description=True,\n        report_scores=True,\n        report_creativity=True,\n        verbose=verbose,\n        head=head,\n    )\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.run_command","title":"<code>run_command(config)</code>","text":"<p>Runs the training and reporting process using the provided configuration.</p> PARAMETER  DESCRIPTION <code>config</code> <p>A dictionary containing the configuration parameters.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>caveat/run.py</code> <pre><code>def run_command(config: dict) -&gt; None:\n    \"\"\"\n    Runs the training and reporting process using the provided configuration.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters.\n\n    Returns:\n        None\n    \"\"\"\n    logger_params = config.get(\"logging_params\", {})\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"))\n    name = str(\n        logger_params.get(\n            \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        )\n    )\n    write_path = log_dir / name\n\n    seed = config.pop(\"seed\", seeder())\n\n    data_path = Path(config[\"data_path\"])\n    observed = data.load_and_validate(data_path)\n\n    print(f\"Loaded {len(observed)} sequences from {data_path}\")\n\n    sampled = {name: build_trainer(name, observed, config, log_dir, seed)}\n\n    report.report(observed, sampled, write_path)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.sample","title":"<code>sample(trainer, population_size, data_encoder, config, logger)</code>","text":"Source code in <code>caveat/run.py</code> <pre><code>def sample(\n    trainer: Trainer,\n    population_size: int,\n    data_encoder: encoders.BaseEncoder,\n    config: dict,\n    logger: TensorBoardLogger,\n) -&gt; DataFrame:\n    print(\"\\n======= Sampling =======\")\n    predict_loader = data.predict_dataloader(\n        population_size, config[\"model_params\"][\"latent_dim\"], 256\n    )\n    predictions = trainer.predict(ckpt_path=\"best\", dataloaders=predict_loader)\n    predictions = torch.concat(predictions)  # type: ignore\n    synthetic = data_encoder.decode(encoded=predictions)\n    data.validate(synthetic)\n    synthesis_path = Path(logger.log_dir, \"synthetic.csv\")\n    synthetic.to_csv(synthesis_path)\n    return synthetic\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.train_and_sample","title":"<code>train_and_sample(name, observed, config, log_dir, seed=None)</code>","text":"<p>Trains a model on the observed data, generates synthetic data using the trained model, and saves the synthetic data. Returns the synthetic data as a population DataFrame.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the experiment.</p> <p> TYPE: <code>str</code> </p> <code>observed</code> <p>The \"observed\" population data to train the model on.</p> <p> TYPE: <code>DataFrame</code> </p> <code>config</code> <p>A dictionary containing the configuration parameters for the experiment.</p> <p> TYPE: <code>dict</code> </p> <code>log_dir</code> <p>The directory to save the experiment logs and checkpoints.</p> <p> TYPE: <code>Path</code> </p> <code>seed</code> <p>The random seed to use for the experiment. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pandas DataFrame.</p> Source code in <code>caveat/run.py</code> <pre><code>def train_and_sample(\n    name: str,\n    observed: DataFrame,\n    config: dict,\n    log_dir: Path,\n    seed: Optional[int] = None,\n) -&gt; DataFrame:\n    \"\"\"\n    Trains a model on the observed data, generates synthetic data using the trained model,\n    and saves the synthetic data. Returns the synthetic data as a population DataFrame.\n\n    Args:\n        name (str): The name of the experiment.\n        observed (pandas.DataFrame): The \"observed\" population data to train the model on.\n        config (dict): A dictionary containing the configuration parameters for the experiment.\n        log_dir (pathlib.Path): The directory to save the experiment logs and checkpoints.\n        seed (int, optional): The random seed to use for the experiment. Defaults to None.\n\n    Returns:\n        pandas DataFrame.\n    \"\"\"\n    if cuda_available():\n        torch.set_float32_matmul_precision(\"medium\")\n    if seed is None:\n        seed = seeder()\n    torch.manual_seed(seed)\n\n    print(f\"\\n======= Training {name} =======\")\n\n    logger = initiate_logger(log_dir, name)\n\n    data_encoder = build_encoder(config)\n    encoded = data_encoder.encode(observed)\n    data_loader = build_dataloader(config, encoded)\n\n    experiment = build_experiment(encoded, config)\n    trainer = build_trainer(logger, config)\n    trainer.fit(experiment, datamodule=data_loader)\n\n    synthetic = sample(trainer, len(observed), data_encoder, config, logger)\n\n    return synthetic\n</code></pre>"}]}