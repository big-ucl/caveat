{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Generative models for human activity sequences.</p> <p> </p> <p>Caveat is for building models that generate human activity sequences.</p>"},{"location":"#framework","title":"Framework","text":"<p>Caveat provides a framework to train and test generative models. A model run is composed of:</p> <ul> <li>Data - see the caveat examples for synthetic and real data generation</li> <li>Encoder - see caveat encoders for available encoders</li> <li>Model - see caveat models for available models</li> <li>Report - see caveat report</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Once installed get started using <code>caveat --help</code>.</p> <p><code>caveat run --help</code></p> <p>Train and report on a model using <code>caveat run configs/toy_run.yaml</code>. The run data, encoder, model and other parameters are controlled using a run config. This will write results and tensorboard logs to <code>logs/</code> (this is configurable). Monitor or review training progress using <code>tensorboard --logdir=logs</code>.</p> <p><code>caveat batch --help</code></p> <p>Train and report on a batch of runs using a special batch config <code>caveat batch configs/toy_batch.yaml</code>. Batch allows comparison of multiple models and/or hyper-params as per the batch config.</p> <p><code>caveat nrun --help</code></p> <p>Run and report the variance of n of the same run using <code>caveat nrun configs/toy_run.yaml --n 3</code>. The config is as per a regular run config but <code>seed</code> is ignored.</p>"},{"location":"#data","title":"Data","text":"<p>Caveat requires a .csv format to represent a population of activity sequences:</p> pid act start end 0 home 0 390 0 work 390 960 0 home 960 1440 1 home 0 390 1 education 390 960 1 home 960 1440 <ul> <li>pid (person id) field is a unique identifier for each sequence</li> <li>act is a categorical value for the type of activity in the sequence</li> <li>start and end are the start and end times of the activities in the sequence</li> </ul> <p>We commonly refer to these as populations. Times are assumed to be in minutes and should be integers. Valid sequences should be complete, ie the start of an activity should be equal to the end of the previous. The convention is to start at midnight. Such that time can be thought of as minutes since midnight.</p> <p>There is an example toy population with 1000 sequences in <code>caveat/examples/data</code>. There are also example notebooks for:</p> <ul> <li>Generation of a synthetic population</li> <li>Generation of a population from UK travel diaries</li> </ul>"},{"location":"#encoder","title":"Encoder","text":"<p>We are keen to test different encodings (such as continuous sequences versus descretised time-steps). The exact encoding required will depend on the model structure being used.</p> <p>The encoder and it's parameters are defined in the config <code>encoder</code> group.</p> <p>Encoders are defined in the <code>encoders</code> module and should be accessed via <code>caveat.encoders.library</code>.</p> <p>Note that encoders must implement both an encode and decode method so that model outputs can be converted back into the population format for reporting.</p>"},{"location":"#model","title":"Model","text":"<p>The model and it's parameters are defined in the config <code>model</code> group. Models are trained until validation stabilises or until some max number of epochs.</p> <p>Models are defined in <code>models</code> and should be accessed via <code>caveat.models.library</code>. Models and their training should be specified via the config.</p> <p>The <code>data_loader</code>, <code>experiment</code> and <code>trainer</code> hyper-params are also configured by similarly named groups. These groups use the standard pytorch-lightning framework.</p>"},{"location":"#report","title":"Report","text":"<p>Each model (with training weights from the best performing validation step) is used to generate a new \"sythetic\" population.</p> <p>Sythetic populations are compared to the original \"observed\" population.</p> <p>Reporting the quality of generated populations is subjective. The <code>features</code> module provides functions for extracting features from populations. Such as \"average activity durations\". These are then used to make comparison metrics between the observed and sythetic populations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>caveat is an actively maintained and utilised project.</p>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<p>to report issues, request features, or exchange with our community, just follow the links below.</p> <p>Is something not working?</p> <p> Report a bug</p> <p>Missing information in our docs?</p> <p> Report a docs issue</p> <p>Want to submit an idea?</p> <p> Request a change</p> <p>Have a question or need help?</p> <p> Ask a question</p>"},{"location":"contributing/#developing-caveat","title":"Developing caveat","text":"<p>To find beginner-friendly existing bugs and feature requests you may like to start out with, take a look at our good first issues.</p>"},{"location":"contributing/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>To create a development environment for caveat, with all libraries required for development and quality assurance installed, it is easiest to install caveat using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li>Open the command line (or the \"miniforge prompt\" in Windows).</li> <li>Download (a.k.a., clone) the caveat repository: <code>git clone git@github.com:fredshone/caveat.git</code></li> <li>Change into the <code>caveat</code> directory: <code>cd caveat</code></li> <li>Create the caveat mamba environment: <code>mamba create -n caveat -c conda-forge --file requirements/base.txt --file requirements/dev.txt</code></li> <li>Activate the caveat mamba environment: <code>mamba activate caveat</code></li> <li>Install the caveat package into the environment, in editable mode and ignoring dependencies (we have dealt with those when creating the mamba environment): <code>pip install --no-deps -e .</code></li> </ol> <p>All together:</p> <pre><code>git clone git@github.com:fredshone/caveat.git\ncd caveat\nmamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch --file requirements/base.txt --file requirements/dev.txt\nmamba activate caveat\npip install --no-deps -e .\n</code></pre> <p>If installing directly with pip, you can install these libraries using the <code>dev</code> option, i.e., <code>pip install -e '.[dev]'</code> Either way, you should add your environment as a jupyter kernel, so the example notebooks can run in the tests: <code>ipython kernel install --user --name=caveat</code> If you plan to make changes to the code then please make regular use of the following tools to verify the codebase while you work:</p> <ul> <li><code>pre-commit</code>: run <code>pre-commit install</code> in your command line to load inbuilt checks that will run every time you commit your changes. The checks are: 1. check no large files have been staged, 2. lint python files for major errors, 3. format python files to conform with the PEP8 standard. You can also run these checks yourself at any time to ensure staged changes are clean by calling <code>pre-commit</code>.</li> <li><code>pytest</code> - run the unit test suite and check test coverage.</li> </ul> <p>Note</p> <p>If you already have an environment called <code>caveat</code> on your system (e.g., for a stable installation of the package), you will need to chose a different environment name. You will then need to add this as a pytest argument when running the tests: <code>pytest --nbmake-kernel=[my-env-name]</code>.</p>"},{"location":"contributing/#rapid-fire-testing","title":"Rapid-fire testing","text":"<p>The following options allow you to strip down the test suite to the bare essentials: 1. The test suite includes unit tests and integration tests (in the form of jupyter notebooks found in the <code>examples</code> directory). The integration tests can be slow, so if you want to avoid them during development, you should run <code>pytest tests/</code>. 2. You can avoid generating coverage reports, by adding the <code>--no-cov</code> argument: <code>pytest --no-cov</code>. 3. By default, the tests run with up to two parallel threads, to increase this to e.g. 4 threads: <code>pytest -n4</code>.</p> <p>All together:</p> <pre><code>pytest tests/ --no-cov -n4\n</code></pre> <p>Note</p> <p>You cannot debug failing tests and have your tests run in parallel, you will need to set <code>-n0</code> if using the <code>--pdb</code> flag</p>"},{"location":"contributing/#memory-profiling","title":"Memory profiling","text":"<p>Note</p> <p>When you open a pull request (PR), one of the GitHub actions will run memory profiling for you. This means you don't have to do any profiling locally. However, if you can, it is still good practice to do so as you will catch issues earlier.</p> <p>caveat can be memory intensive; we like to ensure that any development to the core code does not exacerbate this. If you are running on a UNIX device (i.e., not on Windows), you can test whether any changes you have made adversely impact memory and time performance as follows:</p> <ol> <li>Install memray in your <code>caveat</code> mamba environment: <code>mamba install memray pytest-memray</code>.</li> <li>Run the memory profiling integration test: <code>pytest -p memray -m \"high_mem\" --no-cov</code>.</li> <li>Optionally, to visualise the memory allocation, run <code>pytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]</code> - where you must define <code>[my_path]</code> and <code>[my_prefix]</code> - followed by <code>memray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin</code>. You will then find the HTML report at <code>[my_path]/memray-flamegraph-[my_prefix]-tests-test_100_memory_profiling.py-test_mem.html</code>.</li> </ol> <p>All together:</p> <pre><code>mamba install memray pytest-memray\npytest -p memray -m \"high_mem\" --no-cov --memray-bin-path=[my_path] --memray-bin-prefix=[my_prefix]\nmemray flamegraph [my_path]/[my_prefix]-tests-test_100_memory_profiling.py-test_mem.bin\n</code></pre> <p>For more information on using memray, refer to their documentation.</p>"},{"location":"contributing/#submitting-changes","title":"Submitting changes","text":"<p>To contribute changes:</p> <ol> <li>Fork the project on GitHub.</li> <li>Create a feature branch to work on in your fork (<code>git checkout -b new-fix-or-feature</code>).</li> <li>Test your changes using <code>pytest</code>.</li> <li>Commit your changes to the feature branch (you should have <code>pre-commit</code> installed to ensure your code is correctly formatted when you commit changes).</li> <li>Push the branch to GitHub (<code>git push origin new-fix-or-feature</code>).</li> <li>On GitHub, create a new pull request from the feature branch.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<p>Before submitting a pull request, check whether you have:</p> <ul> <li>Added your changes to <code>CHANGELOG.md</code>.</li> <li>Added or updated documentation for your changes.</li> <li>Added tests if you implemented new functionality.</li> </ul> <p>When opening a pull request, please provide a clear summary of your changes!</p>"},{"location":"contributing/#commit-messages","title":"Commit messages","text":"<p>Please try to write clear commit messages. One-line messages are fine for small changes, but bigger changes should look like this:</p> <pre><code>A brief summary of the commit (max 50 characters)\n\nA paragraph or bullet-point list describing what changed and its impact,\ncovering as many lines as needed.\n</code></pre>"},{"location":"contributing/#code-conventions","title":"Code conventions","text":"<p>Start reading our code and you'll get the hang of it.</p> <p>We mostly follow the official Style Guide for Python Code (PEP8).</p> <p>We have chosen to use the uncompromising code formatter <code>black</code> and the linter <code>ruff</code>. When run from the root directory of this repo, <code>pyproject.toml</code> should ensure that formatting and linting fixes are in line with our custom preferences (e.g., 100 character maximum line length). The philosophy behind using <code>black</code> is to have uniform style throughout the project dictated by code. Since <code>black</code> is designed to minimise diffs, and make patches more human readable, this also makes code reviews more efficient. To make this a smooth experience, you should run <code>pre-commit install</code> after setting up your development environment, so that <code>black</code> makes all the necessary fixes to your code each time you commit, and so that <code>ruff</code> will highlight any errors in your code. If you prefer, you can also set up your IDE to run these two tools whenever you save your files, and to have <code>ruff</code> highlight erroneous code directly as you type. Take a look at their documentation for more information on configuring this.</p> <p>We require all new contributions to have docstrings for all modules, classes and methods. When adding docstrings, we request you use the Google docstring style.</p>"},{"location":"contributing/#release-checklist","title":"Release checklist","text":""},{"location":"contributing/#pre-release","title":"Pre-release","text":"<ul> <li> Make sure all unit and integration tests pass (This is best done by creating a pre-release pull request).</li> <li> Re-run tutorial Jupyter notebooks (<code>pytest examples/ --overwrite</code>).</li> <li> Make sure documentation builds without errors (<code>mike deploy [version]</code>, where <code>[version]</code> is the current minor release of the form <code>X.Y</code>).</li> <li> Make sure the changelog is up-to-date, especially that new features and backward incompatible changes are clearly marked.</li> </ul>"},{"location":"contributing/#create-release","title":"Create release","text":"<ul> <li> Bump the version number in <code>caveat/__init__.py</code></li> <li> Update the changelog with final version number of the form <code>vX.Y.Z</code>, release date, and github <code>compare</code> link (at the bottom of the page).</li> <li> Commit with message <code>Release vX.Y.Z</code>, then add a <code>vX.Y.Z</code> tag.</li> <li> Create a release pull request to verify that the conda package builds successfully.</li> <li> Once the PR is approved and merged, create a release through the GitHub web interface, using the same tag, titling it <code>Release vX.Y.Z</code> and include all the changelog elements that are not flagged as internal.</li> </ul>"},{"location":"contributing/#post-release","title":"Post-release","text":"<ul> <li> Update the changelog, adding a new <code>[Unreleased]</code> heading.</li> <li> Update <code>caveat/__init__.py</code> to the next version appended with <code>.dev0</code>, in preparation for the next main commit.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#setting-up-a-user-environment","title":"Setting up a user environment","text":"<p>As a <code>caveat</code> user, it is easiest to install using the mamba package manager, as follows:</p> <ol> <li>Install mamba with the Mambaforge executable for your operating system.</li> <li> <p>Open the command line (or the \"miniforge prompt\" in Windows).</p> </li> <li> <p>Create the caveat mamba environment: <code>mamba create -n caveat -c conda-forge -c city-modelling-lab caveat</code></p> </li> <li>Activate the caveat mamba environment: <code>mamba activate caveat</code></li> </ol> <p>All together:</p> <pre><code>mamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch\n</code></pre>"},{"location":"installation/#running-the-example-notebooks","title":"Running the example notebooks","text":"<p>If you have followed the non-developer installation instructions above, you will need to install <code>jupyter</code> into your <code>caveat</code> environment to run the example notebooks:</p> <pre><code>mamba install -n caveat jupyter\n</code></pre> <p>With Jupyter installed, it's easiest to then add the environment as a jupyter kernel:</p> <pre><code>mamba activate caveat\nipython kernel install --user --name=caveat\njupyter notebook\n</code></pre>"},{"location":"installation/#choosing-a-different-environment-name","title":"Choosing a different environment name","text":"<p>If you would like to use a different name to <code>caveat</code> for your mamba environment, the installation becomes (where <code>[my-env-name]</code> is your preferred name for the environment):</p> <pre><code>mamba create -n [my-env-name] -c conda-forge --file requirements/base.txt\nmamba activate [my-env-name]\nipython kernel install --user --name=[my-env-name]\n</code></pre>"},{"location":"installation/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>The install instructions are slightly different to create a development environment compared to a user environment:</p> <pre><code>git clone git@github.com:fredshone/caveat.git\ncd caveat\nmamba create -n caveat -c conda-forge -c city-modelling-lab -c pytorch --file requirements/base.txt --file requirements/dev.txt\nmamba activate caveat\npip install --no-deps -e .\n</code></pre> <p>For more detailed installation instructions specific to developing the caveat codebase, see our development documentation.</p>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>This page provides documentation for our command line tools.</p>"},{"location":"api/cli/#caveat","title":"caveat","text":"<p>Console script for caveat.</p> <p>Usage:</p> <pre><code>caveat [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> boolean Show the version and exit. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-batch","title":"caveat batch","text":"<p>Train and report on a batch of encoders and models as per the given configuration file.</p> <p>Usage:</p> <pre><code>caveat batch [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-nrun","title":"caveat nrun","text":"<p>Train and report variance on n identical runs with varying seeds.</p> <p>Usage:</p> <pre><code>caveat nrun [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--n</code> integer N/A <code>5</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#caveat-run","title":"caveat run","text":"<p>Train and report on an encoder and model as per the given configuration file.</p> <p>Usage:</p> <pre><code>caveat run [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"examples/1_synthetic_population_generation/","title":"Introduction to Synthetic Population Generation","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nimport pandas as pd\n\nfrom caveat.data.synth import ActivityGen\nfrom caveat.data.utils import generate_population, trace_to_pam\nfrom caveat.features.sequence import sequence_prob_plot\nfrom caveat.features.times import (\n    joint_time_distributions_plot,\n    times_distributions_plot,\n)\n</pre> from pathlib import Path  import pandas as pd  from caveat.data.synth import ActivityGen from caveat.data.utils import generate_population, trace_to_pam from caveat.features.sequence import sequence_prob_plot from caveat.features.times import (     joint_time_distributions_plot,     times_distributions_plot, ) In\u00a0[2]: Copied! <pre>write_path = Path(\"tmp/synthetic_population.csv\")\n</pre> write_path = Path(\"tmp/synthetic_population.csv\") In\u00a0[3]: Copied! <pre># Example\ngenerator = ActivityGen()\ngenerator.build()\n\ntrace = generator.run()\nplan = trace_to_pam(trace, generator.map)\nplan.plot()\n</pre> # Example generator = ActivityGen() generator.build()  trace = generator.run() plan = trace_to_pam(trace, generator.map) plan.plot() In\u00a0[4]: Copied! <pre>population = generate_population(gen=generator, size=100)\npopulation.act = population.act.map(generator.map)\npopulation = population[[\"pid\", \"act\", \"start\", \"end\", \"duration\"]]\npopulation\n</pre> population = generate_population(gen=generator, size=100) population.act = population.act.map(generator.map) population = population[[\"pid\", \"act\", \"start\", \"end\", \"duration\"]] population Out[4]: pid act start end duration 0 0 home 0 375 375 1 0 shop 375 390 15 2 0 work 390 900 510 3 0 shop 900 960 60 4 0 home 960 1275 315 ... ... ... ... ... ... 510 99 home 0 390 390 511 99 shop 390 450 60 512 99 work 450 1035 585 513 99 education 1035 1080 45 514 99 home 1080 1440 360 <p>515 rows \u00d7 5 columns</p> In\u00a0[5]: Copied! <pre>write_path.parent.mkdir(exist_ok=True)\npopulation.to_csv(write_path, index=False)\n</pre> write_path.parent.mkdir(exist_ok=True) population.to_csv(write_path, index=False) In\u00a0[6]: Copied! <pre>def describe_col(population, col: str) -&gt; pd.DataFrame:\n    description = population.groupby(\"act\")[col].describe()[\n        [\"count\", \"mean\", \"std\", \"min\", \"max\"]\n    ]\n    description[\"attribute\"] = col\n    return description\n\n\ndef describe_cols(population, cols: list[str]) -&gt; pd.DataFrame:\n    description = pd.concat(\n        [describe_col(population, c) for c in cols], ignore_index=False\n    )\n    description = description.reset_index().set_index([\"attribute\", \"act\"])\n    return description\n\n\ndescribe_cols(population, [\"start\", \"end\", \"duration\"]).round()\n</pre> def describe_col(population, col: str) -&gt; pd.DataFrame:     description = population.groupby(\"act\")[col].describe()[         [\"count\", \"mean\", \"std\", \"min\", \"max\"]     ]     description[\"attribute\"] = col     return description   def describe_cols(population, cols: list[str]) -&gt; pd.DataFrame:     description = pd.concat(         [describe_col(population, c) for c in cols], ignore_index=False     )     description = description.reset_index().set_index([\"attribute\", \"act\"])     return description   describe_cols(population, [\"start\", \"end\", \"duration\"]).round() Out[6]: count mean std min max attribute act start education 46.0 867.0 191.0 405.0 1065.0 home 215.0 555.0 524.0 0.0 1320.0 leisure 64.0 869.0 263.0 375.0 1275.0 shop 89.0 607.0 282.0 375.0 1185.0 work 101.0 424.0 60.0 375.0 825.0 end education 46.0 974.0 174.0 495.0 1080.0 home 215.0 926.0 513.0 375.0 1440.0 leisure 64.0 934.0 273.0 405.0 1320.0 shop 89.0 643.0 281.0 390.0 1200.0 work 101.0 939.0 79.0 765.0 1200.0 duration education 46.0 106.0 45.0 15.0 210.0 home 215.0 371.0 76.0 15.0 480.0 leisure 64.0 65.0 24.0 15.0 150.0 shop 89.0 35.0 23.0 15.0 150.0 work 101.0 515.0 71.0 375.0 645.0 In\u00a0[7]: Copied! <pre>def time_distributions(population: pd.DataFrame, mapping: dict):\n    starts = {k: [] for k in mapping.values()}\n    ends = {k: [] for k in mapping.values()}\n    durations = {k: [] for k in mapping.values()}\n    for act, acts in population.groupby(\"act\"):\n        starts[act] = list(acts.start)\n        ends[act] = list(acts.end)\n        durations[act] = list(acts.duration)\n    return starts, ends, durations\n</pre> def time_distributions(population: pd.DataFrame, mapping: dict):     starts = {k: [] for k in mapping.values()}     ends = {k: [] for k in mapping.values()}     durations = {k: [] for k in mapping.values()}     for act, acts in population.groupby(\"act\"):         starts[act] = list(acts.start)         ends[act] = list(acts.end)         durations[act] = list(acts.duration)     return starts, ends, durations In\u00a0[8]: Copied! <pre>starts, ends, durations = time_distributions(population, generator.map)\n</pre> starts, ends, durations = time_distributions(population, generator.map) In\u00a0[9]: Copied! <pre>_ = times_distributions_plot(population, ys={})\n</pre> _ = times_distributions_plot(population, ys={}) In\u00a0[10]: Copied! <pre>_ = joint_time_distributions_plot(population, ys={})\n</pre> _ = joint_time_distributions_plot(population, ys={}) In\u00a0[11]: Copied! <pre>_ = sequence_prob_plot(population, ys={}, figsize=(8, 6))\n</pre> _ = sequence_prob_plot(population, ys={}, figsize=(8, 6)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/1_synthetic_population_generation/#introduction-to-synthetic-population-generation","title":"Introduction to Synthetic Population Generation\u00b6","text":""},{"location":"examples/2_NTS_population_generation/","title":"Generate Population from National Transport Survey Data","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nimport pandas as pd\nfrom pam import read\nfrom pam.core import Population\nfrom pam.utils import datetime_to_matsim_time\n\nfrom caveat.features.sequence import sequence_prob_plot\nfrom caveat.features.times import (\n    joint_time_distributions_plot,\n    times_distributions_plot,\n)\n</pre> from pathlib import Path  import pandas as pd from pam import read from pam.core import Population from pam.utils import datetime_to_matsim_time  from caveat.features.sequence import sequence_prob_plot from caveat.features.times import (     joint_time_distributions_plot,     times_distributions_plot, ) In\u00a0[2]: Copied! <pre>trips_csv = \"data/dummyNTS/trips.tab\"\nyear = 2021\nwrite_dir = Path(\"tmp\")\n</pre> trips_csv = \"data/dummyNTS/trips.tab\" year = 2021 write_dir = Path(\"tmp\") In\u00a0[3]: Copied! <pre>travel_diaries = pd.read_csv(\n    trips_csv,\n    sep=\"\\t\",\n    usecols=[\n        \"TripID\",\n        \"JourSeq\",\n        \"DayID\",\n        \"IndividualID\",\n        \"HouseholdID\",\n        \"MainMode_B04ID\",\n        \"TripPurpFrom_B01ID\",\n        \"TripPurpTo_B01ID\",\n        \"TripStart\",\n        \"TripEnd\",\n        \"TripOrigGOR_B02ID\",\n        \"TripDestGOR_B02ID\",\n        \"W5\",\n        \"SurveyYear\",\n    ],\n)\n\ntravel_diaries = travel_diaries.rename(\n    columns={  # rename data\n        \"TripID\": \"tid\",\n        \"JourSeq\": \"seq\",\n        \"DayID\": \"day\",\n        \"IndividualID\": \"iid\",\n        \"HouseholdID\": \"hid\",\n        \"TripOrigGOR_B02ID\": \"ozone\",\n        \"TripDestGOR_B02ID\": \"dzone\",\n        \"TripPurpFrom_B01ID\": \"oact\",\n        \"TripPurpTo_B01ID\": \"dact\",\n        \"MainMode_B04ID\": \"mode\",\n        \"TripStart\": \"tst\",\n        \"TripEnd\": \"tet\",\n        \"W5\": \"freq\",\n        \"SurveyYear\": \"year\",\n    }\n)\n\ntravel_diaries = travel_diaries[travel_diaries.year == year]\n\ntravel_diaries.tst = pd.to_numeric(travel_diaries.tst, errors=\"coerce\")\ntravel_diaries.tet = pd.to_numeric(travel_diaries.tet, errors=\"coerce\")\ntravel_diaries.ozone = pd.to_numeric(travel_diaries.ozone, errors=\"coerce\")\ntravel_diaries.dzone = pd.to_numeric(travel_diaries.dzone, errors=\"coerce\")\ntravel_diaries.freq = pd.to_numeric(travel_diaries.freq, errors=\"coerce\")\n\ntravel_diaries[\"did\"] = travel_diaries.groupby(\"iid\")[\"day\"].transform(\n    lambda x: pd.factorize(x)[0] + 1\n)\ntravel_diaries[\"pid\"] = [\n    f\"{i}-{d}\" for i, d in zip(travel_diaries.iid, travel_diaries.did)\n]\n\ntravel_diaries = travel_diaries.loc[\n    travel_diaries.groupby(\"pid\")\n    .filter(lambda x: pd.isnull(x).sum().sum() &lt; 1)\n    .index\n]\n# travel_diaries.freq = travel_diaries.freq / travel_diaries.groupby(\"iid\").day.transform(\"nunique\")\ntravel_diaries.loc[travel_diaries.tet == 0, \"tet\"] = 1440\n\ntravel_diaries = travel_diaries.drop(\n    [\"tid\", \"iid\", \"day\", \"year\", \"did\"], axis=1\n)\n\nmode_mapping = {\n    1: \"walk\",\n    2: \"bike\",\n    3: \"car\",  #'Car/van driver'\n    4: \"car\",  #'Car/van driver'\n    5: \"car\",  #'Motorcycle',\n    6: \"car\",  #'Other private transport',\n    7: \"pt\",  # Bus in London',\n    8: \"pt\",  #'Other local bus',\n    9: \"pt\",  #'Non-local bus',\n    10: \"pt\",  #'London Underground',\n    11: \"pt\",  #'Surface Rail',\n    12: \"car\",  #'Taxi/minicab',\n    13: \"pt\",  #'Other public transport',\n    -10: \"DEAD\",\n    -8: \"NA\",\n}\n\npurp_mapping = {\n    1: \"work\",\n    2: \"work\",  #'In course of work',\n    3: \"education\",\n    4: \"shop\",  #'Food shopping',\n    5: \"shop\",  #'Non food shopping',\n    6: \"medical\",  #'Personal business medical',\n    7: \"other\",  #'Personal business eat/drink',\n    8: \"other\",  #'Personal business other',\n    9: \"other\",  #'Eat/drink with friends',\n    10: \"visit\",  #'Visit friends',\n    11: \"other\",  #'Other social',\n    12: \"other\",  #'Entertain/ public activity',\n    13: \"other\",  #'Sport: participate',\n    14: \"home\",  #'Holiday: base',\n    15: \"other\",  #'Day trip/just walk',\n    16: \"other\",  #'Other non-escort',\n    17: \"escort\",  #'Escort home',\n    18: \"escort\",  #'Escort work',\n    19: \"escort\",  #'Escort in course of work',\n    20: \"escort\",  #'Escort education',\n    21: \"escort\",  #'Escort shopping/personal business',\n    22: \"escort\",  #'Other escort',\n    23: \"home\",  #'Home',\n    -10: \"DEAD\",\n    -8: \"NA\",\n}\n\ntravel_diaries[\"mode\"] = travel_diaries[\"mode\"].map(mode_mapping)\ntravel_diaries[\"oact\"] = travel_diaries[\"oact\"].map(purp_mapping)\ntravel_diaries[\"dact\"] = travel_diaries[\"dact\"].map(purp_mapping)\ntravel_diaries.tst = travel_diaries.tst.astype(int)\ntravel_diaries.tet = travel_diaries.tet.astype(int)\n\ntravel_diaries.head()\n</pre> travel_diaries = pd.read_csv(     trips_csv,     sep=\"\\t\",     usecols=[         \"TripID\",         \"JourSeq\",         \"DayID\",         \"IndividualID\",         \"HouseholdID\",         \"MainMode_B04ID\",         \"TripPurpFrom_B01ID\",         \"TripPurpTo_B01ID\",         \"TripStart\",         \"TripEnd\",         \"TripOrigGOR_B02ID\",         \"TripDestGOR_B02ID\",         \"W5\",         \"SurveyYear\",     ], )  travel_diaries = travel_diaries.rename(     columns={  # rename data         \"TripID\": \"tid\",         \"JourSeq\": \"seq\",         \"DayID\": \"day\",         \"IndividualID\": \"iid\",         \"HouseholdID\": \"hid\",         \"TripOrigGOR_B02ID\": \"ozone\",         \"TripDestGOR_B02ID\": \"dzone\",         \"TripPurpFrom_B01ID\": \"oact\",         \"TripPurpTo_B01ID\": \"dact\",         \"MainMode_B04ID\": \"mode\",         \"TripStart\": \"tst\",         \"TripEnd\": \"tet\",         \"W5\": \"freq\",         \"SurveyYear\": \"year\",     } )  travel_diaries = travel_diaries[travel_diaries.year == year]  travel_diaries.tst = pd.to_numeric(travel_diaries.tst, errors=\"coerce\") travel_diaries.tet = pd.to_numeric(travel_diaries.tet, errors=\"coerce\") travel_diaries.ozone = pd.to_numeric(travel_diaries.ozone, errors=\"coerce\") travel_diaries.dzone = pd.to_numeric(travel_diaries.dzone, errors=\"coerce\") travel_diaries.freq = pd.to_numeric(travel_diaries.freq, errors=\"coerce\")  travel_diaries[\"did\"] = travel_diaries.groupby(\"iid\")[\"day\"].transform(     lambda x: pd.factorize(x)[0] + 1 ) travel_diaries[\"pid\"] = [     f\"{i}-{d}\" for i, d in zip(travel_diaries.iid, travel_diaries.did) ]  travel_diaries = travel_diaries.loc[     travel_diaries.groupby(\"pid\")     .filter(lambda x: pd.isnull(x).sum().sum() &lt; 1)     .index ] # travel_diaries.freq = travel_diaries.freq / travel_diaries.groupby(\"iid\").day.transform(\"nunique\") travel_diaries.loc[travel_diaries.tet == 0, \"tet\"] = 1440  travel_diaries = travel_diaries.drop(     [\"tid\", \"iid\", \"day\", \"year\", \"did\"], axis=1 )  mode_mapping = {     1: \"walk\",     2: \"bike\",     3: \"car\",  #'Car/van driver'     4: \"car\",  #'Car/van driver'     5: \"car\",  #'Motorcycle',     6: \"car\",  #'Other private transport',     7: \"pt\",  # Bus in London',     8: \"pt\",  #'Other local bus',     9: \"pt\",  #'Non-local bus',     10: \"pt\",  #'London Underground',     11: \"pt\",  #'Surface Rail',     12: \"car\",  #'Taxi/minicab',     13: \"pt\",  #'Other public transport',     -10: \"DEAD\",     -8: \"NA\", }  purp_mapping = {     1: \"work\",     2: \"work\",  #'In course of work',     3: \"education\",     4: \"shop\",  #'Food shopping',     5: \"shop\",  #'Non food shopping',     6: \"medical\",  #'Personal business medical',     7: \"other\",  #'Personal business eat/drink',     8: \"other\",  #'Personal business other',     9: \"other\",  #'Eat/drink with friends',     10: \"visit\",  #'Visit friends',     11: \"other\",  #'Other social',     12: \"other\",  #'Entertain/ public activity',     13: \"other\",  #'Sport: participate',     14: \"home\",  #'Holiday: base',     15: \"other\",  #'Day trip/just walk',     16: \"other\",  #'Other non-escort',     17: \"escort\",  #'Escort home',     18: \"escort\",  #'Escort work',     19: \"escort\",  #'Escort in course of work',     20: \"escort\",  #'Escort education',     21: \"escort\",  #'Escort shopping/personal business',     22: \"escort\",  #'Other escort',     23: \"home\",  #'Home',     -10: \"DEAD\",     -8: \"NA\", }  travel_diaries[\"mode\"] = travel_diaries[\"mode\"].map(mode_mapping) travel_diaries[\"oact\"] = travel_diaries[\"oact\"].map(purp_mapping) travel_diaries[\"dact\"] = travel_diaries[\"dact\"].map(purp_mapping) travel_diaries.tst = travel_diaries.tst.astype(int) travel_diaries.tet = travel_diaries.tet.astype(int)  travel_diaries.head() Out[3]: hid seq mode oact dact freq tst tet ozone dzone pid 0 1 1 car home visit 0.989618 675 683 7 7 1-1 1 1 2 car visit other 1.002945 720 735 7 7 1-1 2 1 3 car other visit 0.989618 770 780 7 7 1-1 3 1 4 car visit home 0.989618 1110 1130 7 7 1-1 4 1 1 car home visit 0.999891 760 770 7 7 1-2 In\u00a0[4]: Copied! <pre>pam_population = read.load_travel_diary(\n    trips=travel_diaries, trip_freq_as_person_freq=True\n)\nprint(pam_population.stats)\npam_population.fix_plans()\nprint(pam_population.stats)\npam_population.random_person().plot()\n</pre> pam_population = read.load_travel_diary(     trips=travel_diaries, trip_freq_as_person_freq=True ) print(pam_population.stats) pam_population.fix_plans() print(pam_population.stats) pam_population.random_person().plot() <pre>Using from-to activity parser using 'oact' and 'dact' columns\nAdding pid-&gt;hh mapping to persons_attributes from trips.\n\n        Unable to load household area ('hzone') - not found in trips_diary or unable to build from attributes.\n        Pam will try to infer home location from activities, but this behaviour is not recommended.\n        \nUsing freq of 'None' for all trips.\n Person pid:2-5 hid:1 plan does not start with 'home' activity: work\n Person pid:2-6 hid:1 plan does not start with 'home' activity: work\n Person pid:2-7 hid:1 plan does not start with 'home' activity: work\n Person pid:3-4 hid:1 plan does not start with 'home' activity: education\n</pre> <pre>{'num_households': 3, 'num_people': 39, 'num_activities': 188, 'num_legs': 149}\n{'num_households': 3, 'num_people': 39, 'num_activities': 175, 'num_legs': 136}\n</pre> In\u00a0[5]: Copied! <pre>def dt_to_min(dt) -&gt; int:\n    h, m, s = datetime_to_matsim_time(dt).split(\":\")\n    return (int(h) * 60) + int(m)\n\n\ndef pam_to_population(population: Population) -&gt; pd.DataFrame:\n    \"\"\"write trace of population. Ignoring trips.\"\"\"\n    record = []\n    for uid, (hid, pid, person) in enumerate(population.people()):\n        for i in range(0, len(person.plan) - 1, 2):\n            record.append(\n                [\n                    uid,\n                    hid,\n                    person.plan[i].act,\n                    dt_to_min(person.plan[i].start_time),\n                    dt_to_min(person.plan[i + 1].end_time),\n                ]\n            )\n        record.append(\n            [\n                uid,\n                hid,\n                person.plan[-1].act,\n                dt_to_min(person.plan[-1].start_time),\n                dt_to_min(person.plan[-1].end_time),\n            ]\n        )\n\n    df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])\n    df[\"duration\"] = df.end - df.start\n    return df\n\n\npopulation = pam_to_population(population=pam_population)\n\npopulation.describe()\n</pre> def dt_to_min(dt) -&gt; int:     h, m, s = datetime_to_matsim_time(dt).split(\":\")     return (int(h) * 60) + int(m)   def pam_to_population(population: Population) -&gt; pd.DataFrame:     \"\"\"write trace of population. Ignoring trips.\"\"\"     record = []     for uid, (hid, pid, person) in enumerate(population.people()):         for i in range(0, len(person.plan) - 1, 2):             record.append(                 [                     uid,                     hid,                     person.plan[i].act,                     dt_to_min(person.plan[i].start_time),                     dt_to_min(person.plan[i + 1].end_time),                 ]             )         record.append(             [                 uid,                 hid,                 person.plan[-1].act,                 dt_to_min(person.plan[-1].start_time),                 dt_to_min(person.plan[-1].end_time),             ]         )      df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])     df[\"duration\"] = df.end - df.start     return df   population = pam_to_population(population=pam_population)  population.describe() Out[5]: pid hid start end duration count 175.000000 175.000000 175.000000 175.000000 175.000000 mean 17.520000 1.685714 686.988571 1007.902857 320.914286 std 11.502044 0.921375 416.041844 301.367566 262.947656 min 0.000000 1.000000 0.000000 348.000000 6.000000 25% 8.000000 1.000000 465.000000 785.000000 52.000000 50% 17.000000 1.000000 805.000000 950.000000 320.000000 75% 28.000000 3.000000 967.000000 1308.500000 527.500000 max 38.000000 3.000000 1325.000000 1440.000000 962.000000 In\u00a0[6]: Copied! <pre>_ = times_distributions_plot(population, ys={})\n</pre> _ = times_distributions_plot(population, ys={}) In\u00a0[7]: Copied! <pre>_ = joint_time_distributions_plot(population, ys={})\n</pre> _ = joint_time_distributions_plot(population, ys={}) In\u00a0[8]: Copied! <pre>_ = sequence_prob_plot(population, ys={}, figsize=(8, 6))\n</pre> _ = sequence_prob_plot(population, ys={}, figsize=(8, 6)) In\u00a0[9]: Copied! <pre>write_path = write_dir / \"nts_toy_population.csv\"\nwrite_path.parent.mkdir(exist_ok=True)\npopulation.to_csv(write_path, index=False)\n</pre> write_path = write_dir / \"nts_toy_population.csv\" write_path.parent.mkdir(exist_ok=True) population.to_csv(write_path, index=False) In\u00a0[10]: Copied! <pre>def pam_to_population_home_based_plans_only(\n    population: Population,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert population to trace. Only home-based plans are included.\"\"\"\n    record = []\n    for uid, (hid, pid, person) in enumerate(population.people()):\n        person_record = []\n        for i in range(0, len(person.plan) - 1, 2):\n            person_record.append(\n                [\n                    uid,\n                    hid,\n                    person.plan[i].act,\n                    dt_to_min(person.plan[i].start_time),\n                    dt_to_min(person.plan[i + 1].end_time),\n                ]\n            )\n        person_record.append(\n            [\n                uid,\n                hid,\n                person.plan[-1].act,\n                dt_to_min(person.plan[-1].start_time),\n                dt_to_min(person.plan[-1].end_time),\n            ]\n        )\n        if (person_record[0][2] == \"home\") and (person_record[-1][2] == \"home\"):\n            record.extend(person_record)\n\n    df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])\n    df[\"duration\"] = df.end - df.start\n    return df\n\n\nhome_population = pam_to_population_home_based_plans_only(pam_population)\n\nwrite_path = write_dir / \"nts_toy_home_population.csv\"\nwrite_path.parent.mkdir(exist_ok=True)\nhome_population.to_csv(write_path, index=False)\n</pre> def pam_to_population_home_based_plans_only(     population: Population, ) -&gt; pd.DataFrame:     \"\"\"Convert population to trace. Only home-based plans are included.\"\"\"     record = []     for uid, (hid, pid, person) in enumerate(population.people()):         person_record = []         for i in range(0, len(person.plan) - 1, 2):             person_record.append(                 [                     uid,                     hid,                     person.plan[i].act,                     dt_to_min(person.plan[i].start_time),                     dt_to_min(person.plan[i + 1].end_time),                 ]             )         person_record.append(             [                 uid,                 hid,                 person.plan[-1].act,                 dt_to_min(person.plan[-1].start_time),                 dt_to_min(person.plan[-1].end_time),             ]         )         if (person_record[0][2] == \"home\") and (person_record[-1][2] == \"home\"):             record.extend(person_record)      df = pd.DataFrame(record, columns=[\"pid\", \"hid\", \"act\", \"start\", \"end\"])     df[\"duration\"] = df.end - df.start     return df   home_population = pam_to_population_home_based_plans_only(pam_population)  write_path = write_dir / \"nts_toy_home_population.csv\" write_path.parent.mkdir(exist_ok=True) home_population.to_csv(write_path, index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/2_NTS_population_generation/#generate-population-from-national-transport-survey-data","title":"Generate Population from National Transport Survey Data\u00b6","text":"<p>Provided example is toy data. You can access UK travel survey from 2002-2021 from the UK Data Service.</p>"},{"location":"examples/3_metrics/","title":"3 metrics","text":"In\u00a0[1]: Copied! <pre>import random\n\nimport pandas as pd\n\nfrom caveat import report\nfrom caveat.features import participation, sequence, times\n</pre> import random  import pandas as pd  from caveat import report from caveat.features import participation, sequence, times In\u00a0[2]: Copied! <pre>raw = pd.read_csv(\"data/synthetic_population.csv\")\n\ndef down_sample(df, p):\n    n_samples = int(len(df.pid.unique()) * p)\n    sample_ids = random.sample(list(df.pid.unique()), n_samples)\n    sampled = df[df.pid.isin(sample_ids)]\n    return sampled\n\nobserved = down_sample(raw, 0.2)\n\nsampled = {\n    \"sampled_a\": down_sample(observed, 0.2),\n    \"sampled_b\": down_sample(raw, 0.5),\n}\n</pre> raw = pd.read_csv(\"data/synthetic_population.csv\")  def down_sample(df, p):     n_samples = int(len(df.pid.unique()) * p)     sample_ids = random.sample(list(df.pid.unique()), n_samples)     sampled = df[df.pid.isin(sample_ids)]     return sampled  observed = down_sample(raw, 0.2)  sampled = {     \"sampled_a\": down_sample(observed, 0.2),     \"sampled_b\": down_sample(raw, 0.5), } In\u00a0[3]: Copied! <pre>report.report_diff(observed, sampled, participation.participation_rates)\n</pre> report.report_diff(observed, sampled, participation.participation_rates) Out[3]: observed sampled_a sampled_a delta sampled_b sampled_b delta participation rate home 2.160 2.125 -0.035 2.170 0.010 work 1.000 1.000 0.000 1.004 0.004 shop 0.865 0.850 -0.015 0.816 -0.049 leisure 0.680 0.775 0.095 0.676 -0.004 education 0.450 0.400 -0.050 0.446 -0.004 In\u00a0[4]: Copied! <pre>report.report_diff(observed, sampled, times.average_start_times)\n</pre> report.report_diff(observed, sampled, times.average_start_times) Out[4]: observed sampled_a sampled_a delta sampled_b sampled_b delta average start time home 555.034722 554.823529 -0.211193 559.894009 4.859287 work 420.975000 419.250000 -1.725000 419.043825 -1.931175 shop 641.531792 650.294118 8.762326 620.367647 -21.164145 leisure 781.654412 789.677419 8.023008 826.420118 44.765707 education 858.333333 883.125000 24.791667 861.928251 3.594918 In\u00a0[5]: Copied! <pre>_ = times.times_distributions_plot(observed, sampled, figsize=(12, 5))\n</pre> _ = times.times_distributions_plot(observed, sampled, figsize=(12, 5)) In\u00a0[6]: Copied! <pre>_ = times.joint_time_distributions_plot(observed, sampled, figsize=(12, 8))\n</pre> _ = times.joint_time_distributions_plot(observed, sampled, figsize=(12, 8)) In\u00a0[7]: Copied! <pre>_ = sequence.sequence_prob_plot(observed, sampled, figsize=(12, 4))\n</pre> _ = sequence.sequence_prob_plot(observed, sampled, figsize=(12, 4)) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":""},{"location":"CHANGELOG/#removed","title":"Removed","text":""},{"location":"CHANGELOG/#v010-2023-10-04","title":"[v0.1.0] - 2023-10-04","text":"<p>Initial release.</p>"},{"location":"reference/caveat/data/loader/","title":"caveat.data.loader","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule","title":"<code>DataModule(data, val_split=0.1, train_batch_size=128, val_batch_size=128, test_batch_size=128, num_workers=0, pin_memory=False, **kwargs)</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Torch DataModule.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Data</p> <p> TYPE: <code>Dataset</code> </p> <code>val_split</code> <p>description. Defaults to 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>train_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>val_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>test_batch_size</code> <p>description. Defaults to 128.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>num_workers</code> <p>description. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>pin_memory</code> <p>description. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>caveat/data/loader.py</code> <pre><code>def __init__(\n    self,\n    data: Dataset,\n    val_split: float = 0.1,\n    train_batch_size: int = 128,\n    val_batch_size: int = 128,\n    test_batch_size: int = 128,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    **kwargs,\n):\n    \"\"\"Torch DataModule.\n\n    Args:\n        data (Dataset): Data\n        val_split (float, optional): _description_. Defaults to 0.1.\n        train_batch_size (int, optional): _description_. Defaults to 128.\n        val_batch_size (int, optional): _description_. Defaults to 128.\n        test_batch_size (int, optional): _description_. Defaults to 128.\n        num_workers (int, optional): _description_. Defaults to 0.\n        pin_memory (bool, optional): _description_. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.data = data\n    self.val_split = val_split\n    self.train_batch_size = train_batch_size\n    self.val_batch_size = val_batch_size\n    self.test_batch_size = test_batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n    self.mapping = None\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.data","title":"<code>data = data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.mapping","title":"<code>mapping = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.num_workers","title":"<code>num_workers = num_workers</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.pin_memory","title":"<code>pin_memory = pin_memory</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.test_batch_size","title":"<code>test_batch_size = test_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.train_batch_size","title":"<code>train_batch_size = train_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_batch_size","title":"<code>val_batch_size = val_batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_split","title":"<code>val_split = val_split</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.setup","title":"<code>setup(stage=None)</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n        self.data, [1 - self.val_split, self.val_split]\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def test_dataloader(self) -&gt; Union[DataLoader, list[DataLoader]]:\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.train_batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/loader/#caveat.data.loader.DataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"Source code in <code>caveat/data/loader.py</code> <pre><code>def val_dataloader(self) -&gt; Union[DataLoader, list[DataLoader]]:\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.val_batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n        pin_memory=self.pin_memory,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/","title":"caveat.data.synth","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen","title":"<code>ActivityGen()</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def __init__(self):\n    self.map = {i: s for i, s in enumerate(self.possible_states)}\n    self.steps = self.duration // self.step_size\n    self.transition_weights = None\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.duration","title":"<code>duration = 24 * 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.initial_state","title":"<code>initial_state = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.map","title":"<code>map = {i: sfor (i, s) in enumerate(self.possible_states)}</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_sensitivity","title":"<code>max_duration_sensitivity = np.array([0.1, 0.1, 0.1, 0.1, 0.1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_tollerance","title":"<code>max_duration_tollerance = np.array([12 * 60, 6 * 60, 60, 360, 120])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_sensitivity","title":"<code>min_duration_sensitivity = np.array([1, 1.2, 1, 1.2, 1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_tollerance","title":"<code>min_duration_tollerance = np.array([180, 420, 60, 120, 60])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.pivot_adjustment","title":"<code>pivot_adjustment = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.possible_states","title":"<code>possible_states = ['home', 'work', 'shop', 'education', 'leisure']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repetition_sensitivity","title":"<code>repetition_sensitivity = np.array([1, 2, 1, 2, 1])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repetition_tollerance","title":"<code>repetition_tollerance = np.array([10, 1, 1, 1, 2])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.step_size","title":"<code>step_size = 15</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.steps","title":"<code>steps = self.duration // self.step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_config","title":"<code>transition_config = {'home': {'home': [(0, 100), (5, 100), (11, 0.1), (23, 100), (24, 100)], 'work': [(0, 0), (6, 0), (9, 0.2), (11, 0.1), (17, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 2), (11, 1), (20, 0), (24, 0)], 'education': [(0, 0), (7.5, 0), (8.5, 5), (11, 0.01), (17, 0.01), (20, 0), (24, 0)], 'leisure': [(0, 0), (6, 0), (9, 2), (16, 0.1), (22, 0), (24, 0)]}, 'work': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.5), (17, 1), (24, 100)], 'work': [(0, 100), (12, 100), (20, 0), (24, 0)], 'shop': [(0, 0), (12, 0), (13, 0.1), (14, 0), (18, 0.1), (19, 0), (24, 0)], 'education': [(0, 0), (12, 0), (13, 0.1), (14, 0), (16, 0), (17, 0.1), (19, 0), (24, 0)], 'leisure': [(0, 0), (15, 0), (16, 0.1), (17, 0.2), (24, 0)]}, 'shop': {'home': [(0, 0.3), (23, 1), (24, 1)], 'work': [(0, 0.1), (14, 0.1), (15, 0), (24, 0)], 'shop': [(0, 10), (15, 10), (16, 0), (24, 0)], 'education': [(0, 0.1), (15, 0), (24, 0)], 'leisure': [(0, 0.2), (15, 0), (24, 0)]}, 'education': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.1), (17, 100), (24, 100)], 'work': [(0, 0), (12, 1), (15, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 0.1), (11, 0.3), (23, 0), (24, 0)], 'education': [(0, 100), (12, 100), (17, 100), (18, 0), (24, 0)], 'leisure': [(0, 0), (6, 0), (9, 0.1), (16, 0.1), (17, 0), (24, 0)]}, 'leisure': {'home': [(0, 0), (12, 0), (13, 0.2), (16, 0.1), (17, 100), (24, 100)], 'work': [(0, 1), (12, 1), (23, 0), (24, 0)], 'shop': [(0, 0), (6, 0), (7, 0.1), (11, 0.3), (23, 0), (24, 0)], 'education': [(0, 0), (24, 0)], 'leisure': [(0, 100), (19, 100), (23, 0), (24, 0)]}}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_weights","title":"<code>transition_weights = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.build","title":"<code>build()</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def build(self):\n    num_states = len(self.possible_states)\n    self.transition_weights = np.zeros((num_states, num_states, self.steps))\n    for i in range(num_states):\n        in_state = self.possible_states[i]\n        state_transitions = self.transition_config[in_state]\n        for j in range(num_states):\n            out_state = self.possible_states[j]\n            pivots = state_transitions[out_state]\n            self.transition_weights[i][j] = interpolate_from_pivots(\n                pivots, self.steps, self.pivot_adjustment, self.step_size\n            )\n\n    self.transition_weights = np.transpose(\n        self.transition_weights, (0, 2, 1)\n    )  # ie [in_state, minute, out_state]\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.max_duration_adjustment","title":"<code>max_duration_adjustment(activity_durations)</code>","text":"<p>Penalise current activity based on duration.</p> PARAMETER  DESCRIPTION <code>activity_durations</code> <p>activity durations</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def max_duration_adjustment(self, activity_durations: np.array) -&gt; np.array:\n    \"\"\"Penalise current activity based on duration.\n\n    Args:\n        activity_durations (np.array): activity durations\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return 1 / (\n        np.clip((activity_durations - self.max_duration_tollerance), 1, None)\n        ** self.max_duration_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.min_duration_adjustment","title":"<code>min_duration_adjustment(activity_durations)</code>","text":"<p>Penalise current activity based on duration.</p> PARAMETER  DESCRIPTION <code>activity_durations</code> <p>activity durations</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def min_duration_adjustment(self, activity_durations: np.array) -&gt; np.array:\n    \"\"\"Penalise current activity based on duration.\n\n    Args:\n        activity_durations (np.array): activity durations\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return (\n        np.clip(((self.min_duration_tollerance - activity_durations)), 1, None)\n        ** self.min_duration_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.repeat_adjustment","title":"<code>repeat_adjustment(activity_counts)</code>","text":"<p>Penalise activities based on how often they have been done.</p> PARAMETER  DESCRIPTION <code>activity_counts</code> <p>counts of activity repetitions</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: transition factor adjustments</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def repeat_adjustment(self, activity_counts: np.array) -&gt; np.array:\n    \"\"\"Penalise activities based on how often they have been done.\n\n    Args:\n        activity_counts (np.array): counts of activity repetitions\n\n    Returns:\n        np.array: transition factor adjustments\n    \"\"\"\n    return 1 / (\n        np.clip((activity_counts - self.repetition_tollerance), 1, None)\n        ** self.repetition_sensitivity\n    )\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.run","title":"<code>run()</code>","text":"<p>summary</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def run(self):\n    \"\"\"_summary_\"\"\"\n    trace = []  # [(act, start, end, dur), (act, start, end, dur), ...]\n    state = self.initial_state\n    activity_counts = np.zeros((len(self.possible_states)))\n    activity_counts[state] += 1\n    activity_durations = np.zeros((len(self.possible_states)))\n    activity_durations[state] += self.step_size\n\n    for step in range(1, self.steps):\n        new_state = np.random.choice(\n            len(self.possible_states),\n            p=self.transition_probabilities(state, step, activity_counts, activity_durations),\n        )\n        if new_state != state:\n            time = step * self.step_size\n            if not trace:  # first transition\n                prev_end = 0\n            else:\n                prev_end = trace[-1][2]\n            trace.append((state, prev_end, time, time - prev_end))\n\n            # update state\n            state = new_state\n            activity_counts[state] += 1\n            activity_durations[state] = 0  # reset\n\n        activity_durations[state] += self.step_size\n\n    # close\n    prev_end = trace[-1][2]\n    trace.append((state, prev_end, self.duration, self.duration - prev_end))\n    return trace\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.ActivityGen.transition_probabilities","title":"<code>transition_probabilities(state, step, activity_counts, activity_durations)</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def transition_probabilities(\n    self, state, step, activity_counts: np.array, activity_durations: np.array\n):\n    p = self.transition_weights[state][step]\n    p = (\n        p\n        * self.repeat_adjustment(activity_counts)\n        * self.min_duration_adjustment(activity_durations)\n        * self.max_duration_adjustment(activity_durations)\n    )\n    return p / sum(p)\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.interpolate_from_pivots","title":"<code>interpolate_from_pivots(pivots, size=1440, pivot_adjustment=60, step_size=1)</code>","text":"<p>Create a descretised array of shape 'size' based on given 'pivots'.</p> PARAMETER  DESCRIPTION <code>pivots</code> <p>description</p> <p> TYPE: <code>list[tuple[float, float]]</code> </p> <code>size</code> <p>description. Defaults to 1440</p> <p> TYPE: <code>int</code> DEFAULT: <code>1440</code> </p> <code>pivot_adjustment</code> <p>description. Defaults to 60</p> <p> TYPE: <code>int</code> DEFAULT: <code>60</code> </p> <code>step_size</code> <p>Defaults to 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: bins</p> Source code in <code>caveat/data/synth.py</code> <pre><code>def interpolate_from_pivots(\n    pivots: list[tuple[float, float]],\n    size: int = 1440,\n    pivot_adjustment: int = 60,\n    step_size: int = 1,\n) -&gt; np.array:\n    \"\"\"Create a descretised array of shape 'size' based on given 'pivots'.\n\n    Args:\n        pivots (list[tuple[float, float]]): _description_\n        size (int, optional): _description_. Defaults to 1440\n        pivot_adjustment (int, optional): _description_. Defaults to 60\n        step_size (int, optional): Defaults to 1\n\n    Returns:\n        np.array: bins\n    \"\"\"\n    bins = np.zeros((size), dtype=np.float64)\n    for k in range(len(pivots) - 1):\n        a_pivot, a_value = pivots[k]\n        b_pivot, b_value = pivots[k + 1]\n        a_pivot = int(a_pivot * pivot_adjustment / step_size)\n        b_pivot = int(b_pivot * pivot_adjustment / step_size)\n        a = (a_pivot, a_value)\n        b = (b_pivot, b_value)\n        bins[slice(a_pivot, b_pivot)] = interpolate_pivot(a, b)\n    return bins\n</code></pre>"},{"location":"reference/caveat/data/synth/#caveat.data.synth.interpolate_pivot","title":"<code>interpolate_pivot(a, b)</code>","text":"Source code in <code>caveat/data/synth.py</code> <pre><code>def interpolate_pivot(a: tuple[int, float], b: tuple[int, float]) -&gt; np.array:\n    a_pivot, a_value = a\n    b_pivot, b_value = b\n    return np.linspace(a_value, b_value, abs(b_pivot - a_pivot), endpoint=False)\n</code></pre>"},{"location":"reference/caveat/data/utils/","title":"caveat.data.utils","text":""},{"location":"reference/caveat/data/utils/#caveat.data.utils.gen_person","title":"<code>gen_person(gen, pid)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def gen_person(gen, pid) -&gt; pd.DataFrame:\n    trace = gen.run()\n    return trace_to_df(trace, pid=pid)\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.gen_persons","title":"<code>gen_persons(gen, pids)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def gen_persons(gen, pids) -&gt; pd.DataFrame:\n    return pd.concat([gen_person(gen, pid) for pid in pids], ignore_index=True)\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.generate_population","title":"<code>generate_population(gen, size, cores=None)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def generate_population(gen, size: int, cores: int = None):\n    if cores is None:\n        cores = mp.cpu_count()\n\n    batches = list(split(range(size), cores))\n\n    pools = mp.Pool(cores)\n    results = [pools.apply_async(gen_persons, args=(gen, pids)) for pids in batches]\n    pools.close()\n    pools.join()\n    results = [r.get() for r in results]\n    pop = pd.concat(results, ignore_index=True)\n    return pop\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.split","title":"<code>split(a, n)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def split(a, n):\n    k, m = divmod(len(a), n)\n    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.trace_to_df","title":"<code>trace_to_df(trace, **kwargs)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def trace_to_df(trace: list[tuple], **kwargs) -&gt; pd.DataFrame:\n    df = pd.DataFrame(trace, columns=[\"act\", \"start\", \"end\", \"duration\"])\n    for k, v in kwargs.items():\n        df[k] = v\n    return df\n</code></pre>"},{"location":"reference/caveat/data/utils/#caveat.data.utils.trace_to_pam","title":"<code>trace_to_pam(trace, mapping)</code>","text":"Source code in <code>caveat/data/utils.py</code> <pre><code>def trace_to_pam(trace: list[tuple], mapping: dict):\n    plan = Plan()\n    for act, start, end, duration in trace:\n        name = mapping[act]\n        plan.add(Activity(act=name, start_time=mtdt(start), end_time=mtdt(end)))\n        plan.add(Trip(mode=\"car\", start_time=mtdt(end), end_time=mtdt(end)))\n    return plan\n</code></pre>"},{"location":"reference/caveat/data/validate/","title":"caveat.data.validate","text":""},{"location":"reference/caveat/data/validate/#caveat.data.validate.load_and_validate","title":"<code>load_and_validate(data_path)</code>","text":"Source code in <code>caveat/data/validate.py</code> <pre><code>def load_and_validate(data_path: Path) -&gt; pd.DataFrame:\n    data = pd.read_csv(data_path)\n    if data.empty:\n        raise UserWarning(f\"No data found in {data_path}.\")\n    validate(data)\n    return data\n</code></pre>"},{"location":"reference/caveat/data/validate/#caveat.data.validate.validate","title":"<code>validate(data)</code>","text":"Source code in <code>caveat/data/validate.py</code> <pre><code>def validate(data: pd.DataFrame):\n    required_cols = {\"pid\", \"act\", \"start\", \"end\"}\n    found = set(data.columns)\n    missing = required_cols - found\n    if missing:\n        raise UserWarning(\n            f\"\"\"\n    Input data is missing required columns.\n    Required: {required_cols}.\n    Found: {found}.\n    Please add missing: {missing}.\n    \"\"\"\n        )\n    data.act = data.act.astype(\"category\")\n    data.start = data.start.astype(\"int\")\n    data.end = data.end.astype(\"int\")\n\n    data[\"duration\"] = data.end - data.start\n</code></pre>"},{"location":"reference/caveat/encoders/base/","title":"caveat.encoders.base","text":""},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder","title":"<code>BaseEncoder()</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>caveat/encoders/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    super(BaseEncoder, self).__init__()\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder.decode","title":"<code>decode(input)</code>","text":"Source code in <code>caveat/encoders/base.py</code> <pre><code>def decode(self, input: tensor) -&gt; DataFrame:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/base/#caveat.encoders.base.BaseEncoder.encode","title":"<code>encode(input)</code>","text":"Source code in <code>caveat/encoders/base.py</code> <pre><code>def encode(self, input: DataFrame) -&gt; tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/","title":"caveat.encoders.descrete","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder","title":"<code>DescreteEncoder(duration=1440, step_size=10, **kwargs)</code>","text":"<p>             Bases: <code>BaseEncoder</code></p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def __init__(self, duration: int = 1440, step_size: int = 10, **kwargs):\n    self.duration = duration\n    self.step_size = step_size\n    self.steps = duration // step_size\n    self.index_to_acts = None\n    self.acts_to_index = None\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.acts_to_index","title":"<code>acts_to_index = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.duration","title":"<code>duration = duration</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.index_to_acts","title":"<code>index_to_acts = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.step_size","title":"<code>step_size = step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.steps","title":"<code>steps = duration // step_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.decode","title":"<code>decode(encoded)</code>","text":"<p>Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:</p> <p>pid | act | start | end</p> <p>pid is taken as sample enumeration.</p> PARAMETER  DESCRIPTION <code>encoded</code> <p>description</p> <p> TYPE: <code>Tensor</code> </p> <code>mapping</code> <p>description</p> <p> TYPE: <code>dict</code> </p> <code>length</code> <p>Length of plan in minutes.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pd.DataFrame: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def decode(self, encoded: Tensor) -&gt; pd.DataFrame:\n    \"\"\"Decode decretised a sequences ([B, C, T, A]) into DataFrame of 'traces', eg:\n\n    pid | act | start | end\n\n    pid is taken as sample enumeration.\n\n    Args:\n        encoded (Tensor): _description_\n        mapping (dict): _description_\n        length (int): Length of plan in minutes.\n\n    Returns:\n        pd.DataFrame: _description_\n    \"\"\"\n    encoded = torch.argmax(encoded, axis=-1)\n    decoded = []\n\n    for pid in range(len(encoded)):\n        current_act = None\n        act_start = 0\n\n        for step, act_idx in enumerate(encoded[pid, 0]):\n            if int(act_idx) != current_act and current_act is not None:\n                decoded.append(\n                    [\n                        pid,\n                        self.index_to_acts[current_act],\n                        int(act_start * self.step_size),\n                        int(step * self.step_size),\n                    ]\n                )\n                act_start = step\n            current_act = int(act_idx)\n        decoded.append(\n            [\n                pid,\n                self.index_to_acts[current_act],\n                int(act_start * self.step_size),\n                self.duration,\n            ]\n        )\n\n    return pd.DataFrame(decoded, columns=[\"pid\", \"act\", \"start\", \"end\"])\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteEncoder.encode","title":"<code>encode(data)</code>","text":"Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def encode(self, data: pd.DataFrame) -&gt; Dataset:\n    self.index_to_acts = {i: a for i, a in enumerate(data.act.unique())}\n    self.acts_to_index = {a: i for i, a in self.index_to_acts.items()}\n    return DescreteSequenceDataset(\n        descretise_population(\n            data,\n            duration=self.duration,\n            step_size=self.step_size,\n            class_map=self.acts_to_index,\n        )\n    )\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteSequenceDataset","title":"<code>DescreteSequenceDataset(encoded)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Torch Dataset for descretised sequence data.</p> PARAMETER  DESCRIPTION <code>data</code> <p>Population of sequences.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def __init__(self, encoded: Tensor):\n    \"\"\"Torch Dataset for descretised sequence data.\n\n    Args:\n        data (Tensor): Population of sequences.\n    \"\"\"\n    self.encoded = encoded\n    self.size = len(encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteSequenceDataset.encoded","title":"<code>encoded = encoded</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteSequenceDataset.size","title":"<code>size = len(encoded)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.DescreteSequenceDataset.shape","title":"<code>shape()</code>","text":"Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def shape(self):\n    return self.encoded[0].shape\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.descretise_population","title":"<code>descretise_population(data, duration, step_size, class_map)</code>","text":"<p>Convert given population of activity traces into vector [P, C, H, W]. P is the population size. C (channel) is length 1. H is time steps. W is a one-hot encoding of activity type.</p> PARAMETER  DESCRIPTION <code>data</code> <p>description</p> <p> TYPE: <code>DataFrame</code> </p> <code>duration</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>step_size</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>torch.tensor: [P, C, H, W]</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def descretise_population(\n    data: pd.DataFrame, duration: int, step_size: int, class_map: dict\n) -&gt; torch.tensor:\n    \"\"\"Convert given population of activity traces into vector [P, C, H, W].\n    P is the population size.\n    C (channel) is length 1.\n    H is time steps.\n    W is a one-hot encoding of activity type.\n\n    Args:\n        data (pd.DataFrame): _description_\n        duration (int): _description_\n        step_size (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        torch.tensor: [P, C, H, W]\n    \"\"\"\n    persons = data.pid.nunique()\n    num_classes = len(class_map)\n    steps = duration // step_size\n    encoded = np.zeros((persons, steps, num_classes, 1), dtype=np.float32)\n\n    for pid, (_, trace) in enumerate(data.groupby(\"pid\")):\n        trace_encoding = descretise_trace(\n            acts=trace.act,\n            starts=trace.start,\n            ends=trace.end,\n            length=duration,\n            class_map=class_map,\n        )\n        trace_encoding = down_sample(trace_encoding, step_size)\n        trace_encoding = one_hot(trace_encoding, num_classes)\n        trace_encoding = trace_encoding.reshape(steps, num_classes, 1)\n        encoded[pid] = trace_encoding  # [B, H, W, C]\n    encoded = encoded.transpose(0, 3, 1, 2)  # [B, C, H, W]\n    return torch.from_numpy(encoded)\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.descretise_trace","title":"<code>descretise_trace(acts, starts, ends, length, class_map)</code>","text":"<p>Create categorical encoding from ranges with step of 1.</p> PARAMETER  DESCRIPTION <code>acts</code> <p>description</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>starts</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>ends</code> <p>description</p> <p> TYPE: <code>Iterable[int]</code> </p> <code>length</code> <p>description</p> <p> TYPE: <code>int</code> </p> <code>class_map</code> <p>description</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def descretise_trace(\n    acts: Iterable[str],\n    starts: Iterable[int],\n    ends: Iterable[int],\n    length: int,\n    class_map: dict,\n) -&gt; np.array:\n    \"\"\"Create categorical encoding from ranges with step of 1.\n\n    Args:\n        acts (Iterable[str]): _description_\n        starts (Iterable[int]): _description_\n        ends (Iterable[int]): _description_\n        length (int): _description_\n        class_map (dict): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    encoding = np.zeros((length), dtype=np.int8)\n    for act, start, end in zip(acts, starts, ends):\n        encoding[start:end] = class_map[act]\n    return encoding\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.down_sample","title":"<code>down_sample(array, step)</code>","text":"<p>Down-sample by steppiong through given array. todo: Methodology will down sample based on first classification. If we are down sampling a lot (for example from minutes to hours), we would be better of, samplig based on majority class.</p> PARAMETER  DESCRIPTION <code>array</code> <p>description</p> <p> TYPE: <code>array</code> </p> <code>step</code> <p>description</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def down_sample(array: np.array, step: int) -&gt; np.array:\n    \"\"\"Down-sample by steppiong through given array.\n    todo:\n    Methodology will down sample based on first classification.\n    If we are down sampling a lot (for example from minutes to hours),\n    we would be better of, samplig based on majority class.\n\n    Args:\n        array (np.array): _description_\n        step (int): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    return array[::step]\n</code></pre>"},{"location":"reference/caveat/encoders/descrete/#caveat.encoders.descrete.one_hot","title":"<code>one_hot(target, num_classes)</code>","text":"<p>One hot encoding of given categorical array.</p> PARAMETER  DESCRIPTION <code>target</code> <p>description</p> <p> TYPE: <code>array</code> </p> <code>num_classes</code> <p>description</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: description</p> Source code in <code>caveat/encoders/descrete.py</code> <pre><code>def one_hot(target: np.array, num_classes: int) -&gt; np.array:\n    \"\"\"One hot encoding of given categorical array.\n\n    Args:\n        target (np.array): _description_\n        num_classes (int): _description_\n\n    Returns:\n        np.array: _description_\n    \"\"\"\n    return np.eye(num_classes)[target]\n</code></pre>"},{"location":"reference/caveat/experiment/","title":"caveat.experiment","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment","title":"<code>Experiment(model, LR=0.005, weight_decay=0.0, scheduler_gamma=0.95, kld_weight=0.00025)</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>caveat/experiment.py</code> <pre><code>def __init__(\n    self,\n    model: BaseVAE,\n    LR: float = 0.005,\n    weight_decay: float = 0.0,\n    scheduler_gamma: float = 0.95,\n    kld_weight: float = 0.00025,\n) -&gt; None:\n    super(Experiment, self).__init__()\n    self.model = model\n    self.LR = LR\n    self.weight_decay = weight_decay\n    self.scheduler_gamma = scheduler_gamma\n    self.kld_weight = kld_weight\n    self.curr_device = None\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.LR","title":"<code>LR = LR</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.curr_device","title":"<code>curr_device = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.kld_weight","title":"<code>kld_weight = kld_weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.scheduler_gamma","title":"<code>scheduler_gamma = scheduler_gamma</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.weight_decay","title":"<code>weight_decay = weight_decay</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def configure_optimizers(self):\n    optims = []\n    scheds = []\n\n    optimizer = optim.Adam(\n        self.model.parameters(), lr=self.LR, weight_decay=self.weight_decay\n    )\n    optims.append(optimizer)\n\n    if self.scheduler_gamma is not None:\n        scheduler = optim.lr_scheduler.ExponentialLR(\n            optims[0], gamma=self.scheduler_gamma\n        )\n        scheds.append(scheduler)\n    return optims, scheds\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.forward","title":"<code>forward(input, **kwargs)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def forward(self, input: Tensor, **kwargs) -&gt; Tensor:\n    return self.model(input, **kwargs)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.generate","title":"<code>generate(size)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def generate(self, size: int) -&gt; None:\n    return self.model.sample(size, self.curr_device)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.on_validation_end","title":"<code>on_validation_end()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def on_validation_end(self) -&gt; None:\n    self.regenerate_val_batch()\n    self.sample_sequences(100)\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_batch","title":"<code>regenerate_batch(x, name)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_batch(self, x: Tensor, name: str):\n    reconstructed = self.model.generate(x)\n    vutils.save_image(\n        reconstructed.data,\n        Path(\n            self.logger.log_dir,\n            name,\n            f\"recons_{self.logger.name}_epoch_{self.current_epoch}.png\",\n        ),\n        normalize=True,\n        ncol=10,\n        pad_value=0.5,\n    )\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_test_batch","title":"<code>regenerate_test_batch()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_test_batch(self):\n    x = next(iter(self.trainer.datamodule.test_dataloader()))\n    x = x.to(self.curr_device)\n    self.regenerate_batch(x, name=\"test_reconstructions\")\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.regenerate_val_batch","title":"<code>regenerate_val_batch()</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def regenerate_val_batch(self):\n    x = next(iter(self.trainer.datamodule.val_dataloader()))\n    x = x.to(self.curr_device)\n    self.regenerate_batch(x, name=\"reconstructions\")\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.sample_sequences","title":"<code>sample_sequences(size, name='samples')</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def sample_sequences(self, size: int, name: str = \"samples\") -&gt; None:\n    samples = self.model.sample(size, self.curr_device)\n    vutils.save_image(\n        samples.cpu().data,\n        Path(\n            self.logger.log_dir,\n            name,\n            f\"{self.logger.name}_epoch_{self.current_epoch}.png\",\n        ),\n        normalize=True,\n        ncol=10,\n    )\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    self.curr_device = batch.device\n\n    results = self.forward(batch)\n    train_loss = self.model.loss_function(\n        *results,\n        M_N=self.kld_weight,  # al_img.shape[0]/ self.num_train_imgs,\n        batch_idx=batch_idx,\n    )\n\n    self.log_dict(\n        {key: val.item() for key, val in train_loss.items()}, sync_dist=True\n    )\n\n    return train_loss[\"loss\"]\n</code></pre>"},{"location":"reference/caveat/experiment/#caveat.experiment.Experiment.validation_step","title":"<code>validation_step(batch, batch_idx, optimizer_idx=0)</code>","text":"Source code in <code>caveat/experiment.py</code> <pre><code>def validation_step(self, batch, batch_idx, optimizer_idx=0):\n    self.curr_device = batch.device\n\n    results = self.forward(batch)\n    val_loss = self.model.loss_function(\n        *results,\n        M_N=1.0,  # real_img.shape[0]/ self.num_val_imgs,\n        optimizer_idx=optimizer_idx,\n        batch_idx=batch_idx,\n    )\n\n    self.log_dict(\n        {f\"val_{key}\": val.item() for key, val in val_loss.items()},\n        sync_dist=True,\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n    )\n</code></pre>"},{"location":"reference/caveat/features/durations/","title":"caveat.features.durations","text":""},{"location":"reference/caveat/features/durations/#caveat.features.durations.average_activity_durations","title":"<code>average_activity_durations(population)</code>","text":"<p>Calculate the average duration for each activity type in the population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: A Series containing the average duration for each activity.</p> Source code in <code>caveat/features/durations.py</code> <pre><code>def average_activity_durations(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculate the average duration for each activity type in the population.\n\n    Args:\n        population (pandas.DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        pandas.Series: A Series containing the average duration for each activity.\n    \"\"\"\n    metric = population.groupby(\"act\", observed=False).duration.mean()\n    order = (\n        population.groupby(\"act\", observed=False)\n        .duration.count()\n        .sort_values(ascending=False)\n        .index\n    )\n    metric = metric[order]\n    metric.index = MultiIndex.from_tuples(\n        [(\"average duration\", act) for act in metric.index]\n    )\n    return metric\n</code></pre>"},{"location":"reference/caveat/features/durations/#caveat.features.durations.average_activity_plan_seq_durations","title":"<code>average_activity_plan_seq_durations(population)</code>","text":"<p>Calculate the average duration for each activity (indexed by sequence enumeration) in the population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: A Series containing the average duration for each activity enumeration.</p> Source code in <code>caveat/features/durations.py</code> <pre><code>def average_activity_plan_seq_durations(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculate the average duration for each activity (indexed by sequence enumeration) in the population.\n\n    Args:\n        population (pandas.DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        pandas.Series: A Series containing the average duration for each activity enumeration.\n    \"\"\"\n    actseq = population.act.astype(str) + population.groupby(\n        \"pid\", as_index=False\n    ).cumcount().astype(str)\n\n    metric = population.groupby(actseq).duration.mean()\n    order = (\n        population.groupby(actseq, observed=False)\n        .duration.count()\n        .sort_values(ascending=False)\n        .index\n    )\n    metric = metric[order]\n    metric.index = MultiIndex.from_tuples(\n        [(\"average act plan seq duration\", act) for act in metric.index]\n    )\n    return metric\n</code></pre>"},{"location":"reference/caveat/features/durations/#caveat.features.durations.average_activity_seq_durations","title":"<code>average_activity_seq_durations(population)</code>","text":"<p>Calculate the average duration for each activity (indexed by enumeration) in the population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: A Series containing the average duration for each activity enumeration.</p> Source code in <code>caveat/features/durations.py</code> <pre><code>def average_activity_seq_durations(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculate the average duration for each activity (indexed by enumeration) in the population.\n\n    Args:\n        population (pandas.DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        pandas.Series: A Series containing the average duration for each activity enumeration.\n    \"\"\"\n    actseq = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n\n    metric = population.groupby(actseq).duration.mean()\n    order = (\n        population.groupby(actseq, observed=False)\n        .duration.count()\n        .sort_values(ascending=False)\n        .index\n    )\n    metric = metric[order]\n    metric.index = MultiIndex.from_tuples(\n        [(\"average act seq duration\", act) for act in metric.index]\n    )\n    return metric\n</code></pre>"},{"location":"reference/caveat/features/participation/","title":"caveat.features.participation","text":""},{"location":"reference/caveat/features/participation/#caveat.features.participation.act_plan_seq_participation_rates","title":"<code>act_plan_seq_participation_rates(population)</code>","text":"<p>Calculates the participation rates for each activity (indexed by sequence enumeration) in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A Series containing the participation rates for each activity.</p> <p> TYPE: <code>Series</code> </p> Source code in <code>caveat/features/participation.py</code> <pre><code>def act_plan_seq_participation_rates(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculates the participation rates for each activity (indexed by sequence enumeration) in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        Series: A Series containing the participation rates for each activity.\n    \"\"\"\n    actseq = population.act.astype(str) + population.groupby(\n        \"pid\", as_index=False\n    ).cumcount().astype(str)\n    metrics = population.groupby(actseq).pid.count() / population.pid.nunique()\n    metrics.index = MultiIndex.from_tuples(\n        [(\"act plan seq participation rate\", act) for act in metrics.index]\n    )\n    metrics = metrics.sort_values(ascending=False)\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.act_seq_participation_rates","title":"<code>act_seq_participation_rates(population)</code>","text":"<p>Calculates the participation rates for each activity (indexed by enumeration) in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A Series containing the participation rates for each activity.</p> <p> TYPE: <code>Series</code> </p> Source code in <code>caveat/features/participation.py</code> <pre><code>def act_seq_participation_rates(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculates the participation rates for each activity (indexed by enumeration) in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        Series: A Series containing the participation rates for each activity.\n    \"\"\"\n    actseq = population.act.astype(str) + population.groupby(\n        [\"pid\", \"act\"], as_index=False, observed=False\n    ).cumcount().astype(str)\n    metrics = population.groupby(actseq).pid.count() / population.pid.nunique()\n    metrics.index = MultiIndex.from_tuples(\n        [(\"act seq participation rate\", act) for act in metrics.index]\n    )\n    metrics = metrics.sort_values(ascending=False)\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.calc_pair_rate","title":"<code>calc_pair_rate(act_counts, pair)</code>","text":"<p>Calculates the participation rate for given activity pairs given activity counts.</p> <p>Parameters: act_counts (DataFrame): DataFrame of activity counts. pair (tuple): Pair of activities to calculate participation rate for.</p> <p>Returns: float: Participation rate of the pair of users.</p> Source code in <code>caveat/features/participation.py</code> <pre><code>def calc_pair_rate(act_counts: DataFrame, pair: tuple) -&gt; float:\n    \"\"\"\n    Calculates the participation rate for given activity pairs given activity counts.\n\n    Parameters:\n    act_counts (DataFrame): DataFrame of activity counts.\n    pair (tuple): Pair of activities to calculate participation rate for.\n\n    Returns:\n    float: Participation rate of the pair of users.\n    \"\"\"\n    a, b = pair\n    if a == b:\n        return (act_counts[a] &gt; 1).mean()\n    return ((act_counts[a] &gt; 0) &amp; (act_counts[b] &gt; 0)).mean()\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.combinations_with_replacement","title":"<code>combinations_with_replacement(targets, length, prev_array=[])</code>","text":"<p>Returns all possible combinations of elements in the input array with replacement, where each combination has a length of tuple_length.</p> PARAMETER  DESCRIPTION <code>targets</code> <p>The input array to generate combinations from.</p> <p> TYPE: <code>list</code> </p> <code>length</code> <p>The length of each combination.</p> <p> TYPE: <code>int</code> </p> <code>prev_array</code> <p>The previous array generated in the recursion. Defaults to [].</p> <p> TYPE: <code>list</code> DEFAULT: <code>[]</code> </p> RETURNS DESCRIPTION <code>list</code> <p>A list of all possible combinations of elements in the input array with replacement.</p> <p> TYPE: <code>list[list]</code> </p> Source code in <code>caveat/features/participation.py</code> <pre><code>def combinations_with_replacement(\n    targets: list, length: int, prev_array=[]\n) -&gt; list[list]:\n    \"\"\"\n    Returns all possible combinations of elements in the input array with replacement,\n    where each combination has a length of tuple_length.\n\n    Args:\n        targets (list): The input array to generate combinations from.\n        length (int): The length of each combination.\n        prev_array (list, optional): The previous array generated in the recursion. Defaults to [].\n\n    Returns:\n        list: A list of all possible combinations of elements in the input array with replacement.\n    \"\"\"\n    if len(prev_array) == length:\n        return [prev_array]\n    combs = []\n    for i, val in enumerate(targets):\n        prev_array_extended = prev_array.copy()\n        prev_array_extended.append(val)\n        combs += combinations_with_replacement(\n            targets[i:], length, prev_array_extended\n        )\n    return combs\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.joint_participation_rates","title":"<code>joint_participation_rates(population)</code>","text":"<p>Calculate the participation rate for all pairs of activities in the given population.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>pandas.Series: A Series containing the participation rate for all pairs of activities.</p> Source code in <code>caveat/features/participation.py</code> <pre><code>def joint_participation_rates(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculate the participation rate for all pairs of activities in the given population.\n\n    Args:\n        population (pandas.DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        pandas.Series: A Series containing the participation rate for all pairs of activities.\n    \"\"\"\n    act_counts = (\n        population.groupby(\"pid\").act.value_counts().unstack(fill_value=0)\n    )\n    pairs = combinations_with_replacement(list(population.act.unique()), 2)\n    idx = [\"+\".join(pair) for pair in pairs]\n    p = [calc_pair_rate(act_counts, pair) for pair in pairs]\n    metrics = Series(p, index=idx)\n    metrics = metrics.sort_values(ascending=False)\n    metrics.index = MultiIndex.from_tuples(\n        [(\"joint participation rate\", pair) for pair in metrics.index]\n    )\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/participation/#caveat.features.participation.participation_rates","title":"<code>participation_rates(population)</code>","text":"<p>Calculates the participation rates for each activity in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A Series containing the participation rates for each activity.</p> <p> TYPE: <code>Series</code> </p> Source code in <code>caveat/features/participation.py</code> <pre><code>def participation_rates(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculates the participation rates for each activity in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        Series: A Series containing the participation rates for each activity.\n    \"\"\"\n    metrics = (\n        population.groupby(\"act\", observed=False).pid.count()\n        / population.pid.nunique()\n    )\n    metrics.index = MultiIndex.from_tuples(\n        [(\"participation rate\", act) for act in metrics.index]\n    )\n    metrics = metrics.sort_values(ascending=False)\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/sequence/","title":"caveat.features.sequence","text":""},{"location":"reference/caveat/features/sequence/#caveat.features.sequence.collect_sequence","title":"<code>collect_sequence(acts)</code>","text":"Source code in <code>caveat/features/sequence.py</code> <pre><code>def collect_sequence(acts: Series) -&gt; str:\n    return \"&gt;\".join(acts)\n</code></pre>"},{"location":"reference/caveat/features/sequence/#caveat.features.sequence.sequence_prob_plot","title":"<code>sequence_prob_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/features/sequence.py</code> <pre><code>def sequence_prob_plot(\n    observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n) -&gt; Figure:\n    acts = list(observed.act.value_counts(ascending=False).index)\n    if kwargs.pop(\"cmap\", None) is None:\n        cmap = plt.cm.Set3\n    colors = cmap.colors\n    factor = (len(acts) // len(colors)) + 1\n    cmap = dict(zip(acts, colors * factor))\n\n    n_plots = len(ys) + 2\n    ratios = [1 for _ in range(n_plots)]\n    ratios[-1] = 0.5\n\n    fig, axs = plt.subplots(\n        1,\n        n_plots,\n        figsize=kwargs.pop(\"figsize\", (12, 5)),\n        sharex=True,\n        sharey=True,\n        tight_layout=True,\n        gridspec_kw={\"width_ratios\": ratios},\n    )\n    acts = list(observed.act.value_counts(ascending=False).index)\n    _probs_plot(observed, acts, ax=axs[0], cmap=cmap)\n    axs[0].set_title(\"Observed\", fontstyle=\"italic\")\n    if ys is None:\n        return fig\n    for i, (name, y) in enumerate(ys.items()):\n        _probs_plot(y, acts, ax=axs[i + 1], cmap=cmap)\n        axs[i + 1].set_title(name.title(), fontstyle=\"italic\")\n\n    elements = [Patch(facecolor=cmap[act], label=act.title()) for act in acts]\n    axs[-1].axis(\"off\")\n    axs[-1].legend(handles=elements, loc=\"center left\", frameon=False)\n\n    return fig\n</code></pre>"},{"location":"reference/caveat/features/sequence/#caveat.features.sequence.sequence_probs","title":"<code>sequence_probs(population)</code>","text":"<p>Calculates the sequence probabilities in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame containing the probability of each sequence.</p> <p> TYPE: <code>DataFrame</code> </p> Source code in <code>caveat/features/sequence.py</code> <pre><code>def sequence_probs(population: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the sequence probabilities in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        DataFrame: A DataFrame containing the probability of each sequence.\n    \"\"\"\n    metrics = (\n        population.groupby(\"pid\")\n        .act.apply(collect_sequence)\n        .value_counts(normalize=True)\n    )\n    metrics = metrics.sort_values(ascending=False)\n    metrics.index = MultiIndex.from_tuples(\n        [(\"sequence rate\", acts) for acts in metrics.index]\n    )\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/structural/","title":"caveat.features.structural","text":""},{"location":"reference/caveat/features/structural/#caveat.features.structural.start_and_end_acts","title":"<code>start_and_end_acts(population, target='home')</code>","text":"<p>Calculates the proportion of individuals in the population who performed their first and last act at the specified target location.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> <code>target</code> <p>The target location to calculate the proportion of individuals who performed their first and last act there. Defaults to \"home\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'home'</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A pandas Series containing the calculated metrics.</p> <p> TYPE: <code>Series</code> </p> Source code in <code>caveat/features/structural.py</code> <pre><code>def start_and_end_acts(population: DataFrame, target: str = \"home\") -&gt; Series:\n    \"\"\"\n    Calculates the proportion of individuals in the population who performed their first and last act at the specified target location.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n        target (str): The target location to calculate the proportion of individuals who performed their first and last act there. Defaults to \"home\".\n\n    Returns:\n        Series: A pandas Series containing the calculated metrics.\n    \"\"\"\n    first = (population.groupby(\"pid\").first().act == target).mean()\n    last = (population.groupby(\"pid\").last().act == target).mean()\n    metrics = Series(\n        {\n            (\"structural\", f\"first act is {target}\"): first,\n            (\"structural\", f\"last act is {target}\"): last,\n        }\n    )\n    metrics.index.names = [\"type\", \"metric\"]\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/structural/#caveat.features.structural.time_consistency","title":"<code>time_consistency(population)</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def time_consistency(population: DataFrame) -&gt; Series:\n    pass\n</code></pre>"},{"location":"reference/caveat/features/structural/#caveat.features.structural.trip_consistency","title":"<code>trip_consistency(population)</code>","text":"Source code in <code>caveat/features/structural.py</code> <pre><code>def trip_consistency(population: DataFrame) -&gt; Series:\n    pass\n</code></pre>"},{"location":"reference/caveat/features/times/","title":"caveat.features.times","text":""},{"location":"reference/caveat/features/times/#caveat.features.times.average_end_times","title":"<code>average_end_times(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def average_end_times(population: DataFrame) -&gt; Series:\n    metrics = population.groupby(\"act\", observed=False).end.mean()\n    order = (\n        population.groupby(\"act\", observed=False)\n        .end.count()\n        .sort_values(ascending=False)\n        .index\n    )\n    metrics = metrics[order]\n    metrics.index = MultiIndex.from_tuples(\n        [(\"average end time\", act) for act in metrics.index]\n    )\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.average_start_times","title":"<code>average_start_times(population)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def average_start_times(population: DataFrame) -&gt; Series:\n    metrics = population.groupby(\"act\", observed=False).start.mean()\n    order = (\n        population.groupby(\"act\", observed=False)\n        .start.count()\n        .sort_values(ascending=False)\n        .index\n    )\n    metrics = metrics[order]\n    metrics.index = MultiIndex.from_tuples(\n        [(\"average start time\", act) for act in metrics.index]\n    )\n    return metrics\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.joint_time_distributions_plot","title":"<code>joint_time_distributions_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def joint_time_distributions_plot(\n    observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n) -&gt; Figure:\n    if ys is None:\n        ys = dict()\n    acts = list(observed.act.value_counts(ascending=False).index)\n\n    fig = plt.figure(\n        constrained_layout=True, figsize=kwargs.pop(\"figsize\", (15, 4))\n    )\n\n    subfigs = fig.subfigures(nrows=len(ys) + 1, ncols=1)\n    # deal with observed first\n    if not ys:\n        subfig = subfigs\n    else:\n        subfig = subfigs[0]\n    subfig.suptitle(\"Observed\", fontstyle=\"italic\")\n    axs = subfig.subplots(nrows=1, ncols=len(acts), sharex=True, sharey=True)\n    _joint_time_plot(\"obseved\", observed, axs, acts)\n\n    # act column titles\n    for ax, act in zip(axs, acts):\n        ax.set_title(act.title(), fontsize=\"large\")\n\n    # now deal with ys\n    for i, (name, y) in enumerate(ys.items()):\n        subfig = subfigs[i + 1]\n        subfig.suptitle(name.title(), fontstyle=\"italic\")\n        axs = subfig.subplots(\n            nrows=1, ncols=len(acts), sharex=True, sharey=True\n        )\n        _joint_time_plot(name, y, axs, acts)\n\n    # xlabel on bottom row\n    for ax in axs:\n        ax.set(xlabel=\"Start times\\n(minutes)\")\n\n    return fig\n</code></pre>"},{"location":"reference/caveat/features/times/#caveat.features.times.times_distributions_plot","title":"<code>times_distributions_plot(observed, ys, **kwargs)</code>","text":"Source code in <code>caveat/features/times.py</code> <pre><code>def times_distributions_plot(\n    observed: DataFrame, ys: Optional[dict[DataFrame]], **kwargs\n) -&gt; Figure:\n    fig, axs = plt.subplots(\n        3,\n        observed.act.nunique(),\n        figsize=kwargs.pop(\"figsize\", (12, 5)),\n        sharex=True,\n        sharey=False,\n        tight_layout=True,\n    )\n    acts = list(observed.act.value_counts(ascending=False).index)\n    _times_plot(\"observed\", observed, acts, axs=axs)\n    if ys is None:\n        return fig\n    for name, y in ys.items():\n        _times_plot(name, y, acts, axs=axs)\n    return fig\n</code></pre>"},{"location":"reference/caveat/features/transitions/","title":"caveat.features.transitions","text":""},{"location":"reference/caveat/features/transitions/#caveat.features.transitions.transition_rates","title":"<code>transition_rates(population)</code>","text":"<p>Calculates the transition rates per person in the given population DataFrame.</p> PARAMETER  DESCRIPTION <code>population</code> <p>A DataFrame containing the population data.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A Series containing the rate of occurance of each transition per person.</p> <p> TYPE: <code>Series</code> </p> Source code in <code>caveat/features/transitions.py</code> <pre><code>def transition_rates(population: DataFrame) -&gt; Series:\n    \"\"\"\n    Calculates the transition rates per person in the given population DataFrame.\n\n    Args:\n        population (DataFrame): A DataFrame containing the population data.\n\n    Returns:\n        Series: A Series containing the rate of occurance of each transition per person.\n    \"\"\"\n    transitions = {}\n    for acts in population.groupby(\"pid\").act.apply(list):\n        for i in range(len(acts) - 1):\n            t = \"-&gt;\".join(acts[i : i + 2])\n            transitions[t] = transitions.get(t, 0) + 1\n    transitions = Series(transitions).sort_values(ascending=False)\n    transitions /= population.pid.nunique()\n    transitions = transitions.sort_values(ascending=False)\n    transitions.index = MultiIndex.from_tuples(\n        [(\"transition rate\", acts) for acts in transitions.index]\n    )\n    return transitions\n</code></pre>"},{"location":"reference/caveat/models/base/","title":"caveat.models.base","text":""},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE","title":"<code>BaseVAE()</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>caveat/models/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    super(BaseVAE, self).__init__()\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.decode","title":"<code>decode(input)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def decode(self, input: tensor) -&gt; Any:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.encode","title":"<code>encode(input)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def encode(self, input: tensor) -&gt; list[tensor]:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.forward","title":"<code>forward(*inputs)</code>  <code>abstractmethod</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>@abstractmethod\ndef forward(self, *inputs: tensor) -&gt; tensor:\n    pass\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.generate","title":"<code>generate(x, **kwargs)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def generate(self, x: tensor, **kwargs) -&gt; tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.loss_function","title":"<code>loss_function(*inputs, **kwargs)</code>  <code>abstractmethod</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>@abstractmethod\ndef loss_function(self, *inputs: Any, **kwargs) -&gt; tensor:\n    pass\n</code></pre>"},{"location":"reference/caveat/models/base/#caveat.models.base.BaseVAE.sample","title":"<code>sample(batch_size, current_device, **kwargs)</code>","text":"Source code in <code>caveat/models/base.py</code> <pre><code>def sample(self, batch_size: int, current_device: int, **kwargs) -&gt; tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/caveat/models/utils/","title":"caveat.models.utils","text":""},{"location":"reference/caveat/models/utils/#caveat.models.utils.calc_output_padding","title":"<code>calc_output_padding(size)</code>","text":"<p>Calculate output padding for a transposed convolution such that output dims will match dimensions of inputs to a convolution of given size. For each dimension, padding is set to 1 if even size, otherwise 0.</p> PARAMETER  DESCRIPTION <code>size</code> <p>input size (h, w)</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: required padding</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def calc_output_padding(size: Union[tuple[int, int], int]) -&gt; np.array:\n    \"\"\"Calculate output padding for a transposed convolution such that output dims will\n    match dimensions of inputs to a convolution of given size.\n    For each dimension, padding is set to 1 if even size, otherwise 0.\n\n    Args:\n        size (Union[tuple[int, int], int]): input size (h, w)\n\n    Returns:\n        np.array: required padding\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    h, w = size\n    return (int(h % 2 == 0), int(w % 2 == 0))\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.conv_size","title":"<code>conv_size(size, kernel_size=3, stride=2, padding=1, dilation=1)</code>","text":"<p>Calculate output dimensions for 2d convolution.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Input size, may be integer if symetric.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> </p> <code>kernel_size</code> <p>Kernel_size. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>Stride. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>Input padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>Dilation. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: Output size.</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def conv_size(\n    size: Union[tuple[int, int], int],\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n    dilation: Union[tuple[int, int], int] = 1,\n) -&gt; np.array:\n    \"\"\"Calculate output dimensions for 2d convolution.\n\n    Args:\n        size (Union[tuple[int, int], int]): Input size, may be integer if symetric.\n        kernel_size (Union[tuple[int, int], int], optional): Kernel_size. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): Stride. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): Input padding. Defaults to 1.\n        dilation (Union[tuple[int, int], int], optional): Dilation. Defaults to 1.\n\n    Returns:\n        np.array: Output size.\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    if isinstance(padding, int):\n        padding = (padding, padding)\n    if isinstance(dilation, int):\n        dilation = (dilation, dilation)\n    return (\n        np.array(size)\n        + 2 * np.array(padding)\n        - np.array(dilation) * (np.array(kernel_size) - 1)\n        - 1\n    ) // np.array(stride) + 1\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.hot_argmax","title":"<code>hot_argmax(batch, axis=-1)</code>","text":"<p>Encoded given axis as one-hot based on argmax for that axis.</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Input tensor.</p> <p> TYPE: <code>tensor</code> </p> <code>axis</code> <p>Axis index to encode. Defaults to -1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>One hot encoded tensor.</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/utils.py</code> <pre><code>def hot_argmax(batch: tensor, axis: int = -1) -&gt; tensor:\n    \"\"\"Encoded given axis as one-hot based on argmax for that axis.\n\n    Args:\n        batch (tensor): Input tensor.\n        axis (int, optional): Axis index to encode. Defaults to -1.\n\n    Returns:\n        tensor: One hot encoded tensor.\n    \"\"\"\n    batch = batch.swapaxes(axis, -1)\n    argmax = batch.argmax(axis=-1)\n    eye = torch.eye(batch.shape[-1])\n    batch = eye[argmax]\n    return batch.swapaxes(axis, -1)\n</code></pre>"},{"location":"reference/caveat/models/utils/#caveat.models.utils.transconv_size","title":"<code>transconv_size(size, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)</code>","text":"<p>Calculate output dimension for 2d transpose convolution.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Input size, may be integer if symetric.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> </p> <code>kernel_size</code> <p>Kernel size. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>Stride. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>Input padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>Dilation. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> <code>output_padding</code> <p>Output padding. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>array</code> <p>np.array: Output size.</p> Source code in <code>caveat/models/utils.py</code> <pre><code>def transconv_size(\n    size: Union[tuple[int, int], int],\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n    dilation: Union[tuple[int, int], int] = 1,\n    output_padding: Union[tuple[int, int], int] = 1,\n) -&gt; np.array:\n    \"\"\"Calculate output dimension for 2d transpose convolution.\n\n    Args:\n        size (Union[tuple[int, int], int]): Input size, may be integer if symetric.\n        kernel_size (Union[tuple[int, int], int], optional): Kernel size. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): Stride. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): Input padding. Defaults to 1.\n        dilation (Union[tuple[int, int], int], optional): Dilation. Defaults to 1.\n        output_padding (Union[tuple[int, int], int], optional): Output padding. Defaults to 1.\n\n    Returns:\n        np.array: Output size.\n    \"\"\"\n    if isinstance(size, int):\n        size = (size, size)\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    if isinstance(padding, int):\n        padding = (padding, padding)\n    if isinstance(dilation, int):\n        dilation = (dilation, dilation)\n    if isinstance(output_padding, int):\n        output_padding = (output_padding, output_padding)\n    return (\n        (np.array(size) - 1) * np.array(stride)\n        - 2 * np.array(padding)\n        + np.array(dilation) * (np.array(kernel_size) - 1)\n        + np.array(output_padding)\n        + 1\n    )\n</code></pre>"},{"location":"reference/caveat/models/vae/","title":"caveat.models.vae","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D","title":"<code>VAE2D(in_shape, latent_dim, hidden_dims=None, kernel_size=3, stride=2, padding=1, **kwargs)</code>","text":"<p>             Bases: <code>BaseVAE</code></p> <p>Simple VAE model.</p> PARAMETER  DESCRIPTION <code>in_shape</code> <p>[C, time_step, activity_encoding].</p> <p> TYPE: <code>tuple[int, int, int]</code> </p> <code>latent_dim</code> <p>Latent space size.</p> <p> TYPE: <code>int</code> </p> <code>hidden_dims</code> <p>description. Defaults to None.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>kernel_size</code> <p>description. Defaults to 3.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>description. Defaults to 2.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>description. Defaults to 1.</p> <p> TYPE: <code>Union[tuple[int, int], int]</code> DEFAULT: <code>1</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple[int, int, int],\n    latent_dim: int,\n    hidden_dims: list = None,\n    kernel_size: Union[tuple[int, int], int] = 3,\n    stride: Union[tuple[int, int], int] = 2,\n    padding: Union[tuple[int, int], int] = 1,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Simple VAE model.\n\n    Args:\n        in_shape (tuple[int, int, int]): [C, time_step, activity_encoding].\n        latent_dim (int): Latent space size.\n        hidden_dims (list, optional): _description_. Defaults to None.\n        kernel_size (Union[tuple[int, int], int], optional): _description_. Defaults to 3.\n        stride (Union[tuple[int, int], int], optional): _description_. Defaults to 2.\n        padding (Union[tuple[int, int], int], optional): _description_. Defaults to 1.\n    \"\"\"\n    super(VAE2D, self).__init__()\n\n    self.latent_dim = latent_dim\n    self.MSE = MeanSquaredError()\n    self.hamming = MulticlassHammingDistance(\n        num_classes=in_shape[-1], average=\"micro\"\n    )\n    modules = []\n    target_shapes = []\n    channels, h, w = in_shape\n\n    # Build Encoder\n    for hidden_channels in hidden_dims:\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels=channels,\n                    out_channels=hidden_channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                ),\n                nn.BatchNorm2d(hidden_channels),\n                nn.LeakyReLU(),\n            )\n        )\n        target_shapes.append((h, w))\n        h, w = conv_size(\n            (h, w), kernel_size=kernel_size, padding=padding, stride=stride\n        )\n        channels = hidden_channels\n\n    self.shape_before_flattening = (-1, channels, h, w)\n    self.encoder = nn.Sequential(*modules)\n    flat_size = int(channels * h * w)\n    self.fc_mu = nn.Linear(flat_size, latent_dim)\n    self.fc_var = nn.Linear(flat_size, latent_dim)\n\n    # Build Decoder\n    modules = []\n    self.decoder_input = nn.Linear(latent_dim, flat_size)\n\n    hidden_dims.reverse()\n\n    _, channels, h, w = self.shape_before_flattening\n\n    for i in range(len(hidden_dims) - 1):\n        modules.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(\n                    in_channels=channels,\n                    out_channels=hidden_dims[i],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    output_padding=calc_output_padding(target_shapes[i]),\n                ),\n                nn.BatchNorm2d(hidden_dims[i + 1]),\n                nn.LeakyReLU(),\n            )\n        )\n\n    self.decoder = nn.Sequential(*modules)\n\n    self.final_layer = nn.Sequential(\n        nn.ConvTranspose2d(\n            in_channels=hidden_dims[-1],\n            out_channels=in_shape[0],\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            output_padding=calc_output_padding(target_shapes[-1]),\n        ),\n        nn.BatchNorm2d(in_shape[0]),\n        nn.Tanh(),\n    )\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.MSE","title":"<code>MSE = MeanSquaredError()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.decoder","title":"<code>decoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.decoder_input","title":"<code>decoder_input = nn.Linear(latent_dim, flat_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.encoder","title":"<code>encoder = nn.Sequential(*modules)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.fc_mu","title":"<code>fc_mu = nn.Linear(flat_size, latent_dim)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.fc_var","title":"<code>fc_var = nn.Linear(flat_size, latent_dim)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.final_layer","title":"<code>final_layer = nn.Sequential(nn.ConvTranspose2d(in_channels=hidden_dims[-1], out_channels=in_shape[0], kernel_size=kernel_size, stride=stride, padding=padding, output_padding=calc_output_padding(target_shapes[-1])), nn.BatchNorm2d(in_shape[0]), nn.Tanh())</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.hamming","title":"<code>hamming = MulticlassHammingDistance(num_classes=in_shape[-1], average='micro')</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.latent_dim","title":"<code>latent_dim = latent_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.shape_before_flattening","title":"<code>shape_before_flattening = (-1, channels, h, w)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.decode","title":"<code>decode(z)</code>","text":"<p>Maps the given latent codes.</p> PARAMETER  DESCRIPTION <code>z</code> <p>description</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>description</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def decode(self, z: tensor) -&gt; tensor:\n    \"\"\"Maps the given latent codes.\n\n    Args:\n        z (tensor): _description_\n\n    Returns:\n        tensor: _description_\n    \"\"\"\n    result = self.decoder_input(z)\n    result = result.view(self.shape_before_flattening)\n    result = self.decoder(result)\n    result = self.final_layer(result)\n    return result\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.encode","title":"<code>encode(input)</code>","text":"<p>Encodes the input by passing through the encoder network.</p> PARAMETER  DESCRIPTION <code>input</code> <p>description</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>list[tensor]</code> <p>list[tensor]: description</p> Source code in <code>caveat/models/vae.py</code> <pre><code>def encode(self, input: tensor) -&gt; list[tensor]:\n    \"\"\"Encodes the input by passing through the encoder network.\n\n    Args:\n        input (tensor): _description_\n\n    Returns:\n        list[tensor]: _description_\n    \"\"\"\n    result = self.encoder(input)\n    result = torch.flatten(result, start_dim=1)\n\n    # Split the result into mu and var components\n    # of the latent Gaussian distribution\n    mu = self.fc_mu(result)\n    log_var = self.fc_var(result)\n\n    return [mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.forward","title":"<code>forward(input, **kwargs)</code>","text":"<p>Forward pass.</p> PARAMETER  DESCRIPTION <code>input</code> <p>description</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>list[tensor]</code> <p>list[tensor]: description</p> Source code in <code>caveat/models/vae.py</code> <pre><code>def forward(self, input: tensor, **kwargs) -&gt; list[tensor]:\n    \"\"\"Forward pass.\n\n    Args:\n        input (tensor): _description_\n\n    Returns:\n        list[tensor]: _description_\n    \"\"\"\n    mu, log_var = self.encode(input)\n    z = self.reparameterize(mu, log_var)\n    return [self.decode(z), input, mu, log_var]\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.generate","title":"<code>generate(x, **kwargs)</code>","text":"<p>Given an encoder input, return reconstructed output.</p> PARAMETER  DESCRIPTION <code>x</code> <p>[B x C x H x W]</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[B x C x H x W]</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def generate(self, x: tensor, **kwargs) -&gt; tensor:\n    \"\"\"Given an encoder input, return reconstructed output.\n\n    Args:\n        x (tensor): [B x C x H x W]\n\n    Returns:\n        tensor: [B x C x H x W]\n    \"\"\"\n\n    samples = self.forward(x)[0]\n    samples = hot_argmax(samples, -1)\n    return samples\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.loss_function","title":"<code>loss_function(recons, input, mu, log_var, **kwargs)</code>","text":"<p>Computes the VAE loss function. KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}</p> RETURNS DESCRIPTION <code>dict</code> <p>description</p> <p> TYPE: <code>dict</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def loss_function(self, recons, input, mu, log_var, **kwargs) -&gt; dict:\n    r\"\"\"Computes the VAE loss function.\n    KL(N(\\mu, \\sigma), N(0, 1))\n    = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n\n    Returns:\n        dict: _description_\n    \"\"\"\n\n    kld_weight = kwargs[\n        \"M_N\"\n    ]  # Account for the minibatch samples from the dataset\n    # recons_loss = F.mse_loss(recons, input)\n    recons_mse_loss = self.MSE(recons, input)\n    recon_argmax = torch.argmax(recons, dim=-1)\n    input_argmax = torch.argmax(input, dim=-1)\n\n    recons_ham_loss = self.hamming(recon_argmax, input_argmax)\n\n    kld_loss = torch.mean(\n        -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1),\n        dim=0,\n    )\n\n    loss = recons_mse_loss + kld_weight * kld_loss\n    return {\n        \"loss\": loss,\n        \"reconstruction_loss\": recons_mse_loss.detach(),\n        \"recons_ham_loss\": recons_ham_loss.detach(),\n        \"KLD\": -kld_loss.detach(),\n    }\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick to sample from N(mu, var) from N(0,1).</p> PARAMETER  DESCRIPTION <code>mu</code> <p>Mean of the latent Gaussian [B x D]</p> <p> TYPE: <code>tensor</code> </p> <code>logvar</code> <p>Standard deviation of the latent Gaussian [B x D]</p> <p> TYPE: <code>tensor</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>[B x D]</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def reparameterize(self, mu: tensor, logvar: tensor) -&gt; tensor:\n    \"\"\"Reparameterization trick to sample from N(mu, var) from N(0,1).\n\n    Args:\n        mu (tensor): Mean of the latent Gaussian [B x D]\n        logvar (tensor): Standard deviation of the latent Gaussian [B x D]\n\n    Returns:\n        tensor: [B x D]\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return eps * std + mu\n</code></pre>"},{"location":"reference/caveat/models/vae/#caveat.models.vae.VAE2D.sample","title":"<code>sample(num_samples, current_device, **kwargs)</code>","text":"<p>Sample from the latent space and return the corresponding decoder space map.</p> PARAMETER  DESCRIPTION <code>num_samples</code> <p>Number of samples.</p> <p> TYPE: <code>int</code> </p> <code>current_device</code> <p>Device to run the model</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tensor</code> <p>description</p> <p> TYPE: <code>tensor</code> </p> Source code in <code>caveat/models/vae.py</code> <pre><code>def sample(self, num_samples: int, current_device: int, **kwargs) -&gt; tensor:\n    \"\"\"Sample from the latent space and return the corresponding decoder space map.\n\n    Args:\n        num_samples (int): Number of samples.\n        current_device (int): Device to run the model\n\n    Returns:\n        tensor: _description_\n    \"\"\"\n\n    z = torch.randn(num_samples, self.latent_dim)\n    z = z.to(current_device)\n    samples = self.decode(z)\n    samples = hot_argmax(samples, -1)\n    return samples\n</code></pre>"},{"location":"reference/caveat/report/","title":"caveat.report","text":""},{"location":"reference/caveat/report/#caveat.report.describe","title":"<code>describe(observed, ys, metric, head=None)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def describe(\n    observed: DataFrame,\n    ys: dict[str, DataFrame],\n    metric: Callable,\n    head: Optional[int] = None,\n) -&gt; DataFrame:\n    x_report = metric(observed)\n    x_report.name = \"observed\"\n    report = DataFrame(x_report)\n    for name, y in ys.items():\n        y_report = metric(y)\n        y_report.name = name\n        report = concat([report, y_report], axis=1)\n        report = report.fillna(0)\n    report[\"mean\"] = report[ys.keys()].mean(axis=1)\n    report[\"mean delta\"] = report[\"mean\"] - report.observed\n    report[\"std\"] = report[ys.keys()].std(axis=1)\n    if head is not None:\n        report = report.head(head)\n    return report\n</code></pre>"},{"location":"reference/caveat/report/#caveat.report.report_diff","title":"<code>report_diff(observed, ys, feature, head=None)</code>","text":"Source code in <code>caveat/report.py</code> <pre><code>def report_diff(\n    observed: DataFrame,\n    ys: dict[str, DataFrame],\n    feature: Callable,\n    head: Optional[int] = None,\n) -&gt; DataFrame:\n    x_report = feature(observed)\n    x_report.name = \"observed\"\n    report = DataFrame(x_report)\n    for name, y in ys.items():\n        y_report = feature(y)\n        y_report.name = name\n        report = concat([report, y_report], axis=1)\n        report = report.fillna(0)\n        report[f\"{name} delta\"] = report[name] - report.observed\n    if head is not None:\n        report = report.head(head)\n    return report\n</code></pre>"},{"location":"reference/caveat/run/","title":"caveat.run","text":""},{"location":"reference/caveat/run/#caveat.run.batch","title":"<code>batch(batch_config)</code>","text":"<p>Runs a batch of training and reporting runs based on the provided configuration.</p> PARAMETER  DESCRIPTION <code>batch_config</code> <p>A dictionary containing the configuration for each training job.</p> <p> TYPE: <code>dict[dict]</code> </p> RETURNS DESCRIPTION <p>None</p> Source code in <code>caveat/run.py</code> <pre><code>def batch(batch_config: dict[dict]):\n    \"\"\"\n    Runs a batch of training and reporting runs based on the provided configuration.\n\n    Args:\n        batch_config (dict[dict]): A dictionary containing the configuration for each training job.\n\n    Returns:\n        None\n    \"\"\"\n    global_config = batch_config.pop(\"global\")\n    logger_params = global_config.get(\"logging_params\", {})\n    name = logger_params.get(\n        \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"), name)\n\n    seed = batch_config.pop(\"seed\", seeder())\n\n    data_path = global_config[\"data_path\"]\n    observed = data.load_and_validate(data_path)\n\n    sampled = {\n        name: train_sample_and_report(name, observed, config, log_dir, seed)\n        for name, config in batch_config.items()\n    }\n\n    report_results(observed, sampled, log_dir)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.describe_results","title":"<code>describe_results(observed, sampled, log_dir, n=10)</code>","text":"<p>Generate a variance report of the observed and sampled data.</p> <p>Parameters: observed (DataFrame): The observed population. sampled (dict[str, DataFrame]): A dictionary of synthetic populations, where the keys are the names of the models. log_dir (Path): The directory where the report CSV file will be saved. n (int): The number of rows to include in each section of the report. Default is 10.</p> <p>Returns: None</p> Source code in <code>caveat/run.py</code> <pre><code>def describe_results(\n    observed: DataFrame,\n    sampled: dict[str, DataFrame],\n    log_dir: Path,\n    n: int = 10,\n):\n    \"\"\"\n    Generate a variance report of the observed and sampled data.\n\n    Parameters:\n    observed (DataFrame): The observed population.\n    sampled (dict[str, DataFrame]): A dictionary of synthetic populations, where the keys are the names of the models.\n    log_dir (Path): The directory where the report CSV file will be saved.\n    n (int): The number of rows to include in each section of the report. Default is 10.\n\n    Returns:\n    None\n    \"\"\"\n    df = concat(\n        [\n            report.describe(\n                observed, sampled, features.structural.start_and_end_acts\n            ).head(n),\n            report.describe(\n                observed, sampled, features.participation.participation_rates\n            ).head(n),\n            report.describe(\n                observed,\n                sampled,\n                features.participation.act_plan_seq_participation_rates,\n            ).head(n),\n            report.describe(\n                observed,\n                sampled,\n                features.participation.act_seq_participation_rates,\n            ).head(n),\n            report.describe(\n                observed,\n                sampled,\n                features.participation.joint_participation_rates,\n            ).head(n),\n            report.describe(\n                observed, sampled, features.transitions.transition_rates\n            ).head(n),\n            report.describe(\n                observed, sampled, features.sequence.sequence_probs\n            ).head(n),\n            report.describe(\n                observed, sampled, features.durations.average_activity_durations\n            ).head(n),\n            report.describe(\n                observed,\n                sampled,\n                features.durations.average_activity_plan_seq_durations,\n            ).head(n),\n            report.describe(\n                observed,\n                sampled,\n                features.durations.average_activity_seq_durations,\n            ).head(n),\n            report.describe(\n                observed, sampled, features.times.average_start_times\n            ).head(n),\n            report.describe(\n                observed, sampled, features.times.average_end_times\n            ).head(n),\n        ]\n    )\n    set_option(\"display.precision\", 2)\n    write_path = log_dir / \"describe.csv\"\n    print(write_path)\n    df.to_csv(write_path)\n    print(df.to_markdown())\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.initiate_logger","title":"<code>initiate_logger(save_dir, name)</code>","text":"<p>Initializes a TensorBoardLogger object for logging training progress.</p> PARAMETER  DESCRIPTION <code>save_dir</code> <p>The directory where the logs will be saved.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the logger.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>TensorBoardLogger</code> <p>The initialized TensorBoardLogger object.</p> <p> TYPE: <code>TensorBoardLogger</code> </p> Source code in <code>caveat/run.py</code> <pre><code>def initiate_logger(save_dir: str, name: str) -&gt; TensorBoardLogger:\n    \"\"\"\n    Initializes a TensorBoardLogger object for logging training progress.\n\n    Args:\n        save_dir (str): The directory where the logs will be saved.\n        name (str): The name of the logger.\n\n    Returns:\n        TensorBoardLogger: The initialized TensorBoardLogger object.\n    \"\"\"\n    tb_logger = TensorBoardLogger(save_dir=save_dir, name=name)\n    Path(f\"{tb_logger.log_dir}/samples\").mkdir(exist_ok=True, parents=True)\n    Path(f\"{tb_logger.log_dir}/reconstructions\").mkdir(\n        exist_ok=True, parents=True\n    )\n    return tb_logger\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.nrun","title":"<code>nrun(config, n=5)</code>","text":"<p>Repeat a single run while varying the seed, report on variance.</p> PARAMETER  DESCRIPTION <code>config</code> <p>A dictionary containing the configuration parameters.</p> <p> TYPE: <code>dict</code> </p> <code>n</code> <p>The number of times to repeat the run. Defaults to 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> Source code in <code>caveat/run.py</code> <pre><code>def nrun(config: Dict, n: int = 5):\n    \"\"\"\n    Repeat a single run while varying the seed, report on variance.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters.\n        n (int, optional): The number of times to repeat the run. Defaults to 5.\n    \"\"\"\n    logger_params = config.get(\"logging_params\", {})\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"))\n    name = logger_params.get(\n        \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    write_path = log_dir / name\n\n    data_path = Path(config[\"data_path\"])\n    observed = data.load_and_validate(data_path)\n\n    sampled = {\n        f\"{name}_{i}\": train_sample_and_report(\n            f\"{name}_{i}\", observed, config, log_dir\n        )\n        for i in range(n)\n    }\n\n    describe_results(observed, sampled, write_path)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.report_results","title":"<code>report_results(observed, sampled, log_dir, n=10)</code>","text":"<p>Generate a report of the differences between the observed and sampled dataframes.</p> PARAMETER  DESCRIPTION <code>observed</code> <p>The observed population.</p> <p> TYPE: <code>DataFrame</code> </p> <code>sampled</code> <p>A dictionary of sampled populations.</p> <p> TYPE: <code>dict[str, DataFrame]</code> </p> <code>log_dir</code> <p>The directory to save the report CSV file.</p> <p> TYPE: <code>Path</code> </p> <code>n</code> <p>The number of rows to include in the report for each feature. Defaults to 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> Source code in <code>caveat/run.py</code> <pre><code>def report_results(\n    observed: DataFrame,\n    sampled: dict[str, DataFrame],\n    log_dir: Path,\n    n: int = 10,\n):\n    \"\"\"\n    Generate a report of the differences between the observed and sampled dataframes.\n\n    Args:\n        observed (DataFrame): The observed population.\n        sampled (dict[str, DataFrame]): A dictionary of sampled populations.\n        log_dir (Path): The directory to save the report CSV file.\n        n (int, optional): The number of rows to include in the report for each feature. Defaults to 10.\n    \"\"\"\n    df = concat(\n        [\n            report.report_diff(\n                observed, sampled, features.structural.start_and_end_acts\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.participation.participation_rates\n            ).head(n),\n            report.report_diff(\n                observed,\n                sampled,\n                features.participation.act_plan_seq_participation_rates,\n            ).head(n),\n            report.report_diff(\n                observed,\n                sampled,\n                features.participation.act_seq_participation_rates,\n            ).head(n),\n            report.report_diff(\n                observed,\n                sampled,\n                features.participation.joint_participation_rates,\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.transitions.transition_rates\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.sequence.sequence_probs\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.durations.average_activity_durations\n            ).head(n),\n            report.report_diff(\n                observed,\n                sampled,\n                features.durations.average_activity_plan_seq_durations,\n            ).head(n),\n            report.report_diff(\n                observed,\n                sampled,\n                features.durations.average_activity_seq_durations,\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.times.average_start_times\n            ).head(n),\n            report.report_diff(\n                observed, sampled, features.times.average_end_times\n            ).head(n),\n        ]\n    )\n    set_option(\"display.precision\", 2)\n    df.to_csv(Path(log_dir, \"report.csv\"))\n    print(df.to_markdown())\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.run","title":"<code>run(config)</code>","text":"<p>Runs the training and reporting process using the provided configuration.</p> PARAMETER  DESCRIPTION <code>config</code> <p>A dictionary containing the configuration parameters.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>caveat/run.py</code> <pre><code>def run(config: Dict) -&gt; None:\n    \"\"\"\n    Runs the training and reporting process using the provided configuration.\n\n    Args:\n        config (dict): A dictionary containing the configuration parameters.\n\n    Returns:\n        None\n    \"\"\"\n    logger_params = config.get(\"logging_params\", {})\n    log_dir = Path(logger_params.get(\"log_dir\", \"logs\"))\n    name = logger_params.get(\n        \"name\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    write_path = log_dir / name\n\n    seed = config.pop(\"seed\", seeder())\n\n    data_path = Path(config[\"data_path\"])\n    observed = data.load_and_validate(data_path)\n\n    sampled = {\n        name: train_sample_and_report(name, observed, config, log_dir, seed)\n    }\n\n    report_results(observed, sampled, write_path)\n</code></pre>"},{"location":"reference/caveat/run/#caveat.run.train_sample_and_report","title":"<code>train_sample_and_report(name, observed, config, log_dir, seed=None)</code>","text":"<p>Trains a model on the observed data, generates synthetic data using the trained model, and saves the synthetic data. Returns the synthetic data as a population DataFrame.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the experiment.</p> <p> TYPE: <code>str</code> </p> <code>observed</code> <p>The \"observed\" population data to train the model on.</p> <p> TYPE: <code>DataFrame</code> </p> <code>config</code> <p>A dictionary containing the configuration parameters for the experiment.</p> <p> TYPE: <code>dict</code> </p> <code>log_dir</code> <p>The directory to save the experiment logs and checkpoints.</p> <p> TYPE: <code>Path</code> </p> <code>seed</code> <p>The random seed to use for the experiment. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>pandas.DataFrame: The synthetic data generated by the trained model.</p> Source code in <code>caveat/run.py</code> <pre><code>def train_sample_and_report(\n    name: str,\n    observed: DataFrame,\n    config: dict,\n    log_dir: Path,\n    seed: Optional[int] = None,\n):\n    \"\"\"\n    Trains a model on the observed data, generates synthetic data using the trained model,\n    and saves the synthetic data. Returns the synthetic data as a population DataFrame.\n\n    Args:\n        name (str): The name of the experiment.\n        observed (pandas.DataFrame): The \"observed\" population data to train the model on.\n        config (dict): A dictionary containing the configuration parameters for the experiment.\n        log_dir (pathlib.Path): The directory to save the experiment logs and checkpoints.\n        seed (int, optional): The random seed to use for the experiment. Defaults to None.\n\n    Returns:\n        pandas.DataFrame: The synthetic data generated by the trained model.\n    \"\"\"\n    if seed is None:\n        seed = seeder()\n    manual_seed(seed)\n    print(f\"======= Training {name.title()} =======\")\n    logger = initiate_logger(log_dir, name)\n\n    encoder_name = config[\"encoder_params\"][\"name\"]\n    data_encoder = encoders.library[encoder_name](**config[\"encoder_params\"])\n    encoded = data_encoder.encode(observed)\n\n    data_loader_params = config.get(\"loader_params\", {})\n    datamodule = data.DataModule(data=encoded, **data_loader_params)\n    datamodule.setup()\n\n    model_name = config[\"model_params\"][\"name\"]\n    model = models.library[model_name]\n    model = model(in_shape=encoded.shape(), **config[\"model_params\"])\n    experiment = Experiment(model, **config[\"experiment_params\"])\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=Path(logger.log_dir, \"checkpoints\"),\n        monitor=\"val_loss\",\n        save_top_k=1,\n        save_weights_only=True,\n    )\n\n    runner = Trainer(\n        logger=logger,\n        callbacks=[\n            EarlyStopping(\n                monitor=\"val_reconstruction_loss\",\n                patience=3,\n                stopping_threshold=0.0,\n            ),\n            LearningRateMonitor(),\n            checkpoint_callback,\n        ],\n        strategy=\"ddp\",\n        **config.get(\"trainer_params\", {}),\n    )\n\n    runner.fit(experiment, datamodule=datamodule)\n\n    print(f\"======= Sampling {name.title()} =======\")\n    best_path = checkpoint_callback.best_model_path\n    print(f\"Loading best model from {best_path}\")\n    best = Experiment.load_from_checkpoint(\n        best_path, model=model, **config[\"experiment_params\"]\n    )\n    synthetic = best.generate(len(encoded))\n    synthetic = data_encoder.decode(encoded=synthetic)\n    data.validate(synthetic)\n    synthesis_path = Path(experiment.logger.log_dir, \"synthetic.csv\")\n    synthetic.to_csv(synthesis_path)\n    return synthetic\n</code></pre>"}]}